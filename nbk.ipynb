{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-08 12:59:26.570\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnlpinitiative.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: C:\\Users\\Daniel\\Desktop\\GitHub\\NLPinitiative\u001b[0m\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from nlpinitiative.data_preparation import data_import, data_preparation, dataset_normalizer\n",
    "from nlpinitiative.config import (\n",
    "    RAW_DATA_DIR, \n",
    "    EXTERNAL_DATA_DIR, \n",
    "    INTERIM_DATA_DIR, \n",
    "    CONV_SCHEMA_DIR, \n",
    "    DATASET_COLS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration of importing from local source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset\n",
      "\u001b[32m2025-02-08 12:59:29.200\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mnlpinitiative.data_preparation.data_import\u001b[0m:\u001b[36mimport_from_local_source\u001b[0m:\u001b[36m89\u001b[0m - \u001b[32m\u001b[1mData from file, C:/Users/Daniel/Downloads/dataset.csv, imported\u001b[0m\n",
      "                 reply_id     hateful_tweet_id      counter_hate_id   Q1   Q2  \\\n",
      "0     1364504065565093894  1364444425192005639  1364503631160954881  1.0  NaN   \n",
      "1     1507516585485148164  1507475083954036739  1507490000958820354  1.0  NaN   \n",
      "2      976720451148943360   959685036311064576   976558261498515456  1.0  NaN   \n",
      "3      976567337078882309   959685036311064576   976558261498515456  1.0  NaN   \n",
      "4     1359340305494011907  1359330367157764098  1359339633352478728  1.0  NaN   \n",
      "...                   ...                  ...                  ...  ...  ...   \n",
      "2616   473289864176078848   473289562903830528   473289697540993024  0.0  0.0   \n",
      "2617   475688162468319232   475558857700171776   475565607841054721  1.0  NaN   \n",
      "2618   403826060719964160   403818138963173376   403825878603694080  0.0  1.0   \n",
      "2619   850094530061901824   850036460111769600   850088912441790468  0.0  0.0   \n",
      "2620  1516390442945949701  1516388162133757956  1516390264511815690  0.0  0.0   \n",
      "\n",
      "       Q3   Q4  \n",
      "0     NaN  1.0  \n",
      "1     NaN  0.0  \n",
      "2     NaN  0.0  \n",
      "3     NaN  0.0  \n",
      "4     NaN  1.0  \n",
      "...   ...  ...  \n",
      "2616  1.0  NaN  \n",
      "2617  NaN  0.0  \n",
      "2618  0.0  NaN  \n",
      "2619  1.0  NaN  \n",
      "2620  1.0  NaN  \n",
      "\n",
      "[2621 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "local_import_example_df = data_import.import_from_local_source(\"C:/Users/Daniel/Downloads/dataset.csv\")\n",
    "print(local_import_example_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration of importing from remote source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-08 12:59:31.892\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnlpinitiative.data_preparation.data_import\u001b[0m:\u001b[36mformat_url\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSource url identified as GitHub URL, https://github.com/intelligence-csd-auth-gr/Ethos-Hate-Speech-Dataset/blob/master/ethos/ethos_data/Ethos_Dataset_Binary.csv\u001b[0m\n",
      "\u001b[32m2025-02-08 12:59:31.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnlpinitiative.data_preparation.data_import\u001b[0m:\u001b[36mformat_url\u001b[0m:\u001b[36m47\u001b[0m - \u001b[1mURL Formatted, https://raw.githubusercontent.com/intelligence-csd-auth-gr/Ethos-Hate-Speech-Dataset/refs/heads/master/ethos/ethos_data/Ethos_Dataset_Binary.csv\u001b[0m\n",
      "\u001b[32m2025-02-08 12:59:31.893\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnlpinitiative.data_preparation.data_import\u001b[0m:\u001b[36mimport_from_ext_source\u001b[0m:\u001b[36m101\u001b[0m - \u001b[1mFile type identified, '.csv'\u001b[0m\n",
      "\u001b[32m2025-02-08 12:59:31.965\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mnlpinitiative.data_preparation.data_import\u001b[0m:\u001b[36mimport_from_ext_source\u001b[0m:\u001b[36m114\u001b[0m - \u001b[32m\u001b[1mSuccessfully imported the dataset from, https://github.com/intelligence-csd-auth-gr/Ethos-Hate-Speech-Dataset/blob/master/ethos/ethos_data/Ethos_Dataset_Binary.csv\u001b[0m\n",
      "                                               comment  isHate\n",
      "0            You should know women's sports are a joke     1.0\n",
      "1      You look like Sloth with deeper Down’s syndrome     1.0\n",
      "2    You look like Russian and speak like Indian. B...     1.0\n",
      "3                 Women deserve to be abused, I guess.     1.0\n",
      "4    Women are made for making babies and cooking d...     1.0\n",
      "..                                                 ...     ...\n",
      "993   From the midnight sun where the hot springs blow     0.0\n",
      "994                        Don't say I'm not your type     0.0\n",
      "995   And therefore never send to know for whom the...     0.0\n",
      "996                      And I can't stand another day     0.0\n",
      "997   All values, unless otherwise stated, are in U...     0.0\n",
      "\n",
      "[998 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "remote_import_example_df = data_import.import_from_ext_source(\"https://github.com/intelligence-csd-auth-gr/Ethos-Hate-Speech-Dataset/blob/master/ethos/ethos_data/Ethos_Dataset_Binary.csv\")\n",
    "print(remote_import_example_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing thirdparty datasets to a standard format for our applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  TEXT  DISCRIMINATORY  \\\n",
      "0            You should know women's sports are a joke             1.0   \n",
      "1      You look like Sloth with deeper Down’s syndrome             1.0   \n",
      "2    You look like Russian and speak like Indian. B...             1.0   \n",
      "3                 Women deserve to be abused, I guess.             1.0   \n",
      "4    Women are made for making babies and cooking d...             1.0   \n",
      "..                                                 ...             ...   \n",
      "993   From the midnight sun where the hot springs blow             0.0   \n",
      "994                        Don't say I'm not your type             0.0   \n",
      "995   And therefore never send to know for whom the...             0.0   \n",
      "996                      And I can't stand another day             0.0   \n",
      "997   All values, unless otherwise stated, are in U...             0.0   \n",
      "\n",
      "     NEUTRAL    GENDER      RACE  SEXUALITY  DISABILITY  RELIGION  UNSPECIFIED  \n",
      "0        0.0  1.000000  0.000000        0.0         0.0       0.0          0.0  \n",
      "1        0.0  0.000000  0.000000        0.0         1.0       0.0          0.0  \n",
      "2        0.0  0.142857  0.857143        0.0         0.0       0.0          0.0  \n",
      "3        0.0  1.000000  0.000000        0.0         0.0       0.0          0.0  \n",
      "4        0.0  1.000000  0.000000        0.0         0.0       0.0          0.0  \n",
      "..       ...       ...       ...        ...         ...       ...          ...  \n",
      "993      1.0  0.000000  0.000000        0.0         0.0       0.0          0.0  \n",
      "994      1.0  0.000000  0.000000        0.0         0.0       0.0          0.0  \n",
      "995      1.0  0.000000  0.000000        0.0         0.0       0.0          0.0  \n",
      "996      1.0  0.000000  0.000000        0.0         0.0       0.0          0.0  \n",
      "997      1.0  0.000000  0.000000        0.0         0.0       0.0          0.0  \n",
      "\n",
      "[998 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "srcs = [\n",
    "    EXTERNAL_DATA_DIR / \"intelligence-csd-auth-gr_Ethos-Hate-Speech-Dataset_Ethos_Dataset_Binary.csv\",\n",
    "    EXTERNAL_DATA_DIR / \"intelligence-csd-auth-gr_Ethos-Hate-Speech-Dataset_Ethos_Dataset_Multi_Label.csv\"\n",
    "]\n",
    "conv = CONV_SCHEMA_DIR / \"ethos_schema_mapping.json\"\n",
    "\n",
    "normalized_dataset = dataset_normalizer.convert_to_master_schema(srcs, conv, 'ETHOS_dataset_converted')\n",
    "print(normalized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading csv as Dataset object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['TEXT', 'DISCRIMINATORY', 'NEUTRAL', 'GENDER', 'RACE', 'SEXUALITY', 'DISABILITY', 'RELIGION', 'UNSPECIFIED'],\n",
      "        num_rows: 998\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = data_preparation.get_dataset_from_file(\"ETHOS_dataset_converted.csv\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting labels and initializing dicts for converting from labels to ids and ids to labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DISCRIMINATORY', 'NEUTRAL', 'GENDER', 'RACE', 'SEXUALITY', 'DISABILITY', 'RELIGION', 'UNSPECIFIED']\n",
      "{'DISCRIMINATORY': 0, 'NEUTRAL': 1, 'GENDER': 2, 'RACE': 3, 'SEXUALITY': 4, 'DISABILITY': 5, 'RELIGION': 6, 'UNSPECIFIED': 7}\n",
      "{0: 'DISCRIMINATORY', 1: 'NEUTRAL', 2: 'GENDER', 3: 'RACE', 4: 'SEXUALITY', 5: 'DISABILITY', 6: 'RELIGION', 7: 'UNSPECIFIED'}\n"
     ]
    }
   ],
   "source": [
    "labels, lbl2idx, idx2lbl = data_preparation.get_labels_and_dicts(dataset)\n",
    "print(labels)\n",
    "print(lbl2idx)\n",
    "print(idx2lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of a tokenizer (using pre-trained BERT tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8d6fc0d3544253b29d58e1b67f452a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 5604, 19204, 17629, 2005, 17181, 102]\n",
      "[CLS] testing tokenizer for encoding [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = data_preparation.get_tokenizer()\n",
    "encoded_text = tokenizer.encode(\"Testing tokenizer for encoding\")\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(encoded_text)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "preprocess_dataset() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ecd_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdata_preparation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m dataset_entry_ex \u001b[38;5;241m=\u001b[39m ecd_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset_entry_ex\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[1;31mTypeError\u001b[0m: preprocess_dataset() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "ecd_dataset = data_preparation.preprocess_dataset(dataset, labels, tokenizer)\n",
    "dataset_entry_ex = ecd_dataset['train'][0]\n",
    "print(dataset_entry_ex.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPinitiative-sAkSWYnk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
