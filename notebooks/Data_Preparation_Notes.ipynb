{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-14 15:23:06.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mnlpinitiative.config\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: C:\\Users\\Daniel\\Desktop\\GitHub\\NLPinitiative\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from nlpinitiative.data_preparation import data_import, data_preparation, dataset_normalizer\n",
    "from nlpinitiative.config import (\n",
    "    EXTERNAL_DATA_DIR, \n",
    "    CONV_SCHEMA_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing third-party datasets for use in NLP model training:\n",
    "\n",
    "For the purposes of this project, we are going to rely on third-party datasets to make up for a lack of personally procured data. As such, we have implemented some functionality to make this easier for future developers/data analysts.\n",
    "\n",
    "### Importing datasets from a local source (on your local system):\n",
    "For the purposes of our applications, we will consider the \"raw\" datasets to be personally produced datasets rather than those that have already been created (\"external\"). As such, importing from a local source will by default store the datasets within the data/raw directory. If the data to be imported locally is a third-party dataset, the user can change the 'tp_src' value to True, where the data will be stored within the data/external directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_import_example_df = data_import.import_from_local_source(\"C:/Users/Daniel/Downloads/dataset.csv\", tp_src=False)\n",
    "print(local_import_example_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration of importing from remote/external source:\n",
    "This function facillitates importing data from a given URL (primarily remote repositories like GitHub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_import_example_df = data_import.import_from_ext_source(\"https://github.com/intelligence-csd-auth-gr/Ethos-Hate-Speech-Dataset/blob/master/ethos/ethos_data/Ethos_Dataset_Binary.csv\")\n",
    "print(remote_import_example_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversion of third-party datasets:\n",
    "Since we are making use of third-party datasets, we need a means of converting the original dataset schema to a format that will utilize our labeling scheme. As such, we have implemented some functionality to facilitate this process.\n",
    "\n",
    "### Normalizing third-party datasets to a standard format for our applications:\n",
    "This function facilitates taking one or more datasets (all passed datasets that are to be normalized should maintain the same general structure so that they can be merged prior to normalization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  TEXT  DISCRIMINATORY  \\\n",
      "0            You should know women's sports are a joke               1   \n",
      "1      You look like Sloth with deeper Downâ€™s syndrome               1   \n",
      "2    You look like Russian and speak like Indian. B...               1   \n",
      "3                 Women deserve to be abused, I guess.               1   \n",
      "4    Women are made for making babies and cooking d...               1   \n",
      "..                                                 ...             ...   \n",
      "993   From the midnight sun where the hot springs blow               0   \n",
      "994                        Don't say I'm not your type               0   \n",
      "995   And therefore never send to know for whom the...               0   \n",
      "996                      And I can't stand another day               0   \n",
      "997   All values, unless otherwise stated, are in U...               0   \n",
      "\n",
      "       GENDER      RACE  SEXUALITY  DISABILITY  RELIGION  UNSPECIFIED  \n",
      "0    1.000000  0.000000        0.0         0.0       0.0          0.0  \n",
      "1    0.000000  0.000000        0.0         1.0       0.0          0.0  \n",
      "2    0.142857  0.857143        0.0         0.0       0.0          0.0  \n",
      "3    1.000000  0.000000        0.0         0.0       0.0          0.0  \n",
      "4    1.000000  0.000000        0.0         0.0       0.0          0.0  \n",
      "..        ...       ...        ...         ...       ...          ...  \n",
      "993  0.000000  0.000000        0.0         0.0       0.0          0.0  \n",
      "994  0.000000  0.000000        0.0         0.0       0.0          0.0  \n",
      "995  0.000000  0.000000        0.0         0.0       0.0          0.0  \n",
      "996  0.000000  0.000000        0.0         0.0       0.0          0.0  \n",
      "997  0.000000  0.000000        0.0         0.0       0.0          0.0  \n",
      "\n",
      "[764 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "srcs = [\n",
    "    EXTERNAL_DATA_DIR / \"intelligence-csd-auth-gr_Ethos-Hate-Speech-Dataset_Ethos_Dataset_Binary.csv\",\n",
    "    EXTERNAL_DATA_DIR / \"intelligence-csd-auth-gr_Ethos-Hate-Speech-Dataset_Ethos_Dataset_Multi_Label.csv\"\n",
    "]\n",
    "conv = CONV_SCHEMA_DIR / \"ethos_schema_mapping.json\"\n",
    "\n",
    "normalized_dataset = dataset_normalizer.convert_to_master_schema(srcs, conv, 'ETHOS_dataset_converted')\n",
    "print(normalized_dataset)\n",
    "\n",
    "dataset_normalizer.store_normalized_dataset(normalized_dataset, 'ETHOS_dataset_converted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation:\n",
    "Prior to being able to use the dataset in training the model, we must first perpare the data by converting it into a dataset, and tokenizing the textual data (in addition to restructuring the data to a format that can be passed into a model).\n",
    "\n",
    "### Loading csv as Dataset object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['TEXT', 'DISCRIMINATORY', 'NEUTRAL', 'GENDER', 'RACE', 'SEXUALITY', 'DISABILITY', 'RELIGION', 'UNSPECIFIED'],\n",
      "        num_rows: 698\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['TEXT', 'DISCRIMINATORY', 'NEUTRAL', 'GENDER', 'RACE', 'SEXUALITY', 'DISABILITY', 'RELIGION', 'UNSPECIFIED'],\n",
      "        num_rows: 300\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = data_preparation.get_dataset_from_file(\"ETHOS_dataset_converted.csv\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting labels and initializing dicts for converting from labels to ids and ids to labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DISCRIMINATORY', 'NEUTRAL', 'GENDER', 'RACE', 'SEXUALITY', 'DISABILITY', 'RELIGION', 'UNSPECIFIED']\n",
      "{'DISCRIMINATORY': 0, 'NEUTRAL': 1, 'GENDER': 2, 'RACE': 3, 'SEXUALITY': 4, 'DISABILITY': 5, 'RELIGION': 6, 'UNSPECIFIED': 7}\n",
      "{0: 'DISCRIMINATORY', 1: 'NEUTRAL', 2: 'GENDER', 3: 'RACE', 4: 'SEXUALITY', 5: 'DISABILITY', 6: 'RELIGION', 7: 'UNSPECIFIED'}\n"
     ]
    }
   ],
   "source": [
    "labels, lbl2idx, idx2lbl = data_preparation.get_labels_and_dicts(dataset)\n",
    "print(labels)\n",
    "print(lbl2idx)\n",
    "print(idx2lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization of a tokenizer (using pre-trained BERT tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 5604, 19204, 17629, 2005, 17181, 102]\n",
      "[CLS] testing tokenizer for encoding [SEP]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = data_preparation.get_tokenizer()\n",
    "encoded_text = tokenizer.encode(\"Testing tokenizer for encoding\")\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(encoded_text)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode (preprocess) the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb420bff9894551aa0337ab20d1919c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/698 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe30758e60f47538dcec68fd9762ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "ecd_dataset = data_preparation.preprocess_dataset(dataset, labels, tokenizer)\n",
    "dataset_entry_ex = ecd_dataset['train'][0]\n",
    "print(dataset_entry_ex.keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPinitiative-sAkSWYnk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
