{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NLPInitiative Documentation Project Details Description Codebase for training, evaluating and deploying NLP models used to detect discriminatory language targeting marginallized individuals or communities and the type(s) of discrimination detected. Organization \u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 interim <- Intermediate datasets that have been normalized \u2502 \u251c\u2500\u2500 normalization_schema <- The schema used for normalizing 3rd party datasets \u2502 \u251c\u2500\u2500 processed <- The final merged dataset consisting of all normalized datasets \u2502 \u2514\u2500\u2500 raw <- The original, raw datasets prior to normalization \u2502 \u251c\u2500\u2500 docs <- A directory containing documentation used for generating and serving \u2502 project documentation \u2502 \u251c\u2500\u2500 models <- Trained and serialized models, model predictions, or model summaries \u2502 \u2502 \u2502 \u251c\u2500\u2500 binary_classification <- Trained and serialized binary classification \u2502 \u2502 models/model predictions/model summaries \u2502 \u2514\u2500\u2500 multilabel_regression <- Trained and serialized multilabel regression \u2502 models/model predictions/model summaries \u2502 \u251c\u2500\u2500 nlpinitiative <- Source code for use in this project \u2502 \u2502 \u2502 \u251c\u2500\u2500 __init__.py <- Makes nlpinitiative a Python module \u2502 \u251c\u2500\u2500 config.py <- Store useful variables and configuration \u2502 \u2514\u2500\u2500 modeling <- Source code for model training and inference \u2502 \u2502 \u2502 \u251c\u2500\u2500 __init__.py <- Makes modeling a Python module \u2502 \u251c\u2500\u2500 predict.py <- Code to run model inference with trained models \u2502 \u2514\u2500\u2500 train.py <- Code to train models \u2502 \u251c\u2500\u2500 notebooks <- Directory containing Jupyter notebooks detailing research, testing and \u2502 example usage of project modules \u2502 \u251c\u2500\u2500 references <- Directory containing Data dictionaries, manuals, and all other \u2502 explanatory materials \u2502 \u251c\u2500\u2500 LICENSE <- Open-source license if one is chosen \u2502 \u251c\u2500\u2500 mkdocs.yml <- mkdocs project configuration \u2502 \u251c\u2500\u2500 Pipfile <- The project dependency file for reproducing the analysis environment, \u2502 e.g., generated with `pipenv install` \u2502 \u251c\u2500\u2500 Pipfile.lock <- Locked file containing hashes for dependencies \u2502 \u251c\u2500\u2500 pyproject.toml <- Project configuration file with package metadata for nlpinitiative and \u2502 configuration for tools like black \u2502 \u251c\u2500\u2500 README.md <- The top-level README for developers using this project \u2502 \u251c\u2500\u2500 setup.cfg <- Configuration file for flake8 \u2502 \u2514\u2500\u2500 setup.sh <- Bash script containing convenience commands for managing the project Based on the CookieCutter Data Science project structure template Project Model and Dataset Repositories Model Repositories Fine-tuned Model Base Model Repository Link Binary Classification Model BERT NLPinitiative-Binary-Classification Multilabel Regression Model BERT NLPinitiative-Multilabel-Regression Dataset Repository Repository Link Dataset Repository NLPinitiative-Dataset Project Setup The Makefile contains the central entry points for common tasks related to this project. Datasets Used Ethos - multi-lab E l ha T e speec H detecti O n data S et A collection consisting of binary and multilabel data containing hate speech from social media. Links GitHub Repository Binary Dataset Multilabel Dataset BibTeX Reference @article{mollas_ethos_2022, title = {{ETHOS}: a multi-label hate speech detection dataset}, issn = {2198-6053}, url = {https://doi.org/10.1007/s40747-021-00608-2}, doi = {10.1007/s40747-021-00608-2}, journal = {Complex \\& Intelligent Systems}, author = {Mollas, Ioannis and Chrysopoulou, Zoe and Karlos, Stamatis and Tsoumakas, Grigorios}, month = jan, year = {2022}, } Multitarget-CONAN Multi-Target CONAN is a dataset of hate speech/counter-narrative pairs for English comprising several hate targets, collected using a Human-in-the-Loop approach. Links GitHub Repository Multitarget-CONAN Dataset BibTeX Reference @inproceedings{fanton-2021-human, title=\"{Human-in-the-Loop for Data Collection: a Multi-Target Counter Narrative Dataset to Fight Online Hate Speech}\", author=\"{Fanton, Margherita and Bonaldi, Helena and Tekiro\u011flu, Serra Sinem and Guerini, Marco}\", booktitle = \"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\", month = aug, year = \"2021\", publisher = \"Association for Computational Linguistics\", }","title":"Home"},{"location":"#nlpinitiative-documentation","text":"","title":"NLPInitiative Documentation"},{"location":"#project-details","text":"","title":"Project Details"},{"location":"#description","text":"Codebase for training, evaluating and deploying NLP models used to detect discriminatory language targeting marginallized individuals or communities and the type(s) of discrimination detected.","title":"Description"},{"location":"#organization","text":"\u251c\u2500\u2500 data \u2502 \u251c\u2500\u2500 interim <- Intermediate datasets that have been normalized \u2502 \u251c\u2500\u2500 normalization_schema <- The schema used for normalizing 3rd party datasets \u2502 \u251c\u2500\u2500 processed <- The final merged dataset consisting of all normalized datasets \u2502 \u2514\u2500\u2500 raw <- The original, raw datasets prior to normalization \u2502 \u251c\u2500\u2500 docs <- A directory containing documentation used for generating and serving \u2502 project documentation \u2502 \u251c\u2500\u2500 models <- Trained and serialized models, model predictions, or model summaries \u2502 \u2502 \u2502 \u251c\u2500\u2500 binary_classification <- Trained and serialized binary classification \u2502 \u2502 models/model predictions/model summaries \u2502 \u2514\u2500\u2500 multilabel_regression <- Trained and serialized multilabel regression \u2502 models/model predictions/model summaries \u2502 \u251c\u2500\u2500 nlpinitiative <- Source code for use in this project \u2502 \u2502 \u2502 \u251c\u2500\u2500 __init__.py <- Makes nlpinitiative a Python module \u2502 \u251c\u2500\u2500 config.py <- Store useful variables and configuration \u2502 \u2514\u2500\u2500 modeling <- Source code for model training and inference \u2502 \u2502 \u2502 \u251c\u2500\u2500 __init__.py <- Makes modeling a Python module \u2502 \u251c\u2500\u2500 predict.py <- Code to run model inference with trained models \u2502 \u2514\u2500\u2500 train.py <- Code to train models \u2502 \u251c\u2500\u2500 notebooks <- Directory containing Jupyter notebooks detailing research, testing and \u2502 example usage of project modules \u2502 \u251c\u2500\u2500 references <- Directory containing Data dictionaries, manuals, and all other \u2502 explanatory materials \u2502 \u251c\u2500\u2500 LICENSE <- Open-source license if one is chosen \u2502 \u251c\u2500\u2500 mkdocs.yml <- mkdocs project configuration \u2502 \u251c\u2500\u2500 Pipfile <- The project dependency file for reproducing the analysis environment, \u2502 e.g., generated with `pipenv install` \u2502 \u251c\u2500\u2500 Pipfile.lock <- Locked file containing hashes for dependencies \u2502 \u251c\u2500\u2500 pyproject.toml <- Project configuration file with package metadata for nlpinitiative and \u2502 configuration for tools like black \u2502 \u251c\u2500\u2500 README.md <- The top-level README for developers using this project \u2502 \u251c\u2500\u2500 setup.cfg <- Configuration file for flake8 \u2502 \u2514\u2500\u2500 setup.sh <- Bash script containing convenience commands for managing the project Based on the CookieCutter Data Science project structure template","title":"Organization"},{"location":"#project-model-and-dataset-repositories","text":"","title":"Project Model and Dataset Repositories"},{"location":"#model-repositories","text":"Fine-tuned Model Base Model Repository Link Binary Classification Model BERT NLPinitiative-Binary-Classification Multilabel Regression Model BERT NLPinitiative-Multilabel-Regression","title":"Model Repositories"},{"location":"#dataset-repository","text":"Repository Link Dataset Repository NLPinitiative-Dataset","title":"Dataset Repository"},{"location":"#project-setup","text":"The Makefile contains the central entry points for common tasks related to this project.","title":"Project Setup"},{"location":"#datasets-used","text":"","title":"Datasets Used"},{"location":"#ethos-multi-label-hate-speech-detection-dataset","text":"A collection consisting of binary and multilabel data containing hate speech from social media.","title":"Ethos - multi-labEl haTe speecH detectiOn dataSet"},{"location":"#links","text":"GitHub Repository Binary Dataset Multilabel Dataset","title":"Links"},{"location":"#bibtex-reference","text":"@article{mollas_ethos_2022, title = {{ETHOS}: a multi-label hate speech detection dataset}, issn = {2198-6053}, url = {https://doi.org/10.1007/s40747-021-00608-2}, doi = {10.1007/s40747-021-00608-2}, journal = {Complex \\& Intelligent Systems}, author = {Mollas, Ioannis and Chrysopoulou, Zoe and Karlos, Stamatis and Tsoumakas, Grigorios}, month = jan, year = {2022}, }","title":"BibTeX Reference"},{"location":"#multitarget-conan","text":"Multi-Target CONAN is a dataset of hate speech/counter-narrative pairs for English comprising several hate targets, collected using a Human-in-the-Loop approach.","title":"Multitarget-CONAN"},{"location":"#links_1","text":"GitHub Repository Multitarget-CONAN Dataset","title":"Links"},{"location":"#bibtex-reference_1","text":"@inproceedings{fanton-2021-human, title=\"{Human-in-the-Loop for Data Collection: a Multi-Target Counter Narrative Dataset to Fight Online Hate Speech}\", author=\"{Fanton, Margherita and Bonaldi, Helena and Tekiro\u011flu, Serra Sinem and Guerini, Marco}\", booktitle = \"Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\", month = aug, year = \"2021\", publisher = \"Association for Computational Linguistics\", }","title":"BibTeX Reference"},{"location":"dataset-management/","text":"Dataset Operations nlpinitiative.data_preparation.data_management DataManager A class for handling data import, normalization and preprocessing/tokenization. Source code in nlpinitiative\\data_preparation\\data_management.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 class DataManager : \"\"\"A class for handling data import, normalization and preprocessing/tokenization.\"\"\" def __init__ ( self ): \"\"\"Contructor method for instantiating a DataManager object.\"\"\" self . normalizer = DataNormalizer () self . processor = DataProcessor () self . rec_mgr = DatasetRecordManager () # Data Importing Functionality # =================================================================================================================== def _is_valid_url ( self , url : str ) -> bool : \"\"\"Checks that a URL has a valid format. Parameters ---------- url : str The url to be checked. Returns ------- bool True if the URL valid or False if not. \"\"\" if url : parsed_url = urlparse ( url ) return bool ( parsed_url . scheme in [ \"http\" , \"https\" , \"ftp\" ]) else : return False def _generate_import_filename ( self , url : str ) -> str : \"\"\"Generates a filename that will be used when importing new datasets. Parameters ---------- url : str The url of the dataset to be imported. Returns ------- str The generated file name. \"\"\" def github (): \"\"\"Generates a filename based on a GitHub URL. Returns ------- str The generated file name. \"\"\" parsed = urlparse ( url ) path = os . path . splitext ( parsed . path )[ 0 ][ 1 :] path_arr = path . split ( \"/\" ) return \"_\" . join ([ path_arr [ 0 ], path_arr [ 1 ], path_arr [ - 1 ]]) if \"github\" in url : return github () else : split_arr = url . split ( \"/\" ) return split_arr [ - 1 ] def _format_url ( self , url : str ) -> str : \"\"\"Converts URLs to a format that can be used for importing data from a remote source. Parameters ---------- url : str The url of the dataset to be imported. Returns ------- str The reformatted URL. \"\"\" def github (): \"\"\"Handles conversion of GitHub URLs. Returns ------- str The reformatted GitHub URL. \"\"\" base_url = \"https://raw.githubusercontent.com\" if base_url in url : return url else : updated_url = url . replace ( \"https://github.com\" , base_url ) updated_url = updated_url . replace ( \"blob\" , \"refs/heads\" ) return updated_url if \"github\" in url : logger . info ( f \"Source url identified as GitHub URL, { url } \" ) formatted_url = github () logger . info ( f \"URL Formatted, { formatted_url } \" ) return formatted_url else : return url def file_to_df ( self , source : str , ext : str ) -> pd . DataFrame : \"\"\"Converts a dataset to a dataframe. This function also handles conversion of csv files with atypical delimiters. Parameters ---------- source : str The file path for a local dataset or URL for a remote dataset. ext : str The file extension of the dataset. Returns ------- DataFrame The dataset as a Pandas DataFrame. \"\"\" try : match ext : case \".csv\" : try : df = pd . read_csv ( source ) except : df = pd . read_csv ( source , delimiter = \";\" ) case \".xlsx\" : df = pd . read_excel ( source ) case \".json\" : df = pd . read_json ( source ) case _ : df = None return df except Exception as e : err_msg = f \"Failed to import from source - { e } \" logger . error ( err_msg ) raise Exception ( err_msg ) def import_data ( self , import_type : str , source , dataset_name : str , is_third_party : bool = True , local_ds_ref_url : bool = None , overwrite : bool = False , ) -> pd . DataFrame : \"\"\"Imports data from a local (by file path) or remote (ny URL) source. Parameters ---------- import_type : str 'local' if from a local source, 'external' if from remote source. source : str The file path or URL of the dataset. dataset_name : str The name used to id the dataset. is_third_party : bool, optional True if 3rd party dataset, False if not (default is True). local_ds_ref_url : str, optional Reference URL for datasets imported locally (default is None). overwrite : bool, optional True if files with the same name should be overwritten, False if not (default is False). Returns ------- DataFrame The dataset as a Pandas DataFrame. \"\"\" ref_url = None formatted_url = None match import_type : case \"local\" : if source is None or not os . path . exists ( source ): err_msg = \"Dataset filepath does not exist\" logger . error ( err_msg ) raise Exception ( err_msg ) if is_third_party and not self . _is_valid_url ( local_ds_ref_url ): err_msg = ( \"Locally imported 3rd party dataset imports must include a reference URL.\" ) logger . error ( err_msg ) raise Exception ( err_msg ) ref_url = local_ds_ref_url if is_third_party else \"Custom Created Dataset\" tail = os . path . split ( source )[ - 1 ] filename , ext = os . path . splitext ( tail )[ - 2 :] if ext not in ACCEPTED_FILE_FORMATS : err_msg = \"Unsupported file type\" logger . error ( err_msg ) raise Exception ( err_msg ) src = source case \"external\" : if not self . _is_valid_url ( source ): err_msg = \"Invalid URL\" logger . error ( err_msg ) raise Exception ( err_msg ) ref_url = source formatted_url = self . _format_url ( ref_url ) filename = self . _generate_import_filename ( formatted_url ) path = urlparse ( formatted_url ) . path ext = os . path . splitext ( path )[ 1 ] if ext not in ACCEPTED_FILE_FORMATS : err_msg = \"Unsupported file type\" logger . error ( err_msg ) raise Exception ( err_msg ) src = formatted_url case _ : err_msg = \"Invalid import type\" logger . error ( err_msg ) raise Exception ( err_msg ) ds_df = self . file_to_df ( src , ext ) if self . rec_mgr . dataset_src_exists ( ref_url ): return ds_df self . _store_data ( data_df = ds_df , filename = filename , destpath = RAW_DATA_DIR , overwrite = overwrite ) self . rec_mgr . update ( ds_id = dataset_name , src_url = ref_url , download_url = formatted_url , raw_ds_filename = f \" { filename } .csv\" , ) logger . success ( f \"Successfully imported { import_type } dataset from { source } .\" ) return ds_df # Data Normalization Functionality # =================================================================================================================== def _valid_filepath ( self , path : Path ) -> bool : \"\"\"Checks that the specified file path is valid. Parameters ---------- path : Path The file path to be checked. Returns ------- bool True if it exists, False if it does not. \"\"\" return os . path . exists ( path ) def normalize_dataset ( self , ds_files : list [ str ], conv_schema_fn : str , output_fn : str ) -> pd . DataFrame : \"\"\"Handles normalization of a third-party dataset to the schema used for training our models. Parameters ---------- ds_files : list[str] One or more dataset files to merge and normalize. conv_schema_fn : str The file name of the json conversion schema used. output_fn : str The output filename to be used. Returns ------- DataFrame True if it exists, False if it does not. \"\"\" for filename in ds_files : if not self . _valid_filepath (( RAW_DATA_DIR / filename )): err_msg = f \"Invalid filepath for file, { filename } [ { RAW_DATA_DIR / filename } ].\" logger . error ( err_msg ) raise Exception ( err_msg ) if not self . _valid_filepath ( NORM_SCHEMA_DIR / conv_schema_fn ): err_msg = f \"Normalization schema file, { conv_schema_fn } , does not exist.\" logger . error ( err_msg ) raise Exception ( err_msg ) if output_fn is None or not len ( output_fn ) > 0 : err_msg = f \"Invalid output filename\" logger . error ( err_msg ) raise Exception ( err_msg ) try : normalized_df = self . normalizer . normalize_datasets ( files = ds_files , cv_path = NORM_SCHEMA_DIR / conv_schema_fn ) except Exception as e : err_msg = f \"Failed to normalize file(s) - { e } \" logger . error ( err_msg ) raise Exception ( err_msg ) self . _store_data ( normalized_df , output_fn , INTERIM_DATA_DIR ) for filename in ds_files : row_vals = self . rec_mgr . get_entry_by_raw_fn ( filename ) self . rec_mgr . update ( ds_id = row_vals [ 0 ], src_url = row_vals [ 1 ], download_url = row_vals [ 2 ], raw_ds_filename = row_vals [ 3 ], normalization_schema_filename = conv_schema_fn , normalized_ds_filename = f \" { output_fn } .csv\" , ) logger . success ( f \"Successfully normalized dataset files [ { ', ' . join ( ds_files ) } ]\" ) return normalized_df # Master Dataset Creation # =================================================================================================================== def build_master_dataset ( self ): \"\"\"Takes the processed datasets, merges them and then stores the resulting dataset in the data/processed directory. Facilitates consolidation of all imported and normalized datasets into a single, master dataset and stores the master dataset in the data/processed directory. \"\"\" master_df = None for filename in os . listdir ( INTERIM_DATA_DIR ): if filename != \".gitkeep\" : _ , ext = os . path . splitext ( filename ) new_df = self . file_to_df ( INTERIM_DATA_DIR / filename , ext ) if master_df is None : master_df = new_df else : master_df = pd . concat ([ master_df , new_df ]) . dropna () self . _store_data ( data_df = master_df , filename = \"NLPinitiative_Master_Dataset\" , destpath = PROCESSED_DATA_DIR , overwrite = True , ) def pull_dataset_repo ( self , token : str ): \"\"\"Pulls the data directory from the linked Hugging Face Dataset Repository. Parameters ---------- token : str A Hugging Face token with read/write access privileges to allow importing the data. \"\"\" if token is not None : hfh . snapshot_download ( repo_id = DATASET_REPO , repo_type = \"dataset\" , local_dir = DATA_DIR , token = token ) def push_dataset_dir ( self , token : str ): \"\"\"Pushes the data directory (all dataset information) to the linked Hugging Face Dataset Repository. Parameters ---------- token : str A Hugging Face token with read/write access privileges to allow importing the data. \"\"\" if token is not None : hfh . upload_folder ( repo_id = DATASET_REPO , repo_type = \"dataset\" , folder_path = DATA_DIR , token = token ) # Data Preparation Functionality # =================================================================================================================== def prepare_and_preprocess_dataset ( self , filename : str = \"NLPinitiative_Master_Dataset.csv\" , srcdir : Path = PROCESSED_DATA_DIR , bin_model_type : str = DEF_MODEL , ml_model_type : str = DEF_MODEL , ): \"\"\"Preprocesses and tokenizes the specified dataset. Parameters ---------- filename : str, optional The file name of the file to be loaded and processed (default is 'NLPinitiative_Master_Dataset.csv'). srcdir : Path, optional The source directory of the file to be processed (default is data/processed). bin_model_type : str, optional The binary classification base model type (default is the DEF_MODEL defined in the nlpinitiative/config.py file). ml_model_type : str, optional The multilabel regression base model type (default is the DEF_MODEL defined in the nlpinitiative/config.py file). Returns ------- tuple[DatasetContainer, DatasetContainer] Two data container objects consisting of the raw dataset, encoded dataset, metadata and tokenizer for training binary and multilabel models. \"\"\" raw_dataset = self . processor . dataset_from_file ( filename , srcdir ) bin_ds , ml_ds = self . processor . bin_ml_dataset_split ( raw_dataset ) bin_ds_metadata = self . processor . get_dataset_metadata ( bin_ds ) bin_tkzr = self . processor . get_tokenizer ( bin_model_type ) bin_encoded_ds = self . processor . preprocess ( bin_ds , bin_ds_metadata [ \"labels\" ], bin_tkzr ) bin_data_obj = DatasetContainer ( bin_ds , bin_encoded_ds , bin_ds_metadata , bin_tkzr ) ml_ds_metadata = self . processor . get_dataset_metadata ( bin_ds ) ml_tkzr = self . processor . get_tokenizer ( ml_model_type ) ml_encoded_ds = self . processor . preprocess ( ml_ds , ml_ds_metadata [ \"labels\" ], ml_tkzr ) ml_data_obj = DatasetContainer ( ml_ds , ml_encoded_ds , ml_ds_metadata , ml_tkzr ) return bin_data_obj , ml_data_obj # Misc Helper Functions # =================================================================================================================== def _store_data ( self , data_df : pd . DataFrame , filename : str , destpath : Path , overwrite : bool = False ): \"\"\"Stores the specified DataFrame as a csv dataset within the data directory. Parameters ---------- data_df : DataFrame The dataset as a Pandas DataFrame object. filename : str The file name of the data to be stored. destpath : Path The path that the data is to be stored at. overwrite : bool, optional True if file with the same name should be overwritten, False if not (default is False). \"\"\" ## Handles situations of duplicate filenames appended_num = 0 corrected_filename = f \" { filename } .csv\" while not overwrite and os . path . exists ( os . path . join ( destpath , corrected_filename )): appended_num += 1 corrected_filename = f \" { filename } - { appended_num } .csv\" data_df . to_csv ( path_or_buf = os . path . join ( destpath , corrected_filename ), index = False , mode = \"w\" ) def get_dataset_statistics ( self , ds_path : Path ) -> dict : \"\"\"Generates statistics for the specified dataset. Evaluates a dataset and records various details for use in assesing if the dataset is imbalanced. Parameters ---------- dataset_path : Path The file path to the dataset. Returns ------- dict A JSON object containing the generated data. \"\"\" def get_category_details (): \"\"\"Generates statistical data for the category columns of the dataset. Evaluations include: - The sum of the categories values. - The number of rows that a given category has a non-zero value. - The number of rows where the category has a value greater than 0.5. - The number of rows where the category had the highest value among categories. Returns ------- dict A dict containing the data for all categories. \"\"\" cat_dict = dict () for cat in CATEGORY_LABELS : cat_dict [ cat ] = { \"total_combined_value\" : dataset_df [ cat ] . sum (), \"num_positive_rows\" : len ( dataset_df . loc [ dataset_df [ cat ] > 0 ]), \"num_gt_threshold\" : len ( dataset_df . loc [ dataset_df [ cat ] >= 0.5 ]), \"num_rows_as_dominant_category\" : 0 , } for _ , row in dataset_df . iterrows (): dominant_cat = None dominant_val = 0 for cat in CATEGORY_LABELS : cat_val = row [ cat ] if cat_val > dominant_val : dominant_cat = cat dominant_val = cat_val if dominant_cat is not None : cat_dict [ dominant_cat ][ \"num_rows_as_dominant_category\" ] += 1 return cat_dict dataset_df = self . file_to_df ( ds_path , \".csv\" ) json_obj = { \"total_num_entries\" : len ( dataset_df ), \"num_positive_discriminatory\" : len ( dataset_df . loc [ dataset_df [ BINARY_LABELS [ 0 ]] == 1 ]), \"num_negative_discriminatory\" : len ( dataset_df . loc [ dataset_df [ BINARY_LABELS [ 0 ]] == 0 ]), \"category_stats\" : get_category_details (), } return json_obj def remove_file ( self , filename : str , path : Path ): \"\"\"Removes the specified file. Parameters ---------- filename : str The name of the file. path : Path The path to the file. \"\"\" if os . path . exists ( path / filename ): try : os . remove ( path / filename ) logger . success ( f \"Successfully removed file, { filename } .\" ) except Exception as e : logger . error ( f \"Failed to remove file, { filename } - { e } \" ) else : logger . info ( f \"File, { filename } , does not exist.\" ) __init__ () Contructor method for instantiating a DataManager object. Source code in nlpinitiative\\data_preparation\\data_management.py 32 33 34 35 36 37 def __init__ ( self ): \"\"\"Contructor method for instantiating a DataManager object.\"\"\" self . normalizer = DataNormalizer () self . processor = DataProcessor () self . rec_mgr = DatasetRecordManager () build_master_dataset () Takes the processed datasets, merges them and then stores the resulting dataset in the data/processed directory. Facilitates consolidation of all imported and normalized datasets into a single, master dataset and stores the master dataset in the data/processed directory. Source code in nlpinitiative\\data_preparation\\data_management.py 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 def build_master_dataset ( self ): \"\"\"Takes the processed datasets, merges them and then stores the resulting dataset in the data/processed directory. Facilitates consolidation of all imported and normalized datasets into a single, master dataset and stores the master dataset in the data/processed directory. \"\"\" master_df = None for filename in os . listdir ( INTERIM_DATA_DIR ): if filename != \".gitkeep\" : _ , ext = os . path . splitext ( filename ) new_df = self . file_to_df ( INTERIM_DATA_DIR / filename , ext ) if master_df is None : master_df = new_df else : master_df = pd . concat ([ master_df , new_df ]) . dropna () self . _store_data ( data_df = master_df , filename = \"NLPinitiative_Master_Dataset\" , destpath = PROCESSED_DATA_DIR , overwrite = True , ) file_to_df ( source , ext ) Converts a dataset to a dataframe. This function also handles conversion of csv files with atypical delimiters. Parameters: source ( str ) \u2013 The file path for a local dataset or URL for a remote dataset. ext ( str ) \u2013 The file extension of the dataset. Returns: DataFrame \u2013 The dataset as a Pandas DataFrame. Source code in nlpinitiative\\data_preparation\\data_management.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def file_to_df ( self , source : str , ext : str ) -> pd . DataFrame : \"\"\"Converts a dataset to a dataframe. This function also handles conversion of csv files with atypical delimiters. Parameters ---------- source : str The file path for a local dataset or URL for a remote dataset. ext : str The file extension of the dataset. Returns ------- DataFrame The dataset as a Pandas DataFrame. \"\"\" try : match ext : case \".csv\" : try : df = pd . read_csv ( source ) except : df = pd . read_csv ( source , delimiter = \";\" ) case \".xlsx\" : df = pd . read_excel ( source ) case \".json\" : df = pd . read_json ( source ) case _ : df = None return df except Exception as e : err_msg = f \"Failed to import from source - { e } \" logger . error ( err_msg ) raise Exception ( err_msg ) get_dataset_statistics ( ds_path ) Generates statistics for the specified dataset. Evaluates a dataset and records various details for use in assesing if the dataset is imbalanced. Parameters: dataset_path ( Path ) \u2013 The file path to the dataset. Returns: dict \u2013 A JSON object containing the generated data. Source code in nlpinitiative\\data_preparation\\data_management.py 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 def get_dataset_statistics ( self , ds_path : Path ) -> dict : \"\"\"Generates statistics for the specified dataset. Evaluates a dataset and records various details for use in assesing if the dataset is imbalanced. Parameters ---------- dataset_path : Path The file path to the dataset. Returns ------- dict A JSON object containing the generated data. \"\"\" def get_category_details (): \"\"\"Generates statistical data for the category columns of the dataset. Evaluations include: - The sum of the categories values. - The number of rows that a given category has a non-zero value. - The number of rows where the category has a value greater than 0.5. - The number of rows where the category had the highest value among categories. Returns ------- dict A dict containing the data for all categories. \"\"\" cat_dict = dict () for cat in CATEGORY_LABELS : cat_dict [ cat ] = { \"total_combined_value\" : dataset_df [ cat ] . sum (), \"num_positive_rows\" : len ( dataset_df . loc [ dataset_df [ cat ] > 0 ]), \"num_gt_threshold\" : len ( dataset_df . loc [ dataset_df [ cat ] >= 0.5 ]), \"num_rows_as_dominant_category\" : 0 , } for _ , row in dataset_df . iterrows (): dominant_cat = None dominant_val = 0 for cat in CATEGORY_LABELS : cat_val = row [ cat ] if cat_val > dominant_val : dominant_cat = cat dominant_val = cat_val if dominant_cat is not None : cat_dict [ dominant_cat ][ \"num_rows_as_dominant_category\" ] += 1 return cat_dict dataset_df = self . file_to_df ( ds_path , \".csv\" ) json_obj = { \"total_num_entries\" : len ( dataset_df ), \"num_positive_discriminatory\" : len ( dataset_df . loc [ dataset_df [ BINARY_LABELS [ 0 ]] == 1 ]), \"num_negative_discriminatory\" : len ( dataset_df . loc [ dataset_df [ BINARY_LABELS [ 0 ]] == 0 ]), \"category_stats\" : get_category_details (), } return json_obj import_data ( import_type , source , dataset_name , is_third_party = True , local_ds_ref_url = None , overwrite = False ) Imports data from a local (by file path) or remote (ny URL) source. Parameters: import_type ( str ) \u2013 'local' if from a local source, 'external' if from remote source. source ( str ) \u2013 The file path or URL of the dataset. dataset_name ( str ) \u2013 The name used to id the dataset. is_third_party ( bool , default: True ) \u2013 True if 3rd party dataset, False if not (default is True). local_ds_ref_url ( str , default: None ) \u2013 Reference URL for datasets imported locally (default is None). overwrite ( bool , default: False ) \u2013 True if files with the same name should be overwritten, False if not (default is False). Returns: DataFrame \u2013 The dataset as a Pandas DataFrame. Source code in nlpinitiative\\data_preparation\\data_management.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def import_data ( self , import_type : str , source , dataset_name : str , is_third_party : bool = True , local_ds_ref_url : bool = None , overwrite : bool = False , ) -> pd . DataFrame : \"\"\"Imports data from a local (by file path) or remote (ny URL) source. Parameters ---------- import_type : str 'local' if from a local source, 'external' if from remote source. source : str The file path or URL of the dataset. dataset_name : str The name used to id the dataset. is_third_party : bool, optional True if 3rd party dataset, False if not (default is True). local_ds_ref_url : str, optional Reference URL for datasets imported locally (default is None). overwrite : bool, optional True if files with the same name should be overwritten, False if not (default is False). Returns ------- DataFrame The dataset as a Pandas DataFrame. \"\"\" ref_url = None formatted_url = None match import_type : case \"local\" : if source is None or not os . path . exists ( source ): err_msg = \"Dataset filepath does not exist\" logger . error ( err_msg ) raise Exception ( err_msg ) if is_third_party and not self . _is_valid_url ( local_ds_ref_url ): err_msg = ( \"Locally imported 3rd party dataset imports must include a reference URL.\" ) logger . error ( err_msg ) raise Exception ( err_msg ) ref_url = local_ds_ref_url if is_third_party else \"Custom Created Dataset\" tail = os . path . split ( source )[ - 1 ] filename , ext = os . path . splitext ( tail )[ - 2 :] if ext not in ACCEPTED_FILE_FORMATS : err_msg = \"Unsupported file type\" logger . error ( err_msg ) raise Exception ( err_msg ) src = source case \"external\" : if not self . _is_valid_url ( source ): err_msg = \"Invalid URL\" logger . error ( err_msg ) raise Exception ( err_msg ) ref_url = source formatted_url = self . _format_url ( ref_url ) filename = self . _generate_import_filename ( formatted_url ) path = urlparse ( formatted_url ) . path ext = os . path . splitext ( path )[ 1 ] if ext not in ACCEPTED_FILE_FORMATS : err_msg = \"Unsupported file type\" logger . error ( err_msg ) raise Exception ( err_msg ) src = formatted_url case _ : err_msg = \"Invalid import type\" logger . error ( err_msg ) raise Exception ( err_msg ) ds_df = self . file_to_df ( src , ext ) if self . rec_mgr . dataset_src_exists ( ref_url ): return ds_df self . _store_data ( data_df = ds_df , filename = filename , destpath = RAW_DATA_DIR , overwrite = overwrite ) self . rec_mgr . update ( ds_id = dataset_name , src_url = ref_url , download_url = formatted_url , raw_ds_filename = f \" { filename } .csv\" , ) logger . success ( f \"Successfully imported { import_type } dataset from { source } .\" ) return ds_df normalize_dataset ( ds_files , conv_schema_fn , output_fn ) Handles normalization of a third-party dataset to the schema used for training our models. Parameters: ds_files ( list [ str ] ) \u2013 One or more dataset files to merge and normalize. conv_schema_fn ( str ) \u2013 The file name of the json conversion schema used. output_fn ( str ) \u2013 The output filename to be used. Returns: DataFrame \u2013 True if it exists, False if it does not. Source code in nlpinitiative\\data_preparation\\data_management.py 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 def normalize_dataset ( self , ds_files : list [ str ], conv_schema_fn : str , output_fn : str ) -> pd . DataFrame : \"\"\"Handles normalization of a third-party dataset to the schema used for training our models. Parameters ---------- ds_files : list[str] One or more dataset files to merge and normalize. conv_schema_fn : str The file name of the json conversion schema used. output_fn : str The output filename to be used. Returns ------- DataFrame True if it exists, False if it does not. \"\"\" for filename in ds_files : if not self . _valid_filepath (( RAW_DATA_DIR / filename )): err_msg = f \"Invalid filepath for file, { filename } [ { RAW_DATA_DIR / filename } ].\" logger . error ( err_msg ) raise Exception ( err_msg ) if not self . _valid_filepath ( NORM_SCHEMA_DIR / conv_schema_fn ): err_msg = f \"Normalization schema file, { conv_schema_fn } , does not exist.\" logger . error ( err_msg ) raise Exception ( err_msg ) if output_fn is None or not len ( output_fn ) > 0 : err_msg = f \"Invalid output filename\" logger . error ( err_msg ) raise Exception ( err_msg ) try : normalized_df = self . normalizer . normalize_datasets ( files = ds_files , cv_path = NORM_SCHEMA_DIR / conv_schema_fn ) except Exception as e : err_msg = f \"Failed to normalize file(s) - { e } \" logger . error ( err_msg ) raise Exception ( err_msg ) self . _store_data ( normalized_df , output_fn , INTERIM_DATA_DIR ) for filename in ds_files : row_vals = self . rec_mgr . get_entry_by_raw_fn ( filename ) self . rec_mgr . update ( ds_id = row_vals [ 0 ], src_url = row_vals [ 1 ], download_url = row_vals [ 2 ], raw_ds_filename = row_vals [ 3 ], normalization_schema_filename = conv_schema_fn , normalized_ds_filename = f \" { output_fn } .csv\" , ) logger . success ( f \"Successfully normalized dataset files [ { ', ' . join ( ds_files ) } ]\" ) return normalized_df prepare_and_preprocess_dataset ( filename = 'NLPinitiative_Master_Dataset.csv' , srcdir = PROCESSED_DATA_DIR , bin_model_type = DEF_MODEL , ml_model_type = DEF_MODEL ) Preprocesses and tokenizes the specified dataset. Parameters: filename ( str , default: 'NLPinitiative_Master_Dataset.csv' ) \u2013 The file name of the file to be loaded and processed (default is 'NLPinitiative_Master_Dataset.csv'). srcdir ( Path , default: PROCESSED_DATA_DIR ) \u2013 The source directory of the file to be processed (default is data/processed). bin_model_type ( str , default: DEF_MODEL ) \u2013 The binary classification base model type (default is the DEF_MODEL defined in the nlpinitiative/config.py file). ml_model_type ( str , default: DEF_MODEL ) \u2013 The multilabel regression base model type (default is the DEF_MODEL defined in the nlpinitiative/config.py file). Returns: tuple [ DatasetContainer , DatasetContainer ] \u2013 Two data container objects consisting of the raw dataset, encoded dataset, metadata and tokenizer for training binary and multilabel models. Source code in nlpinitiative\\data_preparation\\data_management.py 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 def prepare_and_preprocess_dataset ( self , filename : str = \"NLPinitiative_Master_Dataset.csv\" , srcdir : Path = PROCESSED_DATA_DIR , bin_model_type : str = DEF_MODEL , ml_model_type : str = DEF_MODEL , ): \"\"\"Preprocesses and tokenizes the specified dataset. Parameters ---------- filename : str, optional The file name of the file to be loaded and processed (default is 'NLPinitiative_Master_Dataset.csv'). srcdir : Path, optional The source directory of the file to be processed (default is data/processed). bin_model_type : str, optional The binary classification base model type (default is the DEF_MODEL defined in the nlpinitiative/config.py file). ml_model_type : str, optional The multilabel regression base model type (default is the DEF_MODEL defined in the nlpinitiative/config.py file). Returns ------- tuple[DatasetContainer, DatasetContainer] Two data container objects consisting of the raw dataset, encoded dataset, metadata and tokenizer for training binary and multilabel models. \"\"\" raw_dataset = self . processor . dataset_from_file ( filename , srcdir ) bin_ds , ml_ds = self . processor . bin_ml_dataset_split ( raw_dataset ) bin_ds_metadata = self . processor . get_dataset_metadata ( bin_ds ) bin_tkzr = self . processor . get_tokenizer ( bin_model_type ) bin_encoded_ds = self . processor . preprocess ( bin_ds , bin_ds_metadata [ \"labels\" ], bin_tkzr ) bin_data_obj = DatasetContainer ( bin_ds , bin_encoded_ds , bin_ds_metadata , bin_tkzr ) ml_ds_metadata = self . processor . get_dataset_metadata ( bin_ds ) ml_tkzr = self . processor . get_tokenizer ( ml_model_type ) ml_encoded_ds = self . processor . preprocess ( ml_ds , ml_ds_metadata [ \"labels\" ], ml_tkzr ) ml_data_obj = DatasetContainer ( ml_ds , ml_encoded_ds , ml_ds_metadata , ml_tkzr ) return bin_data_obj , ml_data_obj pull_dataset_repo ( token ) Pulls the data directory from the linked Hugging Face Dataset Repository. Parameters: token ( str ) \u2013 A Hugging Face token with read/write access privileges to allow importing the data. Source code in nlpinitiative\\data_preparation\\data_management.py 377 378 379 380 381 382 383 384 385 386 387 388 389 def pull_dataset_repo ( self , token : str ): \"\"\"Pulls the data directory from the linked Hugging Face Dataset Repository. Parameters ---------- token : str A Hugging Face token with read/write access privileges to allow importing the data. \"\"\" if token is not None : hfh . snapshot_download ( repo_id = DATASET_REPO , repo_type = \"dataset\" , local_dir = DATA_DIR , token = token ) push_dataset_dir ( token ) Pushes the data directory (all dataset information) to the linked Hugging Face Dataset Repository. Parameters: token ( str ) \u2013 A Hugging Face token with read/write access privileges to allow importing the data. Source code in nlpinitiative\\data_preparation\\data_management.py 391 392 393 394 395 396 397 398 399 400 401 402 403 def push_dataset_dir ( self , token : str ): \"\"\"Pushes the data directory (all dataset information) to the linked Hugging Face Dataset Repository. Parameters ---------- token : str A Hugging Face token with read/write access privileges to allow importing the data. \"\"\" if token is not None : hfh . upload_folder ( repo_id = DATASET_REPO , repo_type = \"dataset\" , folder_path = DATA_DIR , token = token ) remove_file ( filename , path ) Removes the specified file. Parameters: filename ( str ) \u2013 The name of the file. path ( Path ) \u2013 The path to the file. Source code in nlpinitiative\\data_preparation\\data_management.py 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 def remove_file ( self , filename : str , path : Path ): \"\"\"Removes the specified file. Parameters ---------- filename : str The name of the file. path : Path The path to the file. \"\"\" if os . path . exists ( path / filename ): try : os . remove ( path / filename ) logger . success ( f \"Successfully removed file, { filename } .\" ) except Exception as e : logger . error ( f \"Failed to remove file, { filename } - { e } \" ) else : logger . info ( f \"File, { filename } , does not exist.\" ) DatasetContainer A class used for organizing dataset information used within model training. Source code in nlpinitiative\\data_preparation\\data_management.py 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 class DatasetContainer : \"\"\"A class used for organizing dataset information used within model training.\"\"\" def __init__ ( self , dataset : DatasetDict , encoded_ds : DatasetDict , metadata : dict , tokenizer : PreTrainedTokenizer | PreTrainedTokenizerFast , ): \"\"\"Constuctor for instantiating DatasetContainer. Parameters ---------- dataset : DatasetDict The raw dataset (before tokenization). encoded_ds : DatasetDict The encoded dataset. metadata : dict The metadata associated with the dataset. tokenizer : PreTrainedTokenizer | PreTrainedTokenizerFast The tokenizer used for tokenizing the dataset. \"\"\" self . _raw_dataset = dataset self . _encoded_dataset = encoded_ds self . _labels = metadata [ \"labels\" ] self . _lbl2idx = metadata [ \"lbl2idx\" ] self . _idx2lbl = metadata [ \"idx2lbl\" ] self . _tkzr = tokenizer @property def raw_dataset ( self ): \"\"\"The unencoded dataset.\"\"\" return self . _raw_dataset @property def encoded_dataset ( self ): \"\"\"The encoded/tokenized dataset.\"\"\" return self . _encoded_dataset @property def labels ( self ): \"\"\"The labels used within the dataset.\"\"\" return self . _labels @property def lbl2idx ( self ): \"\"\"Dict for mapping label indices to the label names.\"\"\" return self . _lbl2idx @property def idx2lbl ( self ): \"\"\"Dict for mapping label names to the label indices.\"\"\" return self . _idx2lbl @property def tokenizer ( self ): \"\"\"The tokenizer for the dataset.\"\"\" return self . _tkzr encoded_dataset property The encoded/tokenized dataset. idx2lbl property Dict for mapping label names to the label indices. labels property The labels used within the dataset. lbl2idx property Dict for mapping label indices to the label names. raw_dataset property The unencoded dataset. tokenizer property The tokenizer for the dataset. __init__ ( dataset , encoded_ds , metadata , tokenizer ) Constuctor for instantiating DatasetContainer. Parameters: dataset ( DatasetDict ) \u2013 The raw dataset (before tokenization). encoded_ds ( DatasetDict ) \u2013 The encoded dataset. metadata ( dict ) \u2013 The metadata associated with the dataset. tokenizer ( PreTrainedTokenizer | PreTrainedTokenizerFast ) \u2013 The tokenizer used for tokenizing the dataset. Source code in nlpinitiative\\data_preparation\\data_management.py 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 def __init__ ( self , dataset : DatasetDict , encoded_ds : DatasetDict , metadata : dict , tokenizer : PreTrainedTokenizer | PreTrainedTokenizerFast , ): \"\"\"Constuctor for instantiating DatasetContainer. Parameters ---------- dataset : DatasetDict The raw dataset (before tokenization). encoded_ds : DatasetDict The encoded dataset. metadata : dict The metadata associated with the dataset. tokenizer : PreTrainedTokenizer | PreTrainedTokenizerFast The tokenizer used for tokenizing the dataset. \"\"\" self . _raw_dataset = dataset self . _encoded_dataset = encoded_ds self . _labels = metadata [ \"labels\" ] self . _lbl2idx = metadata [ \"lbl2idx\" ] self . _idx2lbl = metadata [ \"idx2lbl\" ] self . _tkzr = tokenizer DatasetRecordManager A class for managing and maintaining a record of the datasets imported and used for model training. Source code in nlpinitiative\\data_preparation\\data_management.py 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 class DatasetRecordManager : \"\"\"A class for managing and maintaining a record of the datasets imported and used for model training.\"\"\" def __init__ ( self ): \"\"\"Constructor for instantiating a DatasetRecordManager object.\"\"\" self . rec_df : pd . DataFrame = self . load_dataset_record () def load_dataset_record ( self ) -> pd . DataFrame : \"\"\"Loads the dataset record into a Pandas Dataframe. Returns ------- DataFrame The record of datasets as a DataFrame. \"\"\" if os . path . exists ( DATA_DIR / \"dataset_record.csv\" ): ds_rec_df = pd . read_csv ( filepath_or_buffer = DATA_DIR / \"dataset_record.csv\" ) else : ds_rec_df = pd . DataFrame ( columns = [ \"Dataset ID\" , \"Dataset Reference URL\" , \"Dataset Download URL\" , \"Raw Dataset Filename\" , \"Conversion Schema Filename\" , \"Converted Filename\" , ] ) ds_rec_df . to_csv ( DATA_DIR / \"dataset_record.csv\" , index = False ) return ds_rec_df def save_ds_record ( self ): \"\"\"Faciliates saving the current state of the dataset record as a csv file.\"\"\" self . rec_df . to_csv ( DATA_DIR / \"dataset_record.csv\" , index = False ) def dataset_src_exists ( self , src_url : str ) -> bool : \"\"\"Checks if a dataset record exists with the specified URL. Parameters ---------- src_url : str The source/reference URL of the dataset. Returns ------- bool True if the record exists, False otherwise. \"\"\" return len ( self . rec_df [ self . rec_df [ \"Dataset Reference URL\" ] == src_url ]) > 0 def get_ds_record_copy ( self ) -> pd . DataFrame : \"\"\"Returns a copy of the dataset record dataframe Returns ------- DataFrame A copy of the record as a Pandas DataFrame. \"\"\" return self . rec_df . copy ( deep = True ) def update ( self , ds_id : str , src_url : str = None , download_url : str = None , raw_ds_filename : str = None , normalization_schema_filename : str = None , normalized_ds_filename : str = None , ): \"\"\"Adds a new dataset record or updates an existing record. Parameters ---------- ds_id : str The name/id of the dataset (used for tracking purposes). src_url : str, optional The source or reference URL for an imported dataset (default is None). download_url : str, optional The download URL of the dataset (default is None). raw_ds_filename : str, optional The file name of the raw (non-normalized) dataset (default is None). normalization_schema_filename : str, optional The file name of the conversion schema used to normalize the dataset (default is None). normalized_ds_filename : str, optional The file name of the normalized version of the dataset (default is None). \"\"\" if ds_id is not None : new_df = pd . DataFrame ( { \"Dataset ID\" : [ ds_id ], \"Dataset Reference URL\" : [ src_url ], \"Dataset Download URL\" : [ download_url ], \"Raw Dataset Filename\" : [ raw_ds_filename ], \"Conversion Schema Filename\" : [ normalization_schema_filename ], \"Converted Filename\" : [ normalized_ds_filename ], } ) temp_df = pd . concat ([ self . rec_df , new_df ]) . drop_duplicates ( subset = [ \"Dataset ID\" , \"Dataset Reference URL\" ], keep = \"last\" ) self . rec_df = temp_df self . save_ds_record () def remove_entry ( self , ds_id : str ): \"\"\"Facilitates removal of a dataset record. Parameters ---------- ds_id : str The name/id of the dataset to be removed. \"\"\" if ds_id is not None : self . rec_df = self . rec_df [ self . rec_df [ \"Dataset ID\" ] != ds_id ] self . save_ds_record () def get_entry_by_raw_fn ( self , filename : str ) -> tuple : \"\"\"Retrieves the raw file name of the specified dataset record. Parameters ---------- filename : str The file name of the normalized version of the dataset. Raises ------ Exception If the method failed to retrieve the specified record. Returns ------- str The file name of the non-normalized version of the dataset. \"\"\" try : row = tuple ( self . rec_df [ self . rec_df [ \"Raw Dataset Filename\" ] == filename ] . values [ 0 ]) return row except Exception as e : err_msg = f \"Failed to retrieve row of Dataset Record - { e } \" logger . error ( err_msg ) raise Exception ( err_msg ) __init__ () Constructor for instantiating a DatasetRecordManager object. Source code in nlpinitiative\\data_preparation\\data_management.py 635 636 637 def __init__ ( self ): \"\"\"Constructor for instantiating a DatasetRecordManager object.\"\"\" self . rec_df : pd . DataFrame = self . load_dataset_record () dataset_src_exists ( src_url ) Checks if a dataset record exists with the specified URL. Parameters: src_url ( str ) \u2013 The source/reference URL of the dataset. Returns: bool \u2013 True if the record exists, False otherwise. Source code in nlpinitiative\\data_preparation\\data_management.py 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 def dataset_src_exists ( self , src_url : str ) -> bool : \"\"\"Checks if a dataset record exists with the specified URL. Parameters ---------- src_url : str The source/reference URL of the dataset. Returns ------- bool True if the record exists, False otherwise. \"\"\" return len ( self . rec_df [ self . rec_df [ \"Dataset Reference URL\" ] == src_url ]) > 0 get_ds_record_copy () Returns a copy of the dataset record dataframe Returns: DataFrame \u2013 A copy of the record as a Pandas DataFrame. Source code in nlpinitiative\\data_preparation\\data_management.py 685 686 687 688 689 690 691 692 693 694 def get_ds_record_copy ( self ) -> pd . DataFrame : \"\"\"Returns a copy of the dataset record dataframe Returns ------- DataFrame A copy of the record as a Pandas DataFrame. \"\"\" return self . rec_df . copy ( deep = True ) get_entry_by_raw_fn ( filename ) Retrieves the raw file name of the specified dataset record. Parameters: filename ( str ) \u2013 The file name of the normalized version of the dataset. Raises: Exception \u2013 If the method failed to retrieve the specified record. Returns: str \u2013 The file name of the non-normalized version of the dataset. Source code in nlpinitiative\\data_preparation\\data_management.py 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 def get_entry_by_raw_fn ( self , filename : str ) -> tuple : \"\"\"Retrieves the raw file name of the specified dataset record. Parameters ---------- filename : str The file name of the normalized version of the dataset. Raises ------ Exception If the method failed to retrieve the specified record. Returns ------- str The file name of the non-normalized version of the dataset. \"\"\" try : row = tuple ( self . rec_df [ self . rec_df [ \"Raw Dataset Filename\" ] == filename ] . values [ 0 ]) return row except Exception as e : err_msg = f \"Failed to retrieve row of Dataset Record - { e } \" logger . error ( err_msg ) raise Exception ( err_msg ) load_dataset_record () Loads the dataset record into a Pandas Dataframe. Returns: DataFrame \u2013 The record of datasets as a DataFrame. Source code in nlpinitiative\\data_preparation\\data_management.py 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 def load_dataset_record ( self ) -> pd . DataFrame : \"\"\"Loads the dataset record into a Pandas Dataframe. Returns ------- DataFrame The record of datasets as a DataFrame. \"\"\" if os . path . exists ( DATA_DIR / \"dataset_record.csv\" ): ds_rec_df = pd . read_csv ( filepath_or_buffer = DATA_DIR / \"dataset_record.csv\" ) else : ds_rec_df = pd . DataFrame ( columns = [ \"Dataset ID\" , \"Dataset Reference URL\" , \"Dataset Download URL\" , \"Raw Dataset Filename\" , \"Conversion Schema Filename\" , \"Converted Filename\" , ] ) ds_rec_df . to_csv ( DATA_DIR / \"dataset_record.csv\" , index = False ) return ds_rec_df remove_entry ( ds_id ) Facilitates removal of a dataset record. Parameters: ds_id ( str ) \u2013 The name/id of the dataset to be removed. Source code in nlpinitiative\\data_preparation\\data_management.py 743 744 745 746 747 748 749 750 751 752 753 754 def remove_entry ( self , ds_id : str ): \"\"\"Facilitates removal of a dataset record. Parameters ---------- ds_id : str The name/id of the dataset to be removed. \"\"\" if ds_id is not None : self . rec_df = self . rec_df [ self . rec_df [ \"Dataset ID\" ] != ds_id ] self . save_ds_record () save_ds_record () Faciliates saving the current state of the dataset record as a csv file. Source code in nlpinitiative\\data_preparation\\data_management.py 664 665 666 667 def save_ds_record ( self ): \"\"\"Faciliates saving the current state of the dataset record as a csv file.\"\"\" self . rec_df . to_csv ( DATA_DIR / \"dataset_record.csv\" , index = False ) update ( ds_id , src_url = None , download_url = None , raw_ds_filename = None , normalization_schema_filename = None , normalized_ds_filename = None ) Adds a new dataset record or updates an existing record. Parameters: ds_id ( str ) \u2013 The name/id of the dataset (used for tracking purposes). src_url ( str , default: None ) \u2013 The source or reference URL for an imported dataset (default is None). download_url ( str , default: None ) \u2013 The download URL of the dataset (default is None). raw_ds_filename ( str , default: None ) \u2013 The file name of the raw (non-normalized) dataset (default is None). normalization_schema_filename ( str , default: None ) \u2013 The file name of the conversion schema used to normalize the dataset (default is None). normalized_ds_filename ( str , default: None ) \u2013 The file name of the normalized version of the dataset (default is None). Source code in nlpinitiative\\data_preparation\\data_management.py 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 def update ( self , ds_id : str , src_url : str = None , download_url : str = None , raw_ds_filename : str = None , normalization_schema_filename : str = None , normalized_ds_filename : str = None , ): \"\"\"Adds a new dataset record or updates an existing record. Parameters ---------- ds_id : str The name/id of the dataset (used for tracking purposes). src_url : str, optional The source or reference URL for an imported dataset (default is None). download_url : str, optional The download URL of the dataset (default is None). raw_ds_filename : str, optional The file name of the raw (non-normalized) dataset (default is None). normalization_schema_filename : str, optional The file name of the conversion schema used to normalize the dataset (default is None). normalized_ds_filename : str, optional The file name of the normalized version of the dataset (default is None). \"\"\" if ds_id is not None : new_df = pd . DataFrame ( { \"Dataset ID\" : [ ds_id ], \"Dataset Reference URL\" : [ src_url ], \"Dataset Download URL\" : [ download_url ], \"Raw Dataset Filename\" : [ raw_ds_filename ], \"Conversion Schema Filename\" : [ normalization_schema_filename ], \"Converted Filename\" : [ normalized_ds_filename ], } ) temp_df = pd . concat ([ self . rec_df , new_df ]) . drop_duplicates ( subset = [ \"Dataset ID\" , \"Dataset Reference URL\" ], keep = \"last\" ) self . rec_df = temp_df self . save_ds_record () nlpinitiative.data_preparation.data_normalize Script file used for facillitating normalization of a third-party dataset(s) into the format that we will utilize for training the model. This script can also handle merging complimentary datasets (datasets that come from the same source that may have minor differences in labeling scheme). DataNormalizer A class used to normalize datasets to the format utilized for model training. Source code in nlpinitiative\\data_preparation\\data_normalize.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 class DataNormalizer : \"\"\"A class used to normalize datasets to the format utilized for model training.\"\"\" def _merge_dataframes ( self , df1 : pd . DataFrame , df2 : pd . DataFrame , merge_col : str ) -> pd . DataFrame : \"\"\"Merges two Pandas DataFrames. Parameters ---------- df1 : DataFrame DataFrame to be merged. df2 : DataFrame DataFrame to be merged. merge_col : str The column for which the DataFrames are to be merged on. Returns ------- DataFrame The resulting merged DataFrame. \"\"\" new_df = pd . merge ( df1 , df2 , on = merge_col , how = \"left\" ) . fillna ( 0.0 ) return new_df def _load_src_file ( self , path : Path ) -> pd . DataFrame : \"\"\"Load a source file into a Pandas DataFrame. Parameters ---------- path : Path The file path to the source file. Returns ------- DataFrame The source file data in a DataFrame. \"\"\" return pd . read_csv ( path ) def _load_conv_schema ( self , path : Path ) -> dict [ str : str ]: \"\"\"Loads the dataset conversion schema that will be used for normalizing a dataset. Parameters ---------- path : Path The file path to the JSON conversion schema. Returns ------- dict[str:str] The conversion schema as a JSON object. \"\"\" return json . load ( open ( path , \"r\" )) def normalize_datasets ( self , files : list [ Path ], cv_path : Path ) -> pd . DataFrame : \"\"\"Loads, merges and normalizes one or more datasets to the schema used for model training. Parameters ---------- files : list[Path] A list of dataset file paths that are to be merged and normalized. cv_path : Path The file path to the JSON conversion schema. Returns ------- Dataframe The normalized dataset(s) as a DataFrame. \"\"\" num_files = len ( files ) src_df = None for i in range ( 0 , num_files ): df = self . _load_src_file ( RAW_DATA_DIR / files [ i ]) if src_df is not None : src_df = self . _merge_dataframes ( src_df , df , data_col_name ) else : src_df = df conv_scema = self . _load_conv_schema ( cv_path ) data_col_name = conv_scema [ \"data_col\" ] if conv_scema [ \"mapping_type\" ] == \"many2many\" : schema_cats = conv_scema [ \"column_mapping\" ] . keys () master_df = pd . DataFrame ( data = [], columns = DATASET_COLS ) master_df [ DATASET_COLS [ 0 ]] = src_df [ data_col_name ] for cat in schema_cats : from_columns = conv_scema [ \"column_mapping\" ][ cat ] if len ( from_columns ) > 0 : if cat == DATASET_COLS [ 1 ]: master_df [ cat ] = src_df [ from_columns ] . gt ( 0.0 ) . astype ( pd . Int64Dtype ()) else : master_df [ cat ] = src_df [ from_columns ] . sum ( axis = 1 ) else : master_df [ cat ] = 0.0 cols = master_df . columns for col in cols : if \"unnamed\" in col . lower (): master_df . drop ( col ) indices_to_purge = [] for index , row in master_df . iterrows (): is_hate = row [ DATASET_COLS [ 1 ]] == 1 has_cat_values = row [ CATEGORY_LABELS ] . sum () > 0.0 if is_hate and not has_cat_values or not is_hate and has_cat_values : indices_to_purge . append ( index ) master_df . drop ( master_df . index [ indices_to_purge ], inplace = True ) else : source_column = conv_scema [ \"single_column_label\" ] cat_mapping = conv_scema [ \"column_mapping\" ] data = [] for _ , row in src_df . iterrows (): row_data = [] text = row [ data_col_name ] type_discr = row [ source_column ] row_data . append ( text ) row_data . append ( 1 ) groups = [] for cat in cat_mapping . keys (): if type_discr in cat_mapping [ cat ]: if cat not in groups : groups . append ( cat ) try : val_breakdown = 1 / len ( groups ) for cat in cat_mapping . keys (): val = 0.0 if cat in groups : val = val_breakdown row_data . append ( val ) data . append ( row_data ) except : pass master_df = pd . DataFrame ( data = data , columns = DATASET_COLS ) return master_df normalize_datasets ( files , cv_path ) Loads, merges and normalizes one or more datasets to the schema used for model training. Parameters: files ( list [ Path ] ) \u2013 A list of dataset file paths that are to be merged and normalized. cv_path ( Path ) \u2013 The file path to the JSON conversion schema. Returns: Dataframe \u2013 The normalized dataset(s) as a DataFrame. Source code in nlpinitiative\\data_preparation\\data_normalize.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def normalize_datasets ( self , files : list [ Path ], cv_path : Path ) -> pd . DataFrame : \"\"\"Loads, merges and normalizes one or more datasets to the schema used for model training. Parameters ---------- files : list[Path] A list of dataset file paths that are to be merged and normalized. cv_path : Path The file path to the JSON conversion schema. Returns ------- Dataframe The normalized dataset(s) as a DataFrame. \"\"\" num_files = len ( files ) src_df = None for i in range ( 0 , num_files ): df = self . _load_src_file ( RAW_DATA_DIR / files [ i ]) if src_df is not None : src_df = self . _merge_dataframes ( src_df , df , data_col_name ) else : src_df = df conv_scema = self . _load_conv_schema ( cv_path ) data_col_name = conv_scema [ \"data_col\" ] if conv_scema [ \"mapping_type\" ] == \"many2many\" : schema_cats = conv_scema [ \"column_mapping\" ] . keys () master_df = pd . DataFrame ( data = [], columns = DATASET_COLS ) master_df [ DATASET_COLS [ 0 ]] = src_df [ data_col_name ] for cat in schema_cats : from_columns = conv_scema [ \"column_mapping\" ][ cat ] if len ( from_columns ) > 0 : if cat == DATASET_COLS [ 1 ]: master_df [ cat ] = src_df [ from_columns ] . gt ( 0.0 ) . astype ( pd . Int64Dtype ()) else : master_df [ cat ] = src_df [ from_columns ] . sum ( axis = 1 ) else : master_df [ cat ] = 0.0 cols = master_df . columns for col in cols : if \"unnamed\" in col . lower (): master_df . drop ( col ) indices_to_purge = [] for index , row in master_df . iterrows (): is_hate = row [ DATASET_COLS [ 1 ]] == 1 has_cat_values = row [ CATEGORY_LABELS ] . sum () > 0.0 if is_hate and not has_cat_values or not is_hate and has_cat_values : indices_to_purge . append ( index ) master_df . drop ( master_df . index [ indices_to_purge ], inplace = True ) else : source_column = conv_scema [ \"single_column_label\" ] cat_mapping = conv_scema [ \"column_mapping\" ] data = [] for _ , row in src_df . iterrows (): row_data = [] text = row [ data_col_name ] type_discr = row [ source_column ] row_data . append ( text ) row_data . append ( 1 ) groups = [] for cat in cat_mapping . keys (): if type_discr in cat_mapping [ cat ]: if cat not in groups : groups . append ( cat ) try : val_breakdown = 1 / len ( groups ) for cat in cat_mapping . keys (): val = 0.0 if cat in groups : val = val_breakdown row_data . append ( val ) data . append ( row_data ) except : pass master_df = pd . DataFrame ( data = data , columns = DATASET_COLS ) return master_df nlpinitiative.data_preparation.data_process Script file used for facillitating dataset preparation and preprocessing for use in model training. DataProcessor A class used for performing preprocessing/tokenization. Source code in nlpinitiative\\data_preparation\\data_process.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 class DataProcessor : \"\"\"A class used for performing preprocessing/tokenization.\"\"\" def dataset_from_file ( self , filename : str , srcdir : Path = PROCESSED_DATA_DIR ) -> DatasetDict : \"\"\"Loads a dataset from a specified file into a Dataset object. Parameters ---------- filename : str The file name of the dataset to be loaded. srcdir : Path, optional The file path to the directory that the dataset is stored (default is data/processed). Raises ------ Exception If the file specified does not exist. Returns ------- DatasetDict The loaded dataset as a DatasetDict object. \"\"\" if filename and os . path . exists ( os . path . join ( srcdir , filename )): ext = os . path . splitext ( filename )[ - 1 ] ext = ext . replace ( \".\" , \"\" ) ds = load_dataset ( ext , data_files = os . path . join ( srcdir , filename ), split = \"train\" ) . train_test_split ( test_size = TRAIN_TEST_SPLIT ) return ds else : raise Exception ( \"Invalid file name or file path\" ) def bin_ml_dataset_split ( self , dataset : Dataset ) -> tuple [ DatasetDict , DatasetDict ]: \"\"\"Forms train/test split for use in the binary classification and multilabel regression models training. Takes a loaded dataset and splits it into two separate datasets with the required corresponding labels (the binary model dataset will only have the DISCRIMINATORY column, while the multilabel regression model will only have the columns consisting of the discrimination categories). Additionally, the function will split the prepared datasets into a train/test split. Parameters ---------- dataset : DatasetDict The dataset to be processed. Returns ------- DatasetDict The prepared binary model and multilabel model dataset objects. \"\"\" def get_bin_ds () -> DatasetDict : \"\"\"Prepares the binary model dataset. Returns ------- DatasetDict The prepared binary model dataset. \"\"\" train = dataset [ \"train\" ] . remove_columns ( CATEGORY_LABELS ) test = dataset [ \"test\" ] . remove_columns ( CATEGORY_LABELS ) return DatasetDict ( { \"train\" : train . rename_column ( \"DISCRIMINATORY\" , \"label\" ), \"test\" : test . rename_column ( \"DISCRIMINATORY\" , \"label\" ), } ) def get_ml_regr_ds () -> DatasetDict : \"\"\"Prepares the multilabel model dataset. Returns ------- DatasetDict The prepared multilabel model dataset. \"\"\" def combine_labels ( ex_ds ): \"\"\"Consolidates the mutiple dataset columns for categories into a single column consisting of a list of the category data. Parameters ---------- ex_ds : DatasetDict The multilabel dataset to be corrected. Returns ------- DatasetDict The corrected dataset. \"\"\" ex_ds [ \"labels\" ] = [ float ( ex_ds [ \"GENDER\" ]), float ( ex_ds [ \"RACE\" ]), float ( ex_ds [ \"SEXUALITY\" ]), float ( ex_ds [ \"DISABILITY\" ]), float ( ex_ds [ \"RELIGION\" ]), float ( ex_ds [ \"UNSPECIFIED\" ]), ] return ex_ds train = dataset [ \"train\" ] . remove_columns ( BINARY_LABELS ) train = train . map ( combine_labels ) train = train . remove_columns ( CATEGORY_LABELS ) test = dataset [ \"test\" ] . remove_columns ( BINARY_LABELS ) test = test . map ( combine_labels ) test = test . remove_columns ( CATEGORY_LABELS ) return DatasetDict ({ \"train\" : train , \"test\" : test }) return get_bin_ds (), get_ml_regr_ds () def get_tokenizer ( self , model_type : str = DEF_MODEL ): \"\"\"Generates a tokenizer for the specified model type. Parameters ---------- model_type : str, optional The model type for which the tokenizer is to be created. Returns ------- PreTrainedTokenizer | PreTrainedTokenizerFast The tokenizer object. \"\"\" return AutoTokenizer . from_pretrained ( model_type ) def get_dataset_metadata ( self , dataset : DatasetDict ) -> dict : \"\"\"Gathers metadata for the given dataset into a dict for use in model training. Extracts dataset metadata to include a list of the datasets labels, a dict mapping the labels to their respective indices and a dict mapping the indices to the respective label. Parameters ---------- dataset : DatasetDict The dataset to retrieve metadata from. Returns ------- dict The metadata for the dataset within a dict object. \"\"\" lbls = [ label for label in dataset [ \"train\" ] . features . keys () if label not in [ DATASET_COLS [ 0 ]] ] lbl2idx = { lbl : idx for idx , lbl in enumerate ( lbls )} idx2lbl = { idx : lbl for idx , lbl in enumerate ( lbls )} return { \"labels\" : lbls , \"lbl2idx\" : lbl2idx , \"idx2lbl\" : idx2lbl } def preprocess ( self , dataset : DatasetDict , labels : list [ str ], tokenizer ) -> DatasetDict : \"\"\"Preprocesses and tokenizes a given dataset. Parameters ---------- dataset : DatasetDict The dataset to to be preprocessed/tokenized. labels : list[str] The datasets labels. tokenizer : PreTrainedTokenizer | PreTrainedTokenizerFast The tokenizer used for tokenizing the dataset. Returns ------- DatasetDict The preprocessed dataset. \"\"\" def preprocess_runner ( data : DatasetDict ): \"\"\"Runner for performing tokenization. Parameters ---------- data : DatasetDict The dataset to to be tokenized. Returns ------- DatasetDict The tokenized dataset. \"\"\" return tokenizer ( data [ DATASET_COLS [ 0 ]], padding = \"max_length\" , truncation = True , max_length = 128 ) if not labels : labels = [ label for label in dataset [ \"train\" ] . features . keys () if label not in [ DATASET_COLS [ 0 ]] ] if not tokenizer : tokenizer = self . get_tokenizer () encoded_ds = dataset . map ( preprocess_runner , batched = True ) encoded_ds . set_format ( \"torch\" ) return encoded_ds bin_ml_dataset_split ( dataset ) Forms train/test split for use in the binary classification and multilabel regression models training. Takes a loaded dataset and splits it into two separate datasets with the required corresponding labels (the binary model dataset will only have the DISCRIMINATORY column, while the multilabel regression model will only have the columns consisting of the discrimination categories). Additionally, the function will split the prepared datasets into a train/test split. Parameters: dataset ( DatasetDict ) \u2013 The dataset to be processed. Returns: DatasetDict \u2013 The prepared binary model and multilabel model dataset objects. Source code in nlpinitiative\\data_preparation\\data_process.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def bin_ml_dataset_split ( self , dataset : Dataset ) -> tuple [ DatasetDict , DatasetDict ]: \"\"\"Forms train/test split for use in the binary classification and multilabel regression models training. Takes a loaded dataset and splits it into two separate datasets with the required corresponding labels (the binary model dataset will only have the DISCRIMINATORY column, while the multilabel regression model will only have the columns consisting of the discrimination categories). Additionally, the function will split the prepared datasets into a train/test split. Parameters ---------- dataset : DatasetDict The dataset to be processed. Returns ------- DatasetDict The prepared binary model and multilabel model dataset objects. \"\"\" def get_bin_ds () -> DatasetDict : \"\"\"Prepares the binary model dataset. Returns ------- DatasetDict The prepared binary model dataset. \"\"\" train = dataset [ \"train\" ] . remove_columns ( CATEGORY_LABELS ) test = dataset [ \"test\" ] . remove_columns ( CATEGORY_LABELS ) return DatasetDict ( { \"train\" : train . rename_column ( \"DISCRIMINATORY\" , \"label\" ), \"test\" : test . rename_column ( \"DISCRIMINATORY\" , \"label\" ), } ) def get_ml_regr_ds () -> DatasetDict : \"\"\"Prepares the multilabel model dataset. Returns ------- DatasetDict The prepared multilabel model dataset. \"\"\" def combine_labels ( ex_ds ): \"\"\"Consolidates the mutiple dataset columns for categories into a single column consisting of a list of the category data. Parameters ---------- ex_ds : DatasetDict The multilabel dataset to be corrected. Returns ------- DatasetDict The corrected dataset. \"\"\" ex_ds [ \"labels\" ] = [ float ( ex_ds [ \"GENDER\" ]), float ( ex_ds [ \"RACE\" ]), float ( ex_ds [ \"SEXUALITY\" ]), float ( ex_ds [ \"DISABILITY\" ]), float ( ex_ds [ \"RELIGION\" ]), float ( ex_ds [ \"UNSPECIFIED\" ]), ] return ex_ds train = dataset [ \"train\" ] . remove_columns ( BINARY_LABELS ) train = train . map ( combine_labels ) train = train . remove_columns ( CATEGORY_LABELS ) test = dataset [ \"test\" ] . remove_columns ( BINARY_LABELS ) test = test . map ( combine_labels ) test = test . remove_columns ( CATEGORY_LABELS ) return DatasetDict ({ \"train\" : train , \"test\" : test }) return get_bin_ds (), get_ml_regr_ds () dataset_from_file ( filename , srcdir = PROCESSED_DATA_DIR ) Loads a dataset from a specified file into a Dataset object. Parameters: filename ( str ) \u2013 The file name of the dataset to be loaded. srcdir ( Path , default: PROCESSED_DATA_DIR ) \u2013 The file path to the directory that the dataset is stored (default is data/processed). Raises: Exception \u2013 If the file specified does not exist. Returns: DatasetDict \u2013 The loaded dataset as a DatasetDict object. Source code in nlpinitiative\\data_preparation\\data_process.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def dataset_from_file ( self , filename : str , srcdir : Path = PROCESSED_DATA_DIR ) -> DatasetDict : \"\"\"Loads a dataset from a specified file into a Dataset object. Parameters ---------- filename : str The file name of the dataset to be loaded. srcdir : Path, optional The file path to the directory that the dataset is stored (default is data/processed). Raises ------ Exception If the file specified does not exist. Returns ------- DatasetDict The loaded dataset as a DatasetDict object. \"\"\" if filename and os . path . exists ( os . path . join ( srcdir , filename )): ext = os . path . splitext ( filename )[ - 1 ] ext = ext . replace ( \".\" , \"\" ) ds = load_dataset ( ext , data_files = os . path . join ( srcdir , filename ), split = \"train\" ) . train_test_split ( test_size = TRAIN_TEST_SPLIT ) return ds else : raise Exception ( \"Invalid file name or file path\" ) get_dataset_metadata ( dataset ) Gathers metadata for the given dataset into a dict for use in model training. Extracts dataset metadata to include a list of the datasets labels, a dict mapping the labels to their respective indices and a dict mapping the indices to the respective label. Parameters: dataset ( DatasetDict ) \u2013 The dataset to retrieve metadata from. Returns: dict \u2013 The metadata for the dataset within a dict object. Source code in nlpinitiative\\data_preparation\\data_process.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def get_dataset_metadata ( self , dataset : DatasetDict ) -> dict : \"\"\"Gathers metadata for the given dataset into a dict for use in model training. Extracts dataset metadata to include a list of the datasets labels, a dict mapping the labels to their respective indices and a dict mapping the indices to the respective label. Parameters ---------- dataset : DatasetDict The dataset to retrieve metadata from. Returns ------- dict The metadata for the dataset within a dict object. \"\"\" lbls = [ label for label in dataset [ \"train\" ] . features . keys () if label not in [ DATASET_COLS [ 0 ]] ] lbl2idx = { lbl : idx for idx , lbl in enumerate ( lbls )} idx2lbl = { idx : lbl for idx , lbl in enumerate ( lbls )} return { \"labels\" : lbls , \"lbl2idx\" : lbl2idx , \"idx2lbl\" : idx2lbl } get_tokenizer ( model_type = DEF_MODEL ) Generates a tokenizer for the specified model type. Parameters: model_type ( str , default: DEF_MODEL ) \u2013 The model type for which the tokenizer is to be created. Returns: PreTrainedTokenizer | PreTrainedTokenizerFast \u2013 The tokenizer object. Source code in nlpinitiative\\data_preparation\\data_process.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def get_tokenizer ( self , model_type : str = DEF_MODEL ): \"\"\"Generates a tokenizer for the specified model type. Parameters ---------- model_type : str, optional The model type for which the tokenizer is to be created. Returns ------- PreTrainedTokenizer | PreTrainedTokenizerFast The tokenizer object. \"\"\" return AutoTokenizer . from_pretrained ( model_type ) preprocess ( dataset , labels , tokenizer ) Preprocesses and tokenizes a given dataset. Parameters: dataset ( DatasetDict ) \u2013 The dataset to to be preprocessed/tokenized. labels ( list [ str ] ) \u2013 The datasets labels. tokenizer ( PreTrainedTokenizer | PreTrainedTokenizerFast ) \u2013 The tokenizer used for tokenizing the dataset. Returns: DatasetDict \u2013 The preprocessed dataset. Source code in nlpinitiative\\data_preparation\\data_process.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def preprocess ( self , dataset : DatasetDict , labels : list [ str ], tokenizer ) -> DatasetDict : \"\"\"Preprocesses and tokenizes a given dataset. Parameters ---------- dataset : DatasetDict The dataset to to be preprocessed/tokenized. labels : list[str] The datasets labels. tokenizer : PreTrainedTokenizer | PreTrainedTokenizerFast The tokenizer used for tokenizing the dataset. Returns ------- DatasetDict The preprocessed dataset. \"\"\" def preprocess_runner ( data : DatasetDict ): \"\"\"Runner for performing tokenization. Parameters ---------- data : DatasetDict The dataset to to be tokenized. Returns ------- DatasetDict The tokenized dataset. \"\"\" return tokenizer ( data [ DATASET_COLS [ 0 ]], padding = \"max_length\" , truncation = True , max_length = 128 ) if not labels : labels = [ label for label in dataset [ \"train\" ] . features . keys () if label not in [ DATASET_COLS [ 0 ]] ] if not tokenizer : tokenizer = self . get_tokenizer () encoded_ds = dataset . map ( preprocess_runner , batched = True ) encoded_ds . set_format ( \"torch\" ) return encoded_ds","title":"Dataset Management"},{"location":"dataset-management/#dataset-operations","text":"","title":"Dataset Operations"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management","text":"","title":"data_management"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DataManager","text":"A class for handling data import, normalization and preprocessing/tokenization. Source code in nlpinitiative\\data_preparation\\data_management.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 class DataManager : \"\"\"A class for handling data import, normalization and preprocessing/tokenization.\"\"\" def __init__ ( self ): \"\"\"Contructor method for instantiating a DataManager object.\"\"\" self . normalizer = DataNormalizer () self . processor = DataProcessor () self . rec_mgr = DatasetRecordManager () # Data Importing Functionality # =================================================================================================================== def _is_valid_url ( self , url : str ) -> bool : \"\"\"Checks that a URL has a valid format. Parameters ---------- url : str The url to be checked. Returns ------- bool True if the URL valid or False if not. \"\"\" if url : parsed_url = urlparse ( url ) return bool ( parsed_url . scheme in [ \"http\" , \"https\" , \"ftp\" ]) else : return False def _generate_import_filename ( self , url : str ) -> str : \"\"\"Generates a filename that will be used when importing new datasets. Parameters ---------- url : str The url of the dataset to be imported. Returns ------- str The generated file name. \"\"\" def github (): \"\"\"Generates a filename based on a GitHub URL. Returns ------- str The generated file name. \"\"\" parsed = urlparse ( url ) path = os . path . splitext ( parsed . path )[ 0 ][ 1 :] path_arr = path . split ( \"/\" ) return \"_\" . join ([ path_arr [ 0 ], path_arr [ 1 ], path_arr [ - 1 ]]) if \"github\" in url : return github () else : split_arr = url . split ( \"/\" ) return split_arr [ - 1 ] def _format_url ( self , url : str ) -> str : \"\"\"Converts URLs to a format that can be used for importing data from a remote source. Parameters ---------- url : str The url of the dataset to be imported. Returns ------- str The reformatted URL. \"\"\" def github (): \"\"\"Handles conversion of GitHub URLs. Returns ------- str The reformatted GitHub URL. \"\"\" base_url = \"https://raw.githubusercontent.com\" if base_url in url : return url else : updated_url = url . replace ( \"https://github.com\" , base_url ) updated_url = updated_url . replace ( \"blob\" , \"refs/heads\" ) return updated_url if \"github\" in url : logger . info ( f \"Source url identified as GitHub URL, { url } \" ) formatted_url = github () logger . info ( f \"URL Formatted, { formatted_url } \" ) return formatted_url else : return url def file_to_df ( self , source : str , ext : str ) -> pd . DataFrame : \"\"\"Converts a dataset to a dataframe. This function also handles conversion of csv files with atypical delimiters. Parameters ---------- source : str The file path for a local dataset or URL for a remote dataset. ext : str The file extension of the dataset. Returns ------- DataFrame The dataset as a Pandas DataFrame. \"\"\" try : match ext : case \".csv\" : try : df = pd . read_csv ( source ) except : df = pd . read_csv ( source , delimiter = \";\" ) case \".xlsx\" : df = pd . read_excel ( source ) case \".json\" : df = pd . read_json ( source ) case _ : df = None return df except Exception as e : err_msg = f \"Failed to import from source - { e } \" logger . error ( err_msg ) raise Exception ( err_msg ) def import_data ( self , import_type : str , source , dataset_name : str , is_third_party : bool = True , local_ds_ref_url : bool = None , overwrite : bool = False , ) -> pd . DataFrame : \"\"\"Imports data from a local (by file path) or remote (ny URL) source. Parameters ---------- import_type : str 'local' if from a local source, 'external' if from remote source. source : str The file path or URL of the dataset. dataset_name : str The name used to id the dataset. is_third_party : bool, optional True if 3rd party dataset, False if not (default is True). local_ds_ref_url : str, optional Reference URL for datasets imported locally (default is None). overwrite : bool, optional True if files with the same name should be overwritten, False if not (default is False). Returns ------- DataFrame The dataset as a Pandas DataFrame. \"\"\" ref_url = None formatted_url = None match import_type : case \"local\" : if source is None or not os . path . exists ( source ): err_msg = \"Dataset filepath does not exist\" logger . error ( err_msg ) raise Exception ( err_msg ) if is_third_party and not self . _is_valid_url ( local_ds_ref_url ): err_msg = ( \"Locally imported 3rd party dataset imports must include a reference URL.\" ) logger . error ( err_msg ) raise Exception ( err_msg ) ref_url = local_ds_ref_url if is_third_party else \"Custom Created Dataset\" tail = os . path . split ( source )[ - 1 ] filename , ext = os . path . splitext ( tail )[ - 2 :] if ext not in ACCEPTED_FILE_FORMATS : err_msg = \"Unsupported file type\" logger . error ( err_msg ) raise Exception ( err_msg ) src = source case \"external\" : if not self . _is_valid_url ( source ): err_msg = \"Invalid URL\" logger . error ( err_msg ) raise Exception ( err_msg ) ref_url = source formatted_url = self . _format_url ( ref_url ) filename = self . _generate_import_filename ( formatted_url ) path = urlparse ( formatted_url ) . path ext = os . path . splitext ( path )[ 1 ] if ext not in ACCEPTED_FILE_FORMATS : err_msg = \"Unsupported file type\" logger . error ( err_msg ) raise Exception ( err_msg ) src = formatted_url case _ : err_msg = \"Invalid import type\" logger . error ( err_msg ) raise Exception ( err_msg ) ds_df = self . file_to_df ( src , ext ) if self . rec_mgr . dataset_src_exists ( ref_url ): return ds_df self . _store_data ( data_df = ds_df , filename = filename , destpath = RAW_DATA_DIR , overwrite = overwrite ) self . rec_mgr . update ( ds_id = dataset_name , src_url = ref_url , download_url = formatted_url , raw_ds_filename = f \" { filename } .csv\" , ) logger . success ( f \"Successfully imported { import_type } dataset from { source } .\" ) return ds_df # Data Normalization Functionality # =================================================================================================================== def _valid_filepath ( self , path : Path ) -> bool : \"\"\"Checks that the specified file path is valid. Parameters ---------- path : Path The file path to be checked. Returns ------- bool True if it exists, False if it does not. \"\"\" return os . path . exists ( path ) def normalize_dataset ( self , ds_files : list [ str ], conv_schema_fn : str , output_fn : str ) -> pd . DataFrame : \"\"\"Handles normalization of a third-party dataset to the schema used for training our models. Parameters ---------- ds_files : list[str] One or more dataset files to merge and normalize. conv_schema_fn : str The file name of the json conversion schema used. output_fn : str The output filename to be used. Returns ------- DataFrame True if it exists, False if it does not. \"\"\" for filename in ds_files : if not self . _valid_filepath (( RAW_DATA_DIR / filename )): err_msg = f \"Invalid filepath for file, { filename } [ { RAW_DATA_DIR / filename } ].\" logger . error ( err_msg ) raise Exception ( err_msg ) if not self . _valid_filepath ( NORM_SCHEMA_DIR / conv_schema_fn ): err_msg = f \"Normalization schema file, { conv_schema_fn } , does not exist.\" logger . error ( err_msg ) raise Exception ( err_msg ) if output_fn is None or not len ( output_fn ) > 0 : err_msg = f \"Invalid output filename\" logger . error ( err_msg ) raise Exception ( err_msg ) try : normalized_df = self . normalizer . normalize_datasets ( files = ds_files , cv_path = NORM_SCHEMA_DIR / conv_schema_fn ) except Exception as e : err_msg = f \"Failed to normalize file(s) - { e } \" logger . error ( err_msg ) raise Exception ( err_msg ) self . _store_data ( normalized_df , output_fn , INTERIM_DATA_DIR ) for filename in ds_files : row_vals = self . rec_mgr . get_entry_by_raw_fn ( filename ) self . rec_mgr . update ( ds_id = row_vals [ 0 ], src_url = row_vals [ 1 ], download_url = row_vals [ 2 ], raw_ds_filename = row_vals [ 3 ], normalization_schema_filename = conv_schema_fn , normalized_ds_filename = f \" { output_fn } .csv\" , ) logger . success ( f \"Successfully normalized dataset files [ { ', ' . join ( ds_files ) } ]\" ) return normalized_df # Master Dataset Creation # =================================================================================================================== def build_master_dataset ( self ): \"\"\"Takes the processed datasets, merges them and then stores the resulting dataset in the data/processed directory. Facilitates consolidation of all imported and normalized datasets into a single, master dataset and stores the master dataset in the data/processed directory. \"\"\" master_df = None for filename in os . listdir ( INTERIM_DATA_DIR ): if filename != \".gitkeep\" : _ , ext = os . path . splitext ( filename ) new_df = self . file_to_df ( INTERIM_DATA_DIR / filename , ext ) if master_df is None : master_df = new_df else : master_df = pd . concat ([ master_df , new_df ]) . dropna () self . _store_data ( data_df = master_df , filename = \"NLPinitiative_Master_Dataset\" , destpath = PROCESSED_DATA_DIR , overwrite = True , ) def pull_dataset_repo ( self , token : str ): \"\"\"Pulls the data directory from the linked Hugging Face Dataset Repository. Parameters ---------- token : str A Hugging Face token with read/write access privileges to allow importing the data. \"\"\" if token is not None : hfh . snapshot_download ( repo_id = DATASET_REPO , repo_type = \"dataset\" , local_dir = DATA_DIR , token = token ) def push_dataset_dir ( self , token : str ): \"\"\"Pushes the data directory (all dataset information) to the linked Hugging Face Dataset Repository. Parameters ---------- token : str A Hugging Face token with read/write access privileges to allow importing the data. \"\"\" if token is not None : hfh . upload_folder ( repo_id = DATASET_REPO , repo_type = \"dataset\" , folder_path = DATA_DIR , token = token ) # Data Preparation Functionality # =================================================================================================================== def prepare_and_preprocess_dataset ( self , filename : str = \"NLPinitiative_Master_Dataset.csv\" , srcdir : Path = PROCESSED_DATA_DIR , bin_model_type : str = DEF_MODEL , ml_model_type : str = DEF_MODEL , ): \"\"\"Preprocesses and tokenizes the specified dataset. Parameters ---------- filename : str, optional The file name of the file to be loaded and processed (default is 'NLPinitiative_Master_Dataset.csv'). srcdir : Path, optional The source directory of the file to be processed (default is data/processed). bin_model_type : str, optional The binary classification base model type (default is the DEF_MODEL defined in the nlpinitiative/config.py file). ml_model_type : str, optional The multilabel regression base model type (default is the DEF_MODEL defined in the nlpinitiative/config.py file). Returns ------- tuple[DatasetContainer, DatasetContainer] Two data container objects consisting of the raw dataset, encoded dataset, metadata and tokenizer for training binary and multilabel models. \"\"\" raw_dataset = self . processor . dataset_from_file ( filename , srcdir ) bin_ds , ml_ds = self . processor . bin_ml_dataset_split ( raw_dataset ) bin_ds_metadata = self . processor . get_dataset_metadata ( bin_ds ) bin_tkzr = self . processor . get_tokenizer ( bin_model_type ) bin_encoded_ds = self . processor . preprocess ( bin_ds , bin_ds_metadata [ \"labels\" ], bin_tkzr ) bin_data_obj = DatasetContainer ( bin_ds , bin_encoded_ds , bin_ds_metadata , bin_tkzr ) ml_ds_metadata = self . processor . get_dataset_metadata ( bin_ds ) ml_tkzr = self . processor . get_tokenizer ( ml_model_type ) ml_encoded_ds = self . processor . preprocess ( ml_ds , ml_ds_metadata [ \"labels\" ], ml_tkzr ) ml_data_obj = DatasetContainer ( ml_ds , ml_encoded_ds , ml_ds_metadata , ml_tkzr ) return bin_data_obj , ml_data_obj # Misc Helper Functions # =================================================================================================================== def _store_data ( self , data_df : pd . DataFrame , filename : str , destpath : Path , overwrite : bool = False ): \"\"\"Stores the specified DataFrame as a csv dataset within the data directory. Parameters ---------- data_df : DataFrame The dataset as a Pandas DataFrame object. filename : str The file name of the data to be stored. destpath : Path The path that the data is to be stored at. overwrite : bool, optional True if file with the same name should be overwritten, False if not (default is False). \"\"\" ## Handles situations of duplicate filenames appended_num = 0 corrected_filename = f \" { filename } .csv\" while not overwrite and os . path . exists ( os . path . join ( destpath , corrected_filename )): appended_num += 1 corrected_filename = f \" { filename } - { appended_num } .csv\" data_df . to_csv ( path_or_buf = os . path . join ( destpath , corrected_filename ), index = False , mode = \"w\" ) def get_dataset_statistics ( self , ds_path : Path ) -> dict : \"\"\"Generates statistics for the specified dataset. Evaluates a dataset and records various details for use in assesing if the dataset is imbalanced. Parameters ---------- dataset_path : Path The file path to the dataset. Returns ------- dict A JSON object containing the generated data. \"\"\" def get_category_details (): \"\"\"Generates statistical data for the category columns of the dataset. Evaluations include: - The sum of the categories values. - The number of rows that a given category has a non-zero value. - The number of rows where the category has a value greater than 0.5. - The number of rows where the category had the highest value among categories. Returns ------- dict A dict containing the data for all categories. \"\"\" cat_dict = dict () for cat in CATEGORY_LABELS : cat_dict [ cat ] = { \"total_combined_value\" : dataset_df [ cat ] . sum (), \"num_positive_rows\" : len ( dataset_df . loc [ dataset_df [ cat ] > 0 ]), \"num_gt_threshold\" : len ( dataset_df . loc [ dataset_df [ cat ] >= 0.5 ]), \"num_rows_as_dominant_category\" : 0 , } for _ , row in dataset_df . iterrows (): dominant_cat = None dominant_val = 0 for cat in CATEGORY_LABELS : cat_val = row [ cat ] if cat_val > dominant_val : dominant_cat = cat dominant_val = cat_val if dominant_cat is not None : cat_dict [ dominant_cat ][ \"num_rows_as_dominant_category\" ] += 1 return cat_dict dataset_df = self . file_to_df ( ds_path , \".csv\" ) json_obj = { \"total_num_entries\" : len ( dataset_df ), \"num_positive_discriminatory\" : len ( dataset_df . loc [ dataset_df [ BINARY_LABELS [ 0 ]] == 1 ]), \"num_negative_discriminatory\" : len ( dataset_df . loc [ dataset_df [ BINARY_LABELS [ 0 ]] == 0 ]), \"category_stats\" : get_category_details (), } return json_obj def remove_file ( self , filename : str , path : Path ): \"\"\"Removes the specified file. Parameters ---------- filename : str The name of the file. path : Path The path to the file. \"\"\" if os . path . exists ( path / filename ): try : os . remove ( path / filename ) logger . success ( f \"Successfully removed file, { filename } .\" ) except Exception as e : logger . error ( f \"Failed to remove file, { filename } - { e } \" ) else : logger . info ( f \"File, { filename } , does not exist.\" )","title":"DataManager"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DataManager.__init__","text":"Contructor method for instantiating a DataManager object. Source code in nlpinitiative\\data_preparation\\data_management.py 32 33 34 35 36 37 def __init__ ( self ): \"\"\"Contructor method for instantiating a DataManager object.\"\"\" self . normalizer = DataNormalizer () self . processor = DataProcessor () self . rec_mgr = DatasetRecordManager ()","title":"__init__"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DataManager.build_master_dataset","text":"Takes the processed datasets, merges them and then stores the resulting dataset in the data/processed directory. Facilitates consolidation of all imported and normalized datasets into a single, master dataset and stores the master dataset in the data/processed directory. Source code in nlpinitiative\\data_preparation\\data_management.py 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 def build_master_dataset ( self ): \"\"\"Takes the processed datasets, merges them and then stores the resulting dataset in the data/processed directory. Facilitates consolidation of all imported and normalized datasets into a single, master dataset and stores the master dataset in the data/processed directory. \"\"\" master_df = None for filename in os . listdir ( INTERIM_DATA_DIR ): if filename != \".gitkeep\" : _ , ext = os . path . splitext ( filename ) new_df = self . file_to_df ( INTERIM_DATA_DIR / filename , ext ) if master_df is None : master_df = new_df else : master_df = pd . concat ([ master_df , new_df ]) . dropna () self . _store_data ( data_df = master_df , filename = \"NLPinitiative_Master_Dataset\" , destpath = PROCESSED_DATA_DIR , overwrite = True , )","title":"build_master_dataset"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DataManager.file_to_df","text":"Converts a dataset to a dataframe. This function also handles conversion of csv files with atypical delimiters. Parameters: source ( str ) \u2013 The file path for a local dataset or URL for a remote dataset. ext ( str ) \u2013 The file extension of the dataset. Returns: DataFrame \u2013 The dataset as a Pandas DataFrame. Source code in nlpinitiative\\data_preparation\\data_management.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def file_to_df ( self , source : str , ext : str ) -> pd . DataFrame : \"\"\"Converts a dataset to a dataframe. This function also handles conversion of csv files with atypical delimiters. Parameters ---------- source : str The file path for a local dataset or URL for a remote dataset. ext : str The file extension of the dataset. Returns ------- DataFrame The dataset as a Pandas DataFrame. \"\"\" try : match ext : case \".csv\" : try : df = pd . read_csv ( source ) except : df = pd . read_csv ( source , delimiter = \";\" ) case \".xlsx\" : df = pd . read_excel ( source ) case \".json\" : df = pd . read_json ( source ) case _ : df = None return df except Exception as e : err_msg = f \"Failed to import from source - { e } \" logger . error ( err_msg ) raise Exception ( err_msg )","title":"file_to_df"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DataManager.get_dataset_statistics","text":"Generates statistics for the specified dataset. Evaluates a dataset and records various details for use in assesing if the dataset is imbalanced. Parameters: dataset_path ( Path ) \u2013 The file path to the dataset. Returns: dict \u2013 A JSON object containing the generated data. Source code in nlpinitiative\\data_preparation\\data_management.py 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 def get_dataset_statistics ( self , ds_path : Path ) -> dict : \"\"\"Generates statistics for the specified dataset. Evaluates a dataset and records various details for use in assesing if the dataset is imbalanced. Parameters ---------- dataset_path : Path The file path to the dataset. Returns ------- dict A JSON object containing the generated data. \"\"\" def get_category_details (): \"\"\"Generates statistical data for the category columns of the dataset. Evaluations include: - The sum of the categories values. - The number of rows that a given category has a non-zero value. - The number of rows where the category has a value greater than 0.5. - The number of rows where the category had the highest value among categories. Returns ------- dict A dict containing the data for all categories. \"\"\" cat_dict = dict () for cat in CATEGORY_LABELS : cat_dict [ cat ] = { \"total_combined_value\" : dataset_df [ cat ] . sum (), \"num_positive_rows\" : len ( dataset_df . loc [ dataset_df [ cat ] > 0 ]), \"num_gt_threshold\" : len ( dataset_df . loc [ dataset_df [ cat ] >= 0.5 ]), \"num_rows_as_dominant_category\" : 0 , } for _ , row in dataset_df . iterrows (): dominant_cat = None dominant_val = 0 for cat in CATEGORY_LABELS : cat_val = row [ cat ] if cat_val > dominant_val : dominant_cat = cat dominant_val = cat_val if dominant_cat is not None : cat_dict [ dominant_cat ][ \"num_rows_as_dominant_category\" ] += 1 return cat_dict dataset_df = self . file_to_df ( ds_path , \".csv\" ) json_obj = { \"total_num_entries\" : len ( dataset_df ), \"num_positive_discriminatory\" : len ( dataset_df . loc [ dataset_df [ BINARY_LABELS [ 0 ]] == 1 ]), \"num_negative_discriminatory\" : len ( dataset_df . loc [ dataset_df [ BINARY_LABELS [ 0 ]] == 0 ]), \"category_stats\" : get_category_details (), } return json_obj","title":"get_dataset_statistics"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DataManager.import_data","text":"Imports data from a local (by file path) or remote (ny URL) source. Parameters: import_type ( str ) \u2013 'local' if from a local source, 'external' if from remote source. source ( str ) \u2013 The file path or URL of the dataset. dataset_name ( str ) \u2013 The name used to id the dataset. is_third_party ( bool , default: True ) \u2013 True if 3rd party dataset, False if not (default is True). local_ds_ref_url ( str , default: None ) \u2013 Reference URL for datasets imported locally (default is None). overwrite ( bool , default: False ) \u2013 True if files with the same name should be overwritten, False if not (default is False). Returns: DataFrame \u2013 The dataset as a Pandas DataFrame. Source code in nlpinitiative\\data_preparation\\data_management.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 def import_data ( self , import_type : str , source , dataset_name : str , is_third_party : bool = True , local_ds_ref_url : bool = None , overwrite : bool = False , ) -> pd . DataFrame : \"\"\"Imports data from a local (by file path) or remote (ny URL) source. Parameters ---------- import_type : str 'local' if from a local source, 'external' if from remote source. source : str The file path or URL of the dataset. dataset_name : str The name used to id the dataset. is_third_party : bool, optional True if 3rd party dataset, False if not (default is True). local_ds_ref_url : str, optional Reference URL for datasets imported locally (default is None). overwrite : bool, optional True if files with the same name should be overwritten, False if not (default is False). Returns ------- DataFrame The dataset as a Pandas DataFrame. \"\"\" ref_url = None formatted_url = None match import_type : case \"local\" : if source is None or not os . path . exists ( source ): err_msg = \"Dataset filepath does not exist\" logger . error ( err_msg ) raise Exception ( err_msg ) if is_third_party and not self . _is_valid_url ( local_ds_ref_url ): err_msg = ( \"Locally imported 3rd party dataset imports must include a reference URL.\" ) logger . error ( err_msg ) raise Exception ( err_msg ) ref_url = local_ds_ref_url if is_third_party else \"Custom Created Dataset\" tail = os . path . split ( source )[ - 1 ] filename , ext = os . path . splitext ( tail )[ - 2 :] if ext not in ACCEPTED_FILE_FORMATS : err_msg = \"Unsupported file type\" logger . error ( err_msg ) raise Exception ( err_msg ) src = source case \"external\" : if not self . _is_valid_url ( source ): err_msg = \"Invalid URL\" logger . error ( err_msg ) raise Exception ( err_msg ) ref_url = source formatted_url = self . _format_url ( ref_url ) filename = self . _generate_import_filename ( formatted_url ) path = urlparse ( formatted_url ) . path ext = os . path . splitext ( path )[ 1 ] if ext not in ACCEPTED_FILE_FORMATS : err_msg = \"Unsupported file type\" logger . error ( err_msg ) raise Exception ( err_msg ) src = formatted_url case _ : err_msg = \"Invalid import type\" logger . error ( err_msg ) raise Exception ( err_msg ) ds_df = self . file_to_df ( src , ext ) if self . rec_mgr . dataset_src_exists ( ref_url ): return ds_df self . _store_data ( data_df = ds_df , filename = filename , destpath = RAW_DATA_DIR , overwrite = overwrite ) self . rec_mgr . update ( ds_id = dataset_name , src_url = ref_url , download_url = formatted_url , raw_ds_filename = f \" { filename } .csv\" , ) logger . success ( f \"Successfully imported { import_type } dataset from { source } .\" ) return ds_df","title":"import_data"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DataManager.normalize_dataset","text":"Handles normalization of a third-party dataset to the schema used for training our models. Parameters: ds_files ( list [ str ] ) \u2013 One or more dataset files to merge and normalize. conv_schema_fn ( str ) \u2013 The file name of the json conversion schema used. output_fn ( str ) \u2013 The output filename to be used. Returns: DataFrame \u2013 True if it exists, False if it does not. Source code in nlpinitiative\\data_preparation\\data_management.py 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 def normalize_dataset ( self , ds_files : list [ str ], conv_schema_fn : str , output_fn : str ) -> pd . DataFrame : \"\"\"Handles normalization of a third-party dataset to the schema used for training our models. Parameters ---------- ds_files : list[str] One or more dataset files to merge and normalize. conv_schema_fn : str The file name of the json conversion schema used. output_fn : str The output filename to be used. Returns ------- DataFrame True if it exists, False if it does not. \"\"\" for filename in ds_files : if not self . _valid_filepath (( RAW_DATA_DIR / filename )): err_msg = f \"Invalid filepath for file, { filename } [ { RAW_DATA_DIR / filename } ].\" logger . error ( err_msg ) raise Exception ( err_msg ) if not self . _valid_filepath ( NORM_SCHEMA_DIR / conv_schema_fn ): err_msg = f \"Normalization schema file, { conv_schema_fn } , does not exist.\" logger . error ( err_msg ) raise Exception ( err_msg ) if output_fn is None or not len ( output_fn ) > 0 : err_msg = f \"Invalid output filename\" logger . error ( err_msg ) raise Exception ( err_msg ) try : normalized_df = self . normalizer . normalize_datasets ( files = ds_files , cv_path = NORM_SCHEMA_DIR / conv_schema_fn ) except Exception as e : err_msg = f \"Failed to normalize file(s) - { e } \" logger . error ( err_msg ) raise Exception ( err_msg ) self . _store_data ( normalized_df , output_fn , INTERIM_DATA_DIR ) for filename in ds_files : row_vals = self . rec_mgr . get_entry_by_raw_fn ( filename ) self . rec_mgr . update ( ds_id = row_vals [ 0 ], src_url = row_vals [ 1 ], download_url = row_vals [ 2 ], raw_ds_filename = row_vals [ 3 ], normalization_schema_filename = conv_schema_fn , normalized_ds_filename = f \" { output_fn } .csv\" , ) logger . success ( f \"Successfully normalized dataset files [ { ', ' . join ( ds_files ) } ]\" ) return normalized_df","title":"normalize_dataset"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DataManager.prepare_and_preprocess_dataset","text":"Preprocesses and tokenizes the specified dataset. Parameters: filename ( str , default: 'NLPinitiative_Master_Dataset.csv' ) \u2013 The file name of the file to be loaded and processed (default is 'NLPinitiative_Master_Dataset.csv'). srcdir ( Path , default: PROCESSED_DATA_DIR ) \u2013 The source directory of the file to be processed (default is data/processed). bin_model_type ( str , default: DEF_MODEL ) \u2013 The binary classification base model type (default is the DEF_MODEL defined in the nlpinitiative/config.py file). ml_model_type ( str , default: DEF_MODEL ) \u2013 The multilabel regression base model type (default is the DEF_MODEL defined in the nlpinitiative/config.py file). Returns: tuple [ DatasetContainer , DatasetContainer ] \u2013 Two data container objects consisting of the raw dataset, encoded dataset, metadata and tokenizer for training binary and multilabel models. Source code in nlpinitiative\\data_preparation\\data_management.py 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 def prepare_and_preprocess_dataset ( self , filename : str = \"NLPinitiative_Master_Dataset.csv\" , srcdir : Path = PROCESSED_DATA_DIR , bin_model_type : str = DEF_MODEL , ml_model_type : str = DEF_MODEL , ): \"\"\"Preprocesses and tokenizes the specified dataset. Parameters ---------- filename : str, optional The file name of the file to be loaded and processed (default is 'NLPinitiative_Master_Dataset.csv'). srcdir : Path, optional The source directory of the file to be processed (default is data/processed). bin_model_type : str, optional The binary classification base model type (default is the DEF_MODEL defined in the nlpinitiative/config.py file). ml_model_type : str, optional The multilabel regression base model type (default is the DEF_MODEL defined in the nlpinitiative/config.py file). Returns ------- tuple[DatasetContainer, DatasetContainer] Two data container objects consisting of the raw dataset, encoded dataset, metadata and tokenizer for training binary and multilabel models. \"\"\" raw_dataset = self . processor . dataset_from_file ( filename , srcdir ) bin_ds , ml_ds = self . processor . bin_ml_dataset_split ( raw_dataset ) bin_ds_metadata = self . processor . get_dataset_metadata ( bin_ds ) bin_tkzr = self . processor . get_tokenizer ( bin_model_type ) bin_encoded_ds = self . processor . preprocess ( bin_ds , bin_ds_metadata [ \"labels\" ], bin_tkzr ) bin_data_obj = DatasetContainer ( bin_ds , bin_encoded_ds , bin_ds_metadata , bin_tkzr ) ml_ds_metadata = self . processor . get_dataset_metadata ( bin_ds ) ml_tkzr = self . processor . get_tokenizer ( ml_model_type ) ml_encoded_ds = self . processor . preprocess ( ml_ds , ml_ds_metadata [ \"labels\" ], ml_tkzr ) ml_data_obj = DatasetContainer ( ml_ds , ml_encoded_ds , ml_ds_metadata , ml_tkzr ) return bin_data_obj , ml_data_obj","title":"prepare_and_preprocess_dataset"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DataManager.pull_dataset_repo","text":"Pulls the data directory from the linked Hugging Face Dataset Repository. Parameters: token ( str ) \u2013 A Hugging Face token with read/write access privileges to allow importing the data. Source code in nlpinitiative\\data_preparation\\data_management.py 377 378 379 380 381 382 383 384 385 386 387 388 389 def pull_dataset_repo ( self , token : str ): \"\"\"Pulls the data directory from the linked Hugging Face Dataset Repository. Parameters ---------- token : str A Hugging Face token with read/write access privileges to allow importing the data. \"\"\" if token is not None : hfh . snapshot_download ( repo_id = DATASET_REPO , repo_type = \"dataset\" , local_dir = DATA_DIR , token = token )","title":"pull_dataset_repo"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DataManager.push_dataset_dir","text":"Pushes the data directory (all dataset information) to the linked Hugging Face Dataset Repository. Parameters: token ( str ) \u2013 A Hugging Face token with read/write access privileges to allow importing the data. Source code in nlpinitiative\\data_preparation\\data_management.py 391 392 393 394 395 396 397 398 399 400 401 402 403 def push_dataset_dir ( self , token : str ): \"\"\"Pushes the data directory (all dataset information) to the linked Hugging Face Dataset Repository. Parameters ---------- token : str A Hugging Face token with read/write access privileges to allow importing the data. \"\"\" if token is not None : hfh . upload_folder ( repo_id = DATASET_REPO , repo_type = \"dataset\" , folder_path = DATA_DIR , token = token )","title":"push_dataset_dir"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DataManager.remove_file","text":"Removes the specified file. Parameters: filename ( str ) \u2013 The name of the file. path ( Path ) \u2013 The path to the file. Source code in nlpinitiative\\data_preparation\\data_management.py 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 def remove_file ( self , filename : str , path : Path ): \"\"\"Removes the specified file. Parameters ---------- filename : str The name of the file. path : Path The path to the file. \"\"\" if os . path . exists ( path / filename ): try : os . remove ( path / filename ) logger . success ( f \"Successfully removed file, { filename } .\" ) except Exception as e : logger . error ( f \"Failed to remove file, { filename } - { e } \" ) else : logger . info ( f \"File, { filename } , does not exist.\" )","title":"remove_file"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DatasetContainer","text":"A class used for organizing dataset information used within model training. Source code in nlpinitiative\\data_preparation\\data_management.py 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 class DatasetContainer : \"\"\"A class used for organizing dataset information used within model training.\"\"\" def __init__ ( self , dataset : DatasetDict , encoded_ds : DatasetDict , metadata : dict , tokenizer : PreTrainedTokenizer | PreTrainedTokenizerFast , ): \"\"\"Constuctor for instantiating DatasetContainer. Parameters ---------- dataset : DatasetDict The raw dataset (before tokenization). encoded_ds : DatasetDict The encoded dataset. metadata : dict The metadata associated with the dataset. tokenizer : PreTrainedTokenizer | PreTrainedTokenizerFast The tokenizer used for tokenizing the dataset. \"\"\" self . _raw_dataset = dataset self . _encoded_dataset = encoded_ds self . _labels = metadata [ \"labels\" ] self . _lbl2idx = metadata [ \"lbl2idx\" ] self . _idx2lbl = metadata [ \"idx2lbl\" ] self . _tkzr = tokenizer @property def raw_dataset ( self ): \"\"\"The unencoded dataset.\"\"\" return self . _raw_dataset @property def encoded_dataset ( self ): \"\"\"The encoded/tokenized dataset.\"\"\" return self . _encoded_dataset @property def labels ( self ): \"\"\"The labels used within the dataset.\"\"\" return self . _labels @property def lbl2idx ( self ): \"\"\"Dict for mapping label indices to the label names.\"\"\" return self . _lbl2idx @property def idx2lbl ( self ): \"\"\"Dict for mapping label names to the label indices.\"\"\" return self . _idx2lbl @property def tokenizer ( self ): \"\"\"The tokenizer for the dataset.\"\"\" return self . _tkzr","title":"DatasetContainer"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DatasetContainer.encoded_dataset","text":"The encoded/tokenized dataset.","title":"encoded_dataset"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DatasetContainer.idx2lbl","text":"Dict for mapping label names to the label indices.","title":"idx2lbl"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DatasetContainer.labels","text":"The labels used within the dataset.","title":"labels"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DatasetContainer.lbl2idx","text":"Dict for mapping label indices to the label names.","title":"lbl2idx"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DatasetContainer.raw_dataset","text":"The unencoded dataset.","title":"raw_dataset"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DatasetContainer.tokenizer","text":"The tokenizer for the dataset.","title":"tokenizer"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DatasetContainer.__init__","text":"Constuctor for instantiating DatasetContainer. Parameters: dataset ( DatasetDict ) \u2013 The raw dataset (before tokenization). encoded_ds ( DatasetDict ) \u2013 The encoded dataset. metadata ( dict ) \u2013 The metadata associated with the dataset. tokenizer ( PreTrainedTokenizer | PreTrainedTokenizerFast ) \u2013 The tokenizer used for tokenizing the dataset. Source code in nlpinitiative\\data_preparation\\data_management.py 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 def __init__ ( self , dataset : DatasetDict , encoded_ds : DatasetDict , metadata : dict , tokenizer : PreTrainedTokenizer | PreTrainedTokenizerFast , ): \"\"\"Constuctor for instantiating DatasetContainer. Parameters ---------- dataset : DatasetDict The raw dataset (before tokenization). encoded_ds : DatasetDict The encoded dataset. metadata : dict The metadata associated with the dataset. tokenizer : PreTrainedTokenizer | PreTrainedTokenizerFast The tokenizer used for tokenizing the dataset. \"\"\" self . _raw_dataset = dataset self . _encoded_dataset = encoded_ds self . _labels = metadata [ \"labels\" ] self . _lbl2idx = metadata [ \"lbl2idx\" ] self . _idx2lbl = metadata [ \"idx2lbl\" ] self . _tkzr = tokenizer","title":"__init__"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DatasetRecordManager","text":"A class for managing and maintaining a record of the datasets imported and used for model training. Source code in nlpinitiative\\data_preparation\\data_management.py 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 class DatasetRecordManager : \"\"\"A class for managing and maintaining a record of the datasets imported and used for model training.\"\"\" def __init__ ( self ): \"\"\"Constructor for instantiating a DatasetRecordManager object.\"\"\" self . rec_df : pd . DataFrame = self . load_dataset_record () def load_dataset_record ( self ) -> pd . DataFrame : \"\"\"Loads the dataset record into a Pandas Dataframe. Returns ------- DataFrame The record of datasets as a DataFrame. \"\"\" if os . path . exists ( DATA_DIR / \"dataset_record.csv\" ): ds_rec_df = pd . read_csv ( filepath_or_buffer = DATA_DIR / \"dataset_record.csv\" ) else : ds_rec_df = pd . DataFrame ( columns = [ \"Dataset ID\" , \"Dataset Reference URL\" , \"Dataset Download URL\" , \"Raw Dataset Filename\" , \"Conversion Schema Filename\" , \"Converted Filename\" , ] ) ds_rec_df . to_csv ( DATA_DIR / \"dataset_record.csv\" , index = False ) return ds_rec_df def save_ds_record ( self ): \"\"\"Faciliates saving the current state of the dataset record as a csv file.\"\"\" self . rec_df . to_csv ( DATA_DIR / \"dataset_record.csv\" , index = False ) def dataset_src_exists ( self , src_url : str ) -> bool : \"\"\"Checks if a dataset record exists with the specified URL. Parameters ---------- src_url : str The source/reference URL of the dataset. Returns ------- bool True if the record exists, False otherwise. \"\"\" return len ( self . rec_df [ self . rec_df [ \"Dataset Reference URL\" ] == src_url ]) > 0 def get_ds_record_copy ( self ) -> pd . DataFrame : \"\"\"Returns a copy of the dataset record dataframe Returns ------- DataFrame A copy of the record as a Pandas DataFrame. \"\"\" return self . rec_df . copy ( deep = True ) def update ( self , ds_id : str , src_url : str = None , download_url : str = None , raw_ds_filename : str = None , normalization_schema_filename : str = None , normalized_ds_filename : str = None , ): \"\"\"Adds a new dataset record or updates an existing record. Parameters ---------- ds_id : str The name/id of the dataset (used for tracking purposes). src_url : str, optional The source or reference URL for an imported dataset (default is None). download_url : str, optional The download URL of the dataset (default is None). raw_ds_filename : str, optional The file name of the raw (non-normalized) dataset (default is None). normalization_schema_filename : str, optional The file name of the conversion schema used to normalize the dataset (default is None). normalized_ds_filename : str, optional The file name of the normalized version of the dataset (default is None). \"\"\" if ds_id is not None : new_df = pd . DataFrame ( { \"Dataset ID\" : [ ds_id ], \"Dataset Reference URL\" : [ src_url ], \"Dataset Download URL\" : [ download_url ], \"Raw Dataset Filename\" : [ raw_ds_filename ], \"Conversion Schema Filename\" : [ normalization_schema_filename ], \"Converted Filename\" : [ normalized_ds_filename ], } ) temp_df = pd . concat ([ self . rec_df , new_df ]) . drop_duplicates ( subset = [ \"Dataset ID\" , \"Dataset Reference URL\" ], keep = \"last\" ) self . rec_df = temp_df self . save_ds_record () def remove_entry ( self , ds_id : str ): \"\"\"Facilitates removal of a dataset record. Parameters ---------- ds_id : str The name/id of the dataset to be removed. \"\"\" if ds_id is not None : self . rec_df = self . rec_df [ self . rec_df [ \"Dataset ID\" ] != ds_id ] self . save_ds_record () def get_entry_by_raw_fn ( self , filename : str ) -> tuple : \"\"\"Retrieves the raw file name of the specified dataset record. Parameters ---------- filename : str The file name of the normalized version of the dataset. Raises ------ Exception If the method failed to retrieve the specified record. Returns ------- str The file name of the non-normalized version of the dataset. \"\"\" try : row = tuple ( self . rec_df [ self . rec_df [ \"Raw Dataset Filename\" ] == filename ] . values [ 0 ]) return row except Exception as e : err_msg = f \"Failed to retrieve row of Dataset Record - { e } \" logger . error ( err_msg ) raise Exception ( err_msg )","title":"DatasetRecordManager"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DatasetRecordManager.__init__","text":"Constructor for instantiating a DatasetRecordManager object. Source code in nlpinitiative\\data_preparation\\data_management.py 635 636 637 def __init__ ( self ): \"\"\"Constructor for instantiating a DatasetRecordManager object.\"\"\" self . rec_df : pd . DataFrame = self . load_dataset_record ()","title":"__init__"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DatasetRecordManager.dataset_src_exists","text":"Checks if a dataset record exists with the specified URL. Parameters: src_url ( str ) \u2013 The source/reference URL of the dataset. Returns: bool \u2013 True if the record exists, False otherwise. Source code in nlpinitiative\\data_preparation\\data_management.py 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 def dataset_src_exists ( self , src_url : str ) -> bool : \"\"\"Checks if a dataset record exists with the specified URL. Parameters ---------- src_url : str The source/reference URL of the dataset. Returns ------- bool True if the record exists, False otherwise. \"\"\" return len ( self . rec_df [ self . rec_df [ \"Dataset Reference URL\" ] == src_url ]) > 0","title":"dataset_src_exists"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DatasetRecordManager.get_ds_record_copy","text":"Returns a copy of the dataset record dataframe Returns: DataFrame \u2013 A copy of the record as a Pandas DataFrame. Source code in nlpinitiative\\data_preparation\\data_management.py 685 686 687 688 689 690 691 692 693 694 def get_ds_record_copy ( self ) -> pd . DataFrame : \"\"\"Returns a copy of the dataset record dataframe Returns ------- DataFrame A copy of the record as a Pandas DataFrame. \"\"\" return self . rec_df . copy ( deep = True )","title":"get_ds_record_copy"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DatasetRecordManager.get_entry_by_raw_fn","text":"Retrieves the raw file name of the specified dataset record. Parameters: filename ( str ) \u2013 The file name of the normalized version of the dataset. Raises: Exception \u2013 If the method failed to retrieve the specified record. Returns: str \u2013 The file name of the non-normalized version of the dataset. Source code in nlpinitiative\\data_preparation\\data_management.py 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 def get_entry_by_raw_fn ( self , filename : str ) -> tuple : \"\"\"Retrieves the raw file name of the specified dataset record. Parameters ---------- filename : str The file name of the normalized version of the dataset. Raises ------ Exception If the method failed to retrieve the specified record. Returns ------- str The file name of the non-normalized version of the dataset. \"\"\" try : row = tuple ( self . rec_df [ self . rec_df [ \"Raw Dataset Filename\" ] == filename ] . values [ 0 ]) return row except Exception as e : err_msg = f \"Failed to retrieve row of Dataset Record - { e } \" logger . error ( err_msg ) raise Exception ( err_msg )","title":"get_entry_by_raw_fn"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DatasetRecordManager.load_dataset_record","text":"Loads the dataset record into a Pandas Dataframe. Returns: DataFrame \u2013 The record of datasets as a DataFrame. Source code in nlpinitiative\\data_preparation\\data_management.py 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 def load_dataset_record ( self ) -> pd . DataFrame : \"\"\"Loads the dataset record into a Pandas Dataframe. Returns ------- DataFrame The record of datasets as a DataFrame. \"\"\" if os . path . exists ( DATA_DIR / \"dataset_record.csv\" ): ds_rec_df = pd . read_csv ( filepath_or_buffer = DATA_DIR / \"dataset_record.csv\" ) else : ds_rec_df = pd . DataFrame ( columns = [ \"Dataset ID\" , \"Dataset Reference URL\" , \"Dataset Download URL\" , \"Raw Dataset Filename\" , \"Conversion Schema Filename\" , \"Converted Filename\" , ] ) ds_rec_df . to_csv ( DATA_DIR / \"dataset_record.csv\" , index = False ) return ds_rec_df","title":"load_dataset_record"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DatasetRecordManager.remove_entry","text":"Facilitates removal of a dataset record. Parameters: ds_id ( str ) \u2013 The name/id of the dataset to be removed. Source code in nlpinitiative\\data_preparation\\data_management.py 743 744 745 746 747 748 749 750 751 752 753 754 def remove_entry ( self , ds_id : str ): \"\"\"Facilitates removal of a dataset record. Parameters ---------- ds_id : str The name/id of the dataset to be removed. \"\"\" if ds_id is not None : self . rec_df = self . rec_df [ self . rec_df [ \"Dataset ID\" ] != ds_id ] self . save_ds_record ()","title":"remove_entry"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DatasetRecordManager.save_ds_record","text":"Faciliates saving the current state of the dataset record as a csv file. Source code in nlpinitiative\\data_preparation\\data_management.py 664 665 666 667 def save_ds_record ( self ): \"\"\"Faciliates saving the current state of the dataset record as a csv file.\"\"\" self . rec_df . to_csv ( DATA_DIR / \"dataset_record.csv\" , index = False )","title":"save_ds_record"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_management.DatasetRecordManager.update","text":"Adds a new dataset record or updates an existing record. Parameters: ds_id ( str ) \u2013 The name/id of the dataset (used for tracking purposes). src_url ( str , default: None ) \u2013 The source or reference URL for an imported dataset (default is None). download_url ( str , default: None ) \u2013 The download URL of the dataset (default is None). raw_ds_filename ( str , default: None ) \u2013 The file name of the raw (non-normalized) dataset (default is None). normalization_schema_filename ( str , default: None ) \u2013 The file name of the conversion schema used to normalize the dataset (default is None). normalized_ds_filename ( str , default: None ) \u2013 The file name of the normalized version of the dataset (default is None). Source code in nlpinitiative\\data_preparation\\data_management.py 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 def update ( self , ds_id : str , src_url : str = None , download_url : str = None , raw_ds_filename : str = None , normalization_schema_filename : str = None , normalized_ds_filename : str = None , ): \"\"\"Adds a new dataset record or updates an existing record. Parameters ---------- ds_id : str The name/id of the dataset (used for tracking purposes). src_url : str, optional The source or reference URL for an imported dataset (default is None). download_url : str, optional The download URL of the dataset (default is None). raw_ds_filename : str, optional The file name of the raw (non-normalized) dataset (default is None). normalization_schema_filename : str, optional The file name of the conversion schema used to normalize the dataset (default is None). normalized_ds_filename : str, optional The file name of the normalized version of the dataset (default is None). \"\"\" if ds_id is not None : new_df = pd . DataFrame ( { \"Dataset ID\" : [ ds_id ], \"Dataset Reference URL\" : [ src_url ], \"Dataset Download URL\" : [ download_url ], \"Raw Dataset Filename\" : [ raw_ds_filename ], \"Conversion Schema Filename\" : [ normalization_schema_filename ], \"Converted Filename\" : [ normalized_ds_filename ], } ) temp_df = pd . concat ([ self . rec_df , new_df ]) . drop_duplicates ( subset = [ \"Dataset ID\" , \"Dataset Reference URL\" ], keep = \"last\" ) self . rec_df = temp_df self . save_ds_record ()","title":"update"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_normalize","text":"Script file used for facillitating normalization of a third-party dataset(s) into the format that we will utilize for training the model. This script can also handle merging complimentary datasets (datasets that come from the same source that may have minor differences in labeling scheme).","title":"data_normalize"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_normalize.DataNormalizer","text":"A class used to normalize datasets to the format utilized for model training. Source code in nlpinitiative\\data_preparation\\data_normalize.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 class DataNormalizer : \"\"\"A class used to normalize datasets to the format utilized for model training.\"\"\" def _merge_dataframes ( self , df1 : pd . DataFrame , df2 : pd . DataFrame , merge_col : str ) -> pd . DataFrame : \"\"\"Merges two Pandas DataFrames. Parameters ---------- df1 : DataFrame DataFrame to be merged. df2 : DataFrame DataFrame to be merged. merge_col : str The column for which the DataFrames are to be merged on. Returns ------- DataFrame The resulting merged DataFrame. \"\"\" new_df = pd . merge ( df1 , df2 , on = merge_col , how = \"left\" ) . fillna ( 0.0 ) return new_df def _load_src_file ( self , path : Path ) -> pd . DataFrame : \"\"\"Load a source file into a Pandas DataFrame. Parameters ---------- path : Path The file path to the source file. Returns ------- DataFrame The source file data in a DataFrame. \"\"\" return pd . read_csv ( path ) def _load_conv_schema ( self , path : Path ) -> dict [ str : str ]: \"\"\"Loads the dataset conversion schema that will be used for normalizing a dataset. Parameters ---------- path : Path The file path to the JSON conversion schema. Returns ------- dict[str:str] The conversion schema as a JSON object. \"\"\" return json . load ( open ( path , \"r\" )) def normalize_datasets ( self , files : list [ Path ], cv_path : Path ) -> pd . DataFrame : \"\"\"Loads, merges and normalizes one or more datasets to the schema used for model training. Parameters ---------- files : list[Path] A list of dataset file paths that are to be merged and normalized. cv_path : Path The file path to the JSON conversion schema. Returns ------- Dataframe The normalized dataset(s) as a DataFrame. \"\"\" num_files = len ( files ) src_df = None for i in range ( 0 , num_files ): df = self . _load_src_file ( RAW_DATA_DIR / files [ i ]) if src_df is not None : src_df = self . _merge_dataframes ( src_df , df , data_col_name ) else : src_df = df conv_scema = self . _load_conv_schema ( cv_path ) data_col_name = conv_scema [ \"data_col\" ] if conv_scema [ \"mapping_type\" ] == \"many2many\" : schema_cats = conv_scema [ \"column_mapping\" ] . keys () master_df = pd . DataFrame ( data = [], columns = DATASET_COLS ) master_df [ DATASET_COLS [ 0 ]] = src_df [ data_col_name ] for cat in schema_cats : from_columns = conv_scema [ \"column_mapping\" ][ cat ] if len ( from_columns ) > 0 : if cat == DATASET_COLS [ 1 ]: master_df [ cat ] = src_df [ from_columns ] . gt ( 0.0 ) . astype ( pd . Int64Dtype ()) else : master_df [ cat ] = src_df [ from_columns ] . sum ( axis = 1 ) else : master_df [ cat ] = 0.0 cols = master_df . columns for col in cols : if \"unnamed\" in col . lower (): master_df . drop ( col ) indices_to_purge = [] for index , row in master_df . iterrows (): is_hate = row [ DATASET_COLS [ 1 ]] == 1 has_cat_values = row [ CATEGORY_LABELS ] . sum () > 0.0 if is_hate and not has_cat_values or not is_hate and has_cat_values : indices_to_purge . append ( index ) master_df . drop ( master_df . index [ indices_to_purge ], inplace = True ) else : source_column = conv_scema [ \"single_column_label\" ] cat_mapping = conv_scema [ \"column_mapping\" ] data = [] for _ , row in src_df . iterrows (): row_data = [] text = row [ data_col_name ] type_discr = row [ source_column ] row_data . append ( text ) row_data . append ( 1 ) groups = [] for cat in cat_mapping . keys (): if type_discr in cat_mapping [ cat ]: if cat not in groups : groups . append ( cat ) try : val_breakdown = 1 / len ( groups ) for cat in cat_mapping . keys (): val = 0.0 if cat in groups : val = val_breakdown row_data . append ( val ) data . append ( row_data ) except : pass master_df = pd . DataFrame ( data = data , columns = DATASET_COLS ) return master_df","title":"DataNormalizer"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_normalize.DataNormalizer.normalize_datasets","text":"Loads, merges and normalizes one or more datasets to the schema used for model training. Parameters: files ( list [ Path ] ) \u2013 A list of dataset file paths that are to be merged and normalized. cv_path ( Path ) \u2013 The file path to the JSON conversion schema. Returns: Dataframe \u2013 The normalized dataset(s) as a DataFrame. Source code in nlpinitiative\\data_preparation\\data_normalize.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def normalize_datasets ( self , files : list [ Path ], cv_path : Path ) -> pd . DataFrame : \"\"\"Loads, merges and normalizes one or more datasets to the schema used for model training. Parameters ---------- files : list[Path] A list of dataset file paths that are to be merged and normalized. cv_path : Path The file path to the JSON conversion schema. Returns ------- Dataframe The normalized dataset(s) as a DataFrame. \"\"\" num_files = len ( files ) src_df = None for i in range ( 0 , num_files ): df = self . _load_src_file ( RAW_DATA_DIR / files [ i ]) if src_df is not None : src_df = self . _merge_dataframes ( src_df , df , data_col_name ) else : src_df = df conv_scema = self . _load_conv_schema ( cv_path ) data_col_name = conv_scema [ \"data_col\" ] if conv_scema [ \"mapping_type\" ] == \"many2many\" : schema_cats = conv_scema [ \"column_mapping\" ] . keys () master_df = pd . DataFrame ( data = [], columns = DATASET_COLS ) master_df [ DATASET_COLS [ 0 ]] = src_df [ data_col_name ] for cat in schema_cats : from_columns = conv_scema [ \"column_mapping\" ][ cat ] if len ( from_columns ) > 0 : if cat == DATASET_COLS [ 1 ]: master_df [ cat ] = src_df [ from_columns ] . gt ( 0.0 ) . astype ( pd . Int64Dtype ()) else : master_df [ cat ] = src_df [ from_columns ] . sum ( axis = 1 ) else : master_df [ cat ] = 0.0 cols = master_df . columns for col in cols : if \"unnamed\" in col . lower (): master_df . drop ( col ) indices_to_purge = [] for index , row in master_df . iterrows (): is_hate = row [ DATASET_COLS [ 1 ]] == 1 has_cat_values = row [ CATEGORY_LABELS ] . sum () > 0.0 if is_hate and not has_cat_values or not is_hate and has_cat_values : indices_to_purge . append ( index ) master_df . drop ( master_df . index [ indices_to_purge ], inplace = True ) else : source_column = conv_scema [ \"single_column_label\" ] cat_mapping = conv_scema [ \"column_mapping\" ] data = [] for _ , row in src_df . iterrows (): row_data = [] text = row [ data_col_name ] type_discr = row [ source_column ] row_data . append ( text ) row_data . append ( 1 ) groups = [] for cat in cat_mapping . keys (): if type_discr in cat_mapping [ cat ]: if cat not in groups : groups . append ( cat ) try : val_breakdown = 1 / len ( groups ) for cat in cat_mapping . keys (): val = 0.0 if cat in groups : val = val_breakdown row_data . append ( val ) data . append ( row_data ) except : pass master_df = pd . DataFrame ( data = data , columns = DATASET_COLS ) return master_df","title":"normalize_datasets"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_process","text":"Script file used for facillitating dataset preparation and preprocessing for use in model training.","title":"data_process"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_process.DataProcessor","text":"A class used for performing preprocessing/tokenization. Source code in nlpinitiative\\data_preparation\\data_process.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 class DataProcessor : \"\"\"A class used for performing preprocessing/tokenization.\"\"\" def dataset_from_file ( self , filename : str , srcdir : Path = PROCESSED_DATA_DIR ) -> DatasetDict : \"\"\"Loads a dataset from a specified file into a Dataset object. Parameters ---------- filename : str The file name of the dataset to be loaded. srcdir : Path, optional The file path to the directory that the dataset is stored (default is data/processed). Raises ------ Exception If the file specified does not exist. Returns ------- DatasetDict The loaded dataset as a DatasetDict object. \"\"\" if filename and os . path . exists ( os . path . join ( srcdir , filename )): ext = os . path . splitext ( filename )[ - 1 ] ext = ext . replace ( \".\" , \"\" ) ds = load_dataset ( ext , data_files = os . path . join ( srcdir , filename ), split = \"train\" ) . train_test_split ( test_size = TRAIN_TEST_SPLIT ) return ds else : raise Exception ( \"Invalid file name or file path\" ) def bin_ml_dataset_split ( self , dataset : Dataset ) -> tuple [ DatasetDict , DatasetDict ]: \"\"\"Forms train/test split for use in the binary classification and multilabel regression models training. Takes a loaded dataset and splits it into two separate datasets with the required corresponding labels (the binary model dataset will only have the DISCRIMINATORY column, while the multilabel regression model will only have the columns consisting of the discrimination categories). Additionally, the function will split the prepared datasets into a train/test split. Parameters ---------- dataset : DatasetDict The dataset to be processed. Returns ------- DatasetDict The prepared binary model and multilabel model dataset objects. \"\"\" def get_bin_ds () -> DatasetDict : \"\"\"Prepares the binary model dataset. Returns ------- DatasetDict The prepared binary model dataset. \"\"\" train = dataset [ \"train\" ] . remove_columns ( CATEGORY_LABELS ) test = dataset [ \"test\" ] . remove_columns ( CATEGORY_LABELS ) return DatasetDict ( { \"train\" : train . rename_column ( \"DISCRIMINATORY\" , \"label\" ), \"test\" : test . rename_column ( \"DISCRIMINATORY\" , \"label\" ), } ) def get_ml_regr_ds () -> DatasetDict : \"\"\"Prepares the multilabel model dataset. Returns ------- DatasetDict The prepared multilabel model dataset. \"\"\" def combine_labels ( ex_ds ): \"\"\"Consolidates the mutiple dataset columns for categories into a single column consisting of a list of the category data. Parameters ---------- ex_ds : DatasetDict The multilabel dataset to be corrected. Returns ------- DatasetDict The corrected dataset. \"\"\" ex_ds [ \"labels\" ] = [ float ( ex_ds [ \"GENDER\" ]), float ( ex_ds [ \"RACE\" ]), float ( ex_ds [ \"SEXUALITY\" ]), float ( ex_ds [ \"DISABILITY\" ]), float ( ex_ds [ \"RELIGION\" ]), float ( ex_ds [ \"UNSPECIFIED\" ]), ] return ex_ds train = dataset [ \"train\" ] . remove_columns ( BINARY_LABELS ) train = train . map ( combine_labels ) train = train . remove_columns ( CATEGORY_LABELS ) test = dataset [ \"test\" ] . remove_columns ( BINARY_LABELS ) test = test . map ( combine_labels ) test = test . remove_columns ( CATEGORY_LABELS ) return DatasetDict ({ \"train\" : train , \"test\" : test }) return get_bin_ds (), get_ml_regr_ds () def get_tokenizer ( self , model_type : str = DEF_MODEL ): \"\"\"Generates a tokenizer for the specified model type. Parameters ---------- model_type : str, optional The model type for which the tokenizer is to be created. Returns ------- PreTrainedTokenizer | PreTrainedTokenizerFast The tokenizer object. \"\"\" return AutoTokenizer . from_pretrained ( model_type ) def get_dataset_metadata ( self , dataset : DatasetDict ) -> dict : \"\"\"Gathers metadata for the given dataset into a dict for use in model training. Extracts dataset metadata to include a list of the datasets labels, a dict mapping the labels to their respective indices and a dict mapping the indices to the respective label. Parameters ---------- dataset : DatasetDict The dataset to retrieve metadata from. Returns ------- dict The metadata for the dataset within a dict object. \"\"\" lbls = [ label for label in dataset [ \"train\" ] . features . keys () if label not in [ DATASET_COLS [ 0 ]] ] lbl2idx = { lbl : idx for idx , lbl in enumerate ( lbls )} idx2lbl = { idx : lbl for idx , lbl in enumerate ( lbls )} return { \"labels\" : lbls , \"lbl2idx\" : lbl2idx , \"idx2lbl\" : idx2lbl } def preprocess ( self , dataset : DatasetDict , labels : list [ str ], tokenizer ) -> DatasetDict : \"\"\"Preprocesses and tokenizes a given dataset. Parameters ---------- dataset : DatasetDict The dataset to to be preprocessed/tokenized. labels : list[str] The datasets labels. tokenizer : PreTrainedTokenizer | PreTrainedTokenizerFast The tokenizer used for tokenizing the dataset. Returns ------- DatasetDict The preprocessed dataset. \"\"\" def preprocess_runner ( data : DatasetDict ): \"\"\"Runner for performing tokenization. Parameters ---------- data : DatasetDict The dataset to to be tokenized. Returns ------- DatasetDict The tokenized dataset. \"\"\" return tokenizer ( data [ DATASET_COLS [ 0 ]], padding = \"max_length\" , truncation = True , max_length = 128 ) if not labels : labels = [ label for label in dataset [ \"train\" ] . features . keys () if label not in [ DATASET_COLS [ 0 ]] ] if not tokenizer : tokenizer = self . get_tokenizer () encoded_ds = dataset . map ( preprocess_runner , batched = True ) encoded_ds . set_format ( \"torch\" ) return encoded_ds","title":"DataProcessor"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_process.DataProcessor.bin_ml_dataset_split","text":"Forms train/test split for use in the binary classification and multilabel regression models training. Takes a loaded dataset and splits it into two separate datasets with the required corresponding labels (the binary model dataset will only have the DISCRIMINATORY column, while the multilabel regression model will only have the columns consisting of the discrimination categories). Additionally, the function will split the prepared datasets into a train/test split. Parameters: dataset ( DatasetDict ) \u2013 The dataset to be processed. Returns: DatasetDict \u2013 The prepared binary model and multilabel model dataset objects. Source code in nlpinitiative\\data_preparation\\data_process.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def bin_ml_dataset_split ( self , dataset : Dataset ) -> tuple [ DatasetDict , DatasetDict ]: \"\"\"Forms train/test split for use in the binary classification and multilabel regression models training. Takes a loaded dataset and splits it into two separate datasets with the required corresponding labels (the binary model dataset will only have the DISCRIMINATORY column, while the multilabel regression model will only have the columns consisting of the discrimination categories). Additionally, the function will split the prepared datasets into a train/test split. Parameters ---------- dataset : DatasetDict The dataset to be processed. Returns ------- DatasetDict The prepared binary model and multilabel model dataset objects. \"\"\" def get_bin_ds () -> DatasetDict : \"\"\"Prepares the binary model dataset. Returns ------- DatasetDict The prepared binary model dataset. \"\"\" train = dataset [ \"train\" ] . remove_columns ( CATEGORY_LABELS ) test = dataset [ \"test\" ] . remove_columns ( CATEGORY_LABELS ) return DatasetDict ( { \"train\" : train . rename_column ( \"DISCRIMINATORY\" , \"label\" ), \"test\" : test . rename_column ( \"DISCRIMINATORY\" , \"label\" ), } ) def get_ml_regr_ds () -> DatasetDict : \"\"\"Prepares the multilabel model dataset. Returns ------- DatasetDict The prepared multilabel model dataset. \"\"\" def combine_labels ( ex_ds ): \"\"\"Consolidates the mutiple dataset columns for categories into a single column consisting of a list of the category data. Parameters ---------- ex_ds : DatasetDict The multilabel dataset to be corrected. Returns ------- DatasetDict The corrected dataset. \"\"\" ex_ds [ \"labels\" ] = [ float ( ex_ds [ \"GENDER\" ]), float ( ex_ds [ \"RACE\" ]), float ( ex_ds [ \"SEXUALITY\" ]), float ( ex_ds [ \"DISABILITY\" ]), float ( ex_ds [ \"RELIGION\" ]), float ( ex_ds [ \"UNSPECIFIED\" ]), ] return ex_ds train = dataset [ \"train\" ] . remove_columns ( BINARY_LABELS ) train = train . map ( combine_labels ) train = train . remove_columns ( CATEGORY_LABELS ) test = dataset [ \"test\" ] . remove_columns ( BINARY_LABELS ) test = test . map ( combine_labels ) test = test . remove_columns ( CATEGORY_LABELS ) return DatasetDict ({ \"train\" : train , \"test\" : test }) return get_bin_ds (), get_ml_regr_ds ()","title":"bin_ml_dataset_split"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_process.DataProcessor.dataset_from_file","text":"Loads a dataset from a specified file into a Dataset object. Parameters: filename ( str ) \u2013 The file name of the dataset to be loaded. srcdir ( Path , default: PROCESSED_DATA_DIR ) \u2013 The file path to the directory that the dataset is stored (default is data/processed). Raises: Exception \u2013 If the file specified does not exist. Returns: DatasetDict \u2013 The loaded dataset as a DatasetDict object. Source code in nlpinitiative\\data_preparation\\data_process.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def dataset_from_file ( self , filename : str , srcdir : Path = PROCESSED_DATA_DIR ) -> DatasetDict : \"\"\"Loads a dataset from a specified file into a Dataset object. Parameters ---------- filename : str The file name of the dataset to be loaded. srcdir : Path, optional The file path to the directory that the dataset is stored (default is data/processed). Raises ------ Exception If the file specified does not exist. Returns ------- DatasetDict The loaded dataset as a DatasetDict object. \"\"\" if filename and os . path . exists ( os . path . join ( srcdir , filename )): ext = os . path . splitext ( filename )[ - 1 ] ext = ext . replace ( \".\" , \"\" ) ds = load_dataset ( ext , data_files = os . path . join ( srcdir , filename ), split = \"train\" ) . train_test_split ( test_size = TRAIN_TEST_SPLIT ) return ds else : raise Exception ( \"Invalid file name or file path\" )","title":"dataset_from_file"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_process.DataProcessor.get_dataset_metadata","text":"Gathers metadata for the given dataset into a dict for use in model training. Extracts dataset metadata to include a list of the datasets labels, a dict mapping the labels to their respective indices and a dict mapping the indices to the respective label. Parameters: dataset ( DatasetDict ) \u2013 The dataset to retrieve metadata from. Returns: dict \u2013 The metadata for the dataset within a dict object. Source code in nlpinitiative\\data_preparation\\data_process.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 def get_dataset_metadata ( self , dataset : DatasetDict ) -> dict : \"\"\"Gathers metadata for the given dataset into a dict for use in model training. Extracts dataset metadata to include a list of the datasets labels, a dict mapping the labels to their respective indices and a dict mapping the indices to the respective label. Parameters ---------- dataset : DatasetDict The dataset to retrieve metadata from. Returns ------- dict The metadata for the dataset within a dict object. \"\"\" lbls = [ label for label in dataset [ \"train\" ] . features . keys () if label not in [ DATASET_COLS [ 0 ]] ] lbl2idx = { lbl : idx for idx , lbl in enumerate ( lbls )} idx2lbl = { idx : lbl for idx , lbl in enumerate ( lbls )} return { \"labels\" : lbls , \"lbl2idx\" : lbl2idx , \"idx2lbl\" : idx2lbl }","title":"get_dataset_metadata"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_process.DataProcessor.get_tokenizer","text":"Generates a tokenizer for the specified model type. Parameters: model_type ( str , default: DEF_MODEL ) \u2013 The model type for which the tokenizer is to be created. Returns: PreTrainedTokenizer | PreTrainedTokenizerFast \u2013 The tokenizer object. Source code in nlpinitiative\\data_preparation\\data_process.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def get_tokenizer ( self , model_type : str = DEF_MODEL ): \"\"\"Generates a tokenizer for the specified model type. Parameters ---------- model_type : str, optional The model type for which the tokenizer is to be created. Returns ------- PreTrainedTokenizer | PreTrainedTokenizerFast The tokenizer object. \"\"\" return AutoTokenizer . from_pretrained ( model_type )","title":"get_tokenizer"},{"location":"dataset-management/#nlpinitiative.data_preparation.data_process.DataProcessor.preprocess","text":"Preprocesses and tokenizes a given dataset. Parameters: dataset ( DatasetDict ) \u2013 The dataset to to be preprocessed/tokenized. labels ( list [ str ] ) \u2013 The datasets labels. tokenizer ( PreTrainedTokenizer | PreTrainedTokenizerFast ) \u2013 The tokenizer used for tokenizing the dataset. Returns: DatasetDict \u2013 The preprocessed dataset. Source code in nlpinitiative\\data_preparation\\data_process.py 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def preprocess ( self , dataset : DatasetDict , labels : list [ str ], tokenizer ) -> DatasetDict : \"\"\"Preprocesses and tokenizes a given dataset. Parameters ---------- dataset : DatasetDict The dataset to to be preprocessed/tokenized. labels : list[str] The datasets labels. tokenizer : PreTrainedTokenizer | PreTrainedTokenizerFast The tokenizer used for tokenizing the dataset. Returns ------- DatasetDict The preprocessed dataset. \"\"\" def preprocess_runner ( data : DatasetDict ): \"\"\"Runner for performing tokenization. Parameters ---------- data : DatasetDict The dataset to to be tokenized. Returns ------- DatasetDict The tokenized dataset. \"\"\" return tokenizer ( data [ DATASET_COLS [ 0 ]], padding = \"max_length\" , truncation = True , max_length = 128 ) if not labels : labels = [ label for label in dataset [ \"train\" ] . features . keys () if label not in [ DATASET_COLS [ 0 ]] ] if not tokenizer : tokenizer = self . get_tokenizer () encoded_ds = dataset . map ( preprocess_runner , batched = True ) encoded_ds . set_format ( \"torch\" ) return encoded_ds","title":"preprocess"},{"location":"getting-started/","text":"Getting started This is where you describe how to get set up on a clean install, including the commands necessary to get the raw data (using the sync_data_from_s3 command, for example), and then how to make the cleaned, final data sets.","title":"Getting started"},{"location":"getting-started/#getting-started","text":"This is where you describe how to get set up on a clean install, including the commands necessary to get the raw data (using the sync_data_from_s3 command, for example), and then how to make the cleaned, final data sets.","title":"Getting started"},{"location":"model-management/","text":"Model Pipeline Operations Model Training and Evaluation nlpinitiative.modeling.train Script file containing the logic for training NLP models. RegressionTrainer Bases: Trainer A custom class for overriding the compute_loss method used in regression model training. Source code in nlpinitiative\\modeling\\train.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class RegressionTrainer ( Trainer ): \"\"\"A custom class for overriding the compute_loss method used in regression model training.\"\"\" def compute_loss ( self , model , inputs , return_outputs : bool = False , ** kwargs ): \"\"\"Overridden comput_loss method necessary for training the multilabel regression model. Parameters ---------- model The model being trained. inputs The dataset being used to train the model. return_outputs : bool, optional True if the outputs should be returned with the losses, False otherwise (default is False). **kwargs Any other additional parameters/configurations to use. Returns ------- tuple[Any, Any] | Any A tuple consisting of the loss values and the corresponding outputs (if return_outputs is True), or just the loss values (if return_outputs is False). \"\"\" labels = inputs . get ( \"labels\" ) outputs = model ( ** inputs ) logits = outputs . get ( \"logits\" ) # Ensure labels and logits are float labels = labels . to ( torch . float32 ) logits = logits . to ( torch . float32 ) loss_fct = torch . nn . MSELoss () loss = loss_fct ( logits , labels ) return ( loss , outputs ) if return_outputs else loss compute_loss ( model , inputs , return_outputs = False , ** kwargs ) Overridden comput_loss method necessary for training the multilabel regression model. Parameters: model \u2013 The model being trained. inputs \u2013 The dataset being used to train the model. return_outputs ( bool , default: False ) \u2013 True if the outputs should be returned with the losses, False otherwise (default is False). **kwargs \u2013 Any other additional parameters/configurations to use. Returns: tuple [ Any , Any ] | Any \u2013 A tuple consisting of the loss values and the corresponding outputs (if return_outputs is True), or just the loss values (if return_outputs is False). Source code in nlpinitiative\\modeling\\train.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def compute_loss ( self , model , inputs , return_outputs : bool = False , ** kwargs ): \"\"\"Overridden comput_loss method necessary for training the multilabel regression model. Parameters ---------- model The model being trained. inputs The dataset being used to train the model. return_outputs : bool, optional True if the outputs should be returned with the losses, False otherwise (default is False). **kwargs Any other additional parameters/configurations to use. Returns ------- tuple[Any, Any] | Any A tuple consisting of the loss values and the corresponding outputs (if return_outputs is True), or just the loss values (if return_outputs is False). \"\"\" labels = inputs . get ( \"labels\" ) outputs = model ( ** inputs ) logits = outputs . get ( \"logits\" ) # Ensure labels and logits are float labels = labels . to ( torch . float32 ) logits = logits . to ( torch . float32 ) loss_fct = torch . nn . MSELoss () loss = loss_fct ( logits , labels ) return ( loss , outputs ) if return_outputs else loss bin_train_args ( output_dir = MODELS_DIR / 'binary_classification' , eval_strat = 'steps' , save_strat = 'steps' , logging_steps = 500 , save_steps = 500 , learn_rate = 2e-05 , batch_sz = 8 , num_train_epochs = 3 , weight_decay = 0.01 , best_model_at_end = True , best_model_metric = 'f1' , greater_better = True ) Generates training arguments for use in binary classification model training. Parameters: output_dir ( str , default: MODELS_DIR / 'binary_classification' ) \u2013 The output directory to store the trained model (default is models/binary_classification). eval_strat ( str , default: 'steps' ) \u2013 The evaluation strategy (default is 'steps'). save_strat ( str , default: 'steps' ) \u2013 The save strategy (default is 'steps'). logging_steps ( int , default: 500 ) \u2013 The periodicity for which logging will occur (default is 500). save_steps ( int , default: 500 ) \u2013 The step periodicity for which a model will be saved (default is 500). learn_rate ( float , default: 2e-05 ) \u2013 A hyper parameter for determining model parameter adjustment during training (default is 2e-5). batch_sz ( int , default: 8 ) \u2013 The training data batch sizes (default is 8). num_train_epochs ( int , default: 3 ) \u2013 The number of training epochs to be performed (default is 3). weight_decay ( float , default: 0.01 ) \u2013 The weight decay to apply (default is 0.01). best_model_at_end ( bool , default: True ) \u2013 True if the best model is to be saved at the completion of model training, False otherwise (default is True). best_model_metric ( str , default: 'f1' ) \u2013 The metric used for determining the best performing model (default is 'f1'). greater_better ( bool , default: True ) \u2013 True for if the better performing model should have the greater value (based on the specified metric), False otherwise (default is True). Returns: TrainingArguments \u2013 The training arguments object used for conducting binary classification model training. Source code in nlpinitiative\\modeling\\train.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 def bin_train_args ( output_dir : Path = MODELS_DIR / \"binary_classification\" , eval_strat : str = \"steps\" , save_strat : str = \"steps\" , logging_steps : int = 500 , save_steps : int = 500 , learn_rate : float = 2e-5 , batch_sz : int = 8 , num_train_epochs : int = 3 , weight_decay : float = 0.01 , best_model_at_end : bool = True , best_model_metric : str = \"f1\" , greater_better : bool = True , ): \"\"\"Generates training arguments for use in binary classification model training. Parameters ---------- output_dir : str, optional The output directory to store the trained model (default is models/binary_classification). eval_strat : str, optional The evaluation strategy (default is 'steps'). save_strat : str, optional The save strategy (default is 'steps'). logging_steps : int, optional The periodicity for which logging will occur (default is 500). save_steps : int, optional The step periodicity for which a model will be saved (default is 500). learn_rate : float, optional A hyper parameter for determining model parameter adjustment during training (default is 2e-5). batch_sz : int, optional The training data batch sizes (default is 8). num_train_epochs : int, optional The number of training epochs to be performed (default is 3). weight_decay : float, optional The weight decay to apply (default is 0.01). best_model_at_end : bool, optional True if the best model is to be saved at the completion of model training, False otherwise (default is True). best_model_metric : str, optional The metric used for determining the best performing model (default is 'f1'). greater_better : bool, optional True for if the better performing model should have the greater value (based on the specified metric), False otherwise (default is True). Returns ------- TrainingArguments The training arguments object used for conducting binary classification model training. \"\"\" return _train_args ( output_dir , eval_strat , save_strat , logging_steps , save_steps , learn_rate , batch_sz , num_train_epochs , weight_decay , best_model_at_end , best_model_metric , greater_better , ) compute_bin_metrics ( eval_predictions ) Computes the metrics values resulting from the evaluation of the trained binary classification model. Parameters: eval_predictions \u2013 A tuple consisting of the label and prediction pair. Returns: dict [ str , Any ] \u2013 A dict consisting of the computed metrics values from evaluation of the trained binary classification model. Source code in nlpinitiative\\modeling\\train.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def compute_bin_metrics ( eval_predictions ): \"\"\"Computes the metrics values resulting from the evaluation of the trained binary classification model. Parameters ---------- eval_predictions A tuple consisting of the label and prediction pair. Returns ------- dict[str, Any] A dict consisting of the computed metrics values from evaluation of the trained binary classification model. \"\"\" predictions , lbls = eval_predictions preds = predictions . argmax ( axis = 1 ) probs = torch . nn . functional . softmax ( torch . tensor ( predictions ), dim = 1 ) . numpy () prec , recall , f1 , _ = precision_recall_fscore_support ( lbls , preds , average = \"binary\" ) acc = accuracy_score ( lbls , preds ) auprc = average_precision_score ( lbls , probs [:, 0 ]) auroc = roc_auc_score ( lbls , probs [:, 0 ]) return { \"accuracy\" : acc , \"precision\" : prec , \"recall\" : recall , \"f1\" : f1 , \"auprc\" : auprc , \"auroc\" : auroc , } compute_reg_metrics ( eval_predictions ) Computes the metrics values resulting from the evaluation of the trained multilabel regression model. Parameters: eval_predictions \u2013 A tuple consisting of the labels corresponding prediction pair. Returns: dict [ str , float ] \u2013 A dict consisting of the computed metrics values from evaluation of the trained multilabel regression model. Source code in nlpinitiative\\modeling\\train.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def compute_reg_metrics ( eval_predictions ): \"\"\"Computes the metrics values resulting from the evaluation of the trained multilabel regression model. Parameters ---------- eval_predictions A tuple consisting of the labels corresponding prediction pair. Returns ------- dict[str, float] A dict consisting of the computed metrics values from evaluation of the trained multilabel regression model. \"\"\" preds , lbls = eval_predictions mse = mean_squared_error ( lbls , preds , multioutput = \"raw_values\" ) sqrt_mse = np . sqrt ( mse ) mae = mean_absolute_error ( lbls , preds , multioutput = \"raw_values\" ) r2 = r2_score ( lbls , preds , multioutput = \"raw_values\" ) pear_corr = [ pearsonr ( lbls [:, i ], preds [:, i ])[ 0 ] if len ( np . unique ( lbls [:, i ])) > 1 else np . nan for i in range ( lbls . shape [ 1 ]) ] mean_rmse = sqrt_mse . mean () mean_mae = mae . mean () mean_r2 = r2 . mean () mean_pear = np . nanmean ( pear_corr ) return { \"mean_rmse\" : mean_rmse , \"mean_mae\" : mean_mae , \"mean_r2\" : mean_r2 , \"mean_pearson\" : mean_pear , ## Per category \"rmse_per_cat\" : sqrt_mse . tolist (), \"mae_per_cat\" : mae . tolist (), \"r2_per_cat\" : r2 . tolist (), \"pearson_per_cat\" : pear_corr , } get_bin_model ( model_name = DEF_MODEL ) Generates a model object to be trained for binary classification. Parameters: model_name ( str , default: DEF_MODEL ) \u2013 The name of the pretrained model to be fine-tuned (default is the DEF_MODEL specified in nlpinitiative/config.py). Returns: PreTrainedModel \u2013 The corresponding pretrained subclass model object for binary classification. Source code in nlpinitiative\\modeling\\train.py 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 def get_bin_model ( model_name : str = DEF_MODEL ): \"\"\"Generates a model object to be trained for binary classification. Parameters ---------- model_name: str, optional The name of the pretrained model to be fine-tuned (default is the DEF_MODEL specified in nlpinitiative/config.py). Returns ------- PreTrainedModel The corresponding pretrained subclass model object for binary classification. \"\"\" return AutoModelForSequenceClassification . from_pretrained ( model_name ) get_ml_model ( model_name = DEF_MODEL ) Generates a model object to be trained for multilabel regression. Parameters: model_name ( str , default: DEF_MODEL ) \u2013 The name of the pretrained model to be fine-tuned (default is the DEF_MODEL specified in nlpinitiative/config.py). Returns: PreTrainedModel \u2013 The corresponding pretrained subclass model object for multilabel regression. Source code in nlpinitiative\\modeling\\train.py 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 def get_ml_model ( model_name : str = DEF_MODEL ): \"\"\"Generates a model object to be trained for multilabel regression. Parameters ---------- model_name: str, optional The name of the pretrained model to be fine-tuned (default is the DEF_MODEL specified in nlpinitiative/config.py). Returns ------- PreTrainedModel The corresponding pretrained subclass model object for multilabel regression. \"\"\" return AutoModelForSequenceClassification . from_pretrained ( model_name , num_labels = len ( CATEGORY_LABELS ) ) ml_regr_train_args ( output_dir = MODELS_DIR / 'multilabel_regression' , eval_strat = 'steps' , save_strat = 'steps' , logging_steps = 500 , save_steps = 500 , learn_rate = 2e-05 , batch_sz = 8 , num_train_epochs = 3 , weight_decay = 0.01 , best_model_at_end = True , best_model_metric = 'eval_mean_rmse' , greater_better = False ) Generates training arguments for use in multilabel regression model training. Parameters: output_dir ( str , default: MODELS_DIR / 'multilabel_regression' ) \u2013 The output directory to store the trained model (default is models/multilabel_regression). eval_strat ( str , default: 'steps' ) \u2013 The evaluation strategy (default is 'steps'). save_strat ( str , default: 'steps' ) \u2013 The save strategy (default is 'steps'). logging_steps ( int , default: 500 ) \u2013 The periodicity for which logging will occur (default is 500). save_steps ( int , default: 500 ) \u2013 The step periodicity for which a model will be saved (default is 500). learn_rate ( float , default: 2e-05 ) \u2013 A hyper parameter for determining model parameter adjustment during training (default is 2e-5). batch_sz ( int , default: 8 ) \u2013 The training data batch sizes (default is 8). num_train_epochs ( int , default: 3 ) \u2013 The number of training epochs to be performed (default is 3). weight_decay ( float , default: 0.01 ) \u2013 The weight decay to apply (default is 0.01). best_model_at_end ( bool , default: True ) \u2013 True if the best model is to be saved at the completion of model training, False otherwise (default is True). best_model_metric ( str , default: 'eval_mean_rmse' ) \u2013 The metric used for determining the best performing model (default is 'eval_mean_rmse'). greater_better ( bool , default: False ) \u2013 True for if the better performing model should have the greater value (based on the specified metric), False otherwise (default is False). Returns: TrainingArguments \u2013 The training arguments object used for conducting multilabel regression model training. Source code in nlpinitiative\\modeling\\train.py 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 def ml_regr_train_args ( output_dir : Path = MODELS_DIR / \"multilabel_regression\" , eval_strat : str = \"steps\" , save_strat : str = \"steps\" , logging_steps : int = 500 , save_steps : int = 500 , learn_rate : float = 2e-5 , batch_sz : int = 8 , num_train_epochs : int = 3 , weight_decay : float = 0.01 , best_model_at_end : bool = True , best_model_metric : str = \"eval_mean_rmse\" , greater_better : bool = False , ): \"\"\"Generates training arguments for use in multilabel regression model training. Parameters ---------- output_dir : str, optional The output directory to store the trained model (default is models/multilabel_regression). eval_strat : str, optional The evaluation strategy (default is 'steps'). save_strat : str, optional The save strategy (default is 'steps'). logging_steps : int, optional The periodicity for which logging will occur (default is 500). save_steps : int, optional The step periodicity for which a model will be saved (default is 500). learn_rate : float, optional A hyper parameter for determining model parameter adjustment during training (default is 2e-5). batch_sz : int, optional The training data batch sizes (default is 8). num_train_epochs : int, optional The number of training epochs to be performed (default is 3). weight_decay : float, optional The weight decay to apply (default is 0.01). best_model_at_end : bool, optional True if the best model is to be saved at the completion of model training, False otherwise (default is True). best_model_metric : str, optional The metric used for determining the best performing model (default is 'eval_mean_rmse'). greater_better : bool, optional True for if the better performing model should have the greater value (based on the specified metric), False otherwise (default is False). Returns ------- TrainingArguments The training arguments object used for conducting multilabel regression model training. \"\"\" return _train_args ( output_dir , eval_strat , save_strat , logging_steps , save_steps , learn_rate , batch_sz , num_train_epochs , weight_decay , best_model_at_end , best_model_metric , greater_better , ) train ( bin_trainer , ml_trainer , token = None ) Performs training on the binary classification and multilabel regression models. Parameters: bin_trainer ( Trainer ) \u2013 The trainer object for performing binary classification model training. ml_trainer ( Trainer ) \u2013 The trainer object for performing multilabel regression model training. token ( str , default: None ) \u2013 A Hugging Face token with read/write access privileges to allow exporting the trained models (default is None). Returns: tuple [ dict [ str , float ], dict [ str , float ]] \u2013 A tuple consisting of the dicts containing the metrics evaluation results for the binary classification and multilabel regression model training. Source code in nlpinitiative\\modeling\\train.py 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 def train ( bin_trainer : Trainer , ml_trainer : Trainer , token : str = None ): \"\"\"Performs training on the binary classification and multilabel regression models. Parameters ---------- bin_trainer : Trainer The trainer object for performing binary classification model training. ml_trainer : Trainer The trainer object for performing multilabel regression model training. token : str, optional A Hugging Face token with read/write access privileges to allow exporting the trained models (default is None). Returns ------- tuple[dict[str, float], dict[str, float]] A tuple consisting of the dicts containing the metrics evaluation results for the binary classification and multilabel regression model training. \"\"\" bin_trainer . train () bin_model = bin_trainer . model bin_model . save_pretrained ( save_directory = MODELS_DIR / \"binary_classification\" / \"best_model\" ) ml_trainer . train () ml_model = ml_trainer . model ml_model . save_pretrained ( save_directory = MODELS_DIR / \"multilabel_regression\" / \"best_model\" ) if token : upload_best_models ( token = token ) bin_eval = bin_trainer . evaluate () ml_eval = ml_trainer . evaluate () return bin_eval , ml_eval upload_best_models ( token ) Pushes the current best performing binary classification and multilabel regression models to their respective linked Hugging Face Model Repositories. Parameters: token ( str ) \u2013 A Hugging Face token with read/write access privileges to allow exporting the trained models (default is None). Source code in nlpinitiative\\modeling\\train.py 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 def upload_best_models ( token : str ): \"\"\"Pushes the current best performing binary classification and multilabel regression models to their respective linked Hugging Face Model Repositories. Parameters ---------- token : str, optional A Hugging Face token with read/write access privileges to allow exporting the trained models (default is None). \"\"\" def load_model ( path : Path ): \"\"\"Initializes a PreTrainedModel object from the specified model path. Parameters ---------- path : Path The file path to the model. Returns ------- PreTrainedModel An instantiated PreTrainedModel object. \"\"\" with open ( path / \"config.json\" ) as config_file : config_json = json . load ( config_file ) model_type = config_json [ \"model_type\" ] return AutoModelForSequenceClassification . from_pretrained ( path , model_type = model_type ) bin_model = load_model ( MODELS_DIR / \"binary_classification/best_model\" ) ml_model = load_model ( MODELS_DIR / \"multilabel_regression/best_model\" ) bin_model . push_to_hub ( BIN_REPO , token = token ) hfh . upload_file ( path_or_fileobj = MODELS_DIR / \"binary_classification/best_model/config.json\" , path_in_repo = \"config.json\" , repo_id = BIN_REPO , token = token , ) ml_model . push_to_hub ( ML_REPO , token = token ) hfh . upload_file ( path_or_fileobj = MODELS_DIR / \"multilabel_regression/best_model/config.json\" , path_in_repo = \"config.json\" , repo_id = ML_REPO , token = token , ) Model Inference nlpinitiative.modeling.predict Script file used for performing inference with an existing model. InferenceHandler A class that handles performing inference using the trained binary classification and multilabel regression models. Source code in nlpinitiative\\modeling\\predict.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 class InferenceHandler : \"\"\"A class that handles performing inference using the trained binary classification and multilabel regression models.\"\"\" def __init__ ( self , bin_model_path : Path = MODELS_DIR / \"binary_classification/best_model\" , ml_regr_model_path : Path = MODELS_DIR / \"multilabel_regression/best_model\" , ): \"\"\"Constructor for instantiating an InferenceHandler object. Parameters ---------- bin_model_path : Path, optional Directory path to the binary model tensor file (default is models/binary_classification/best_model). ml_regr_model_path : Path, optional Directory path to the multilabel regression model tensor file (default is models/multilabel_regression/best_model). \"\"\" self . bin_tokenizer , self . bin_model = self . init_model_and_tokenizer ( bin_model_path ) self . ml_regr_tokenizer , self . ml_regr_model = self . init_model_and_tokenizer ( ml_regr_model_path ) def init_model_and_tokenizer ( self , model_path : Path ): \"\"\"Initializes a model and tokenizer for use in inference using the models path. Parameters ---------- model_path : Path Directory path to the models tensor file. Returns ------- tuple[PreTrainedTokenizer | PreTrainedTokenizerFast, PreTrainedModel] A tuple containing the tokenizer and model objects. \"\"\" with open ( model_path / \"config.json\" ) as config_file : config_json = json . load ( config_file ) model_name = config_json [ \"_name_or_path\" ] model_type = config_json [ \"model_type\" ] tokenizer = AutoTokenizer . from_pretrained ( model_name ) model = AutoModelForSequenceClassification . from_pretrained ( model_path , model_type = model_type ) model . eval () return tokenizer , model def encode_binary ( self , text : str ): \"\"\"Preprocesses and tokenizes the input text for binary classification. Parameters ---------- text : str The input text to be preprocessed and tokenized. Returns ------- BatchEncoding The preprocessed and tokenized input text. \"\"\" bin_tokenized_input = self . bin_tokenizer ( text , return_tensors = \"pt\" , truncation = True , padding = True , max_length = 512 ) return bin_tokenized_input def encode_multilabel ( self , text : str ): \"\"\"Preprocesses and tokenizes the input text for multilabel regression. Parameters ---------- text : str The input text to be preprocessed and tokenized. Returns ------- BatchEncoding The preprocessed and tokenized input text. \"\"\" ml_tokenized_input = self . ml_regr_tokenizer ( text , return_tensors = \"pt\" , truncation = True , padding = True , max_length = 512 ) return ml_tokenized_input def encode_input ( self , text : str ): \"\"\"Preprocesses and tokenizes the input text sentiment classification (both models). Parameters ---------- text : str The input text to be preprocessed and tokenized. Returns ------- tuple[BatchEncoding, BatchEncoding] A tuple containing preprocessed and tokenized input text for both the binary and multilabel regression models. \"\"\" bin_inputs = self . encode_binary ( text ) ml_inputs = self . encode_multilabel ( text ) return bin_inputs , ml_inputs def classify_text ( self , text : str ): \"\"\"Performs inference on the input text to determine the binary classification and the multilabel regression for the categories. Determines whether the text is discriminatory. If it is discriminatory, it will then perform regression on the input text to determine the assesed percentage that each category applies. Parameters ---------- text : str The input text to be classified. Returns ------- dict[str, Any] The resulting classification and regression values for each category. \"\"\" result = { \"text_input\" : text , \"results\" : []} sent_res_arr = [] sentences = sent_tokenize ( text ) for sent in sentences : text_prediction , pred_class = self . discriminatory_inference ( sent ) sent_result = { \"sentence\" : sent , \"binary_classification\" : { \"classification\" : text_prediction , \"prediction_class\" : pred_class , }, \"multilabel_regression\" : None , } if pred_class == 1 : ml_results = { \"Gender\" : None , \"Race\" : None , \"Sexuality\" : None , \"Disability\" : None , \"Religion\" : None , \"Unspecified\" : None , } ml_infer_results = self . category_inference ( sent ) for idx , key in enumerate ( ml_results . keys ()): ml_results [ key ] = min ( max ( ml_infer_results [ idx ], 0.0 ), 1.0 ) sent_result [ \"multilabel_regression\" ] = ml_results sent_res_arr . append ( sent_result ) result [ \"results\" ] = sent_res_arr return result def discriminatory_inference ( self , text : str ): \"\"\"Performs inference on the input text to determine the binary classification. Parameters ---------- text : str The input text to be classified. Returns ------- tuple[str, Number] A tuple consisting of the string classification (Discriminatory or Non-Discriminatory) and the numeric prediction class (1 or 0). \"\"\" bin_inputs = self . encode_binary ( text ) with torch . no_grad (): bin_logits = self . bin_model ( ** bin_inputs ) . logits probs = torch . nn . functional . softmax ( bin_logits , dim =- 1 ) pred_class = torch . argmax ( probs ) . item () bin_label_map = { 0 : \"Non-Discriminatory\" , 1 : \"Discriminatory\" } bin_text_pred = bin_label_map [ pred_class ] return bin_text_pred , pred_class def category_inference ( self , text : str ): \"\"\"Performs inference on the input text to determine the regression values for the categories of discrimination. Parameters ---------- text : str The input text to be classified. Returns ------- list[float] A tuple consisting of the string classification (Discriminatory or Non-Discriminatory) and the numeric prediction class (1 or 0). \"\"\" ml_inputs = self . encode_multilabel ( text ) with torch . no_grad (): ml_outputs = self . ml_regr_model ( ** ml_inputs ) . logits ml_op_list = ml_outputs . squeeze () . tolist () results = [] for item in ml_op_list : results . append ( min ( 1.0 , max ( 0.0 , item ))) return results __init__ ( bin_model_path = MODELS_DIR / 'binary_classification/best_model' , ml_regr_model_path = MODELS_DIR / 'multilabel_regression/best_model' ) Constructor for instantiating an InferenceHandler object. Parameters: bin_model_path ( Path , default: MODELS_DIR / 'binary_classification/best_model' ) \u2013 Directory path to the binary model tensor file (default is models/binary_classification/best_model). ml_regr_model_path ( Path , default: MODELS_DIR / 'multilabel_regression/best_model' ) \u2013 Directory path to the multilabel regression model tensor file (default is models/multilabel_regression/best_model). Source code in nlpinitiative\\modeling\\predict.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , bin_model_path : Path = MODELS_DIR / \"binary_classification/best_model\" , ml_regr_model_path : Path = MODELS_DIR / \"multilabel_regression/best_model\" , ): \"\"\"Constructor for instantiating an InferenceHandler object. Parameters ---------- bin_model_path : Path, optional Directory path to the binary model tensor file (default is models/binary_classification/best_model). ml_regr_model_path : Path, optional Directory path to the multilabel regression model tensor file (default is models/multilabel_regression/best_model). \"\"\" self . bin_tokenizer , self . bin_model = self . init_model_and_tokenizer ( bin_model_path ) self . ml_regr_tokenizer , self . ml_regr_model = self . init_model_and_tokenizer ( ml_regr_model_path ) category_inference ( text ) Performs inference on the input text to determine the regression values for the categories of discrimination. Parameters: text ( str ) \u2013 The input text to be classified. Returns: list [ float ] \u2013 A tuple consisting of the string classification (Discriminatory or Non-Discriminatory) and the numeric prediction class (1 or 0). Source code in nlpinitiative\\modeling\\predict.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def category_inference ( self , text : str ): \"\"\"Performs inference on the input text to determine the regression values for the categories of discrimination. Parameters ---------- text : str The input text to be classified. Returns ------- list[float] A tuple consisting of the string classification (Discriminatory or Non-Discriminatory) and the numeric prediction class (1 or 0). \"\"\" ml_inputs = self . encode_multilabel ( text ) with torch . no_grad (): ml_outputs = self . ml_regr_model ( ** ml_inputs ) . logits ml_op_list = ml_outputs . squeeze () . tolist () results = [] for item in ml_op_list : results . append ( min ( 1.0 , max ( 0.0 , item ))) return results classify_text ( text ) Performs inference on the input text to determine the binary classification and the multilabel regression for the categories. Determines whether the text is discriminatory. If it is discriminatory, it will then perform regression on the input text to determine the assesed percentage that each category applies. Parameters: text ( str ) \u2013 The input text to be classified. Returns: dict [ str , Any ] \u2013 The resulting classification and regression values for each category. Source code in nlpinitiative\\modeling\\predict.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def classify_text ( self , text : str ): \"\"\"Performs inference on the input text to determine the binary classification and the multilabel regression for the categories. Determines whether the text is discriminatory. If it is discriminatory, it will then perform regression on the input text to determine the assesed percentage that each category applies. Parameters ---------- text : str The input text to be classified. Returns ------- dict[str, Any] The resulting classification and regression values for each category. \"\"\" result = { \"text_input\" : text , \"results\" : []} sent_res_arr = [] sentences = sent_tokenize ( text ) for sent in sentences : text_prediction , pred_class = self . discriminatory_inference ( sent ) sent_result = { \"sentence\" : sent , \"binary_classification\" : { \"classification\" : text_prediction , \"prediction_class\" : pred_class , }, \"multilabel_regression\" : None , } if pred_class == 1 : ml_results = { \"Gender\" : None , \"Race\" : None , \"Sexuality\" : None , \"Disability\" : None , \"Religion\" : None , \"Unspecified\" : None , } ml_infer_results = self . category_inference ( sent ) for idx , key in enumerate ( ml_results . keys ()): ml_results [ key ] = min ( max ( ml_infer_results [ idx ], 0.0 ), 1.0 ) sent_result [ \"multilabel_regression\" ] = ml_results sent_res_arr . append ( sent_result ) result [ \"results\" ] = sent_res_arr return result discriminatory_inference ( text ) Performs inference on the input text to determine the binary classification. Parameters: text ( str ) \u2013 The input text to be classified. Returns: tuple [ str , Number ] \u2013 A tuple consisting of the string classification (Discriminatory or Non-Discriminatory) and the numeric prediction class (1 or 0). Source code in nlpinitiative\\modeling\\predict.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def discriminatory_inference ( self , text : str ): \"\"\"Performs inference on the input text to determine the binary classification. Parameters ---------- text : str The input text to be classified. Returns ------- tuple[str, Number] A tuple consisting of the string classification (Discriminatory or Non-Discriminatory) and the numeric prediction class (1 or 0). \"\"\" bin_inputs = self . encode_binary ( text ) with torch . no_grad (): bin_logits = self . bin_model ( ** bin_inputs ) . logits probs = torch . nn . functional . softmax ( bin_logits , dim =- 1 ) pred_class = torch . argmax ( probs ) . item () bin_label_map = { 0 : \"Non-Discriminatory\" , 1 : \"Discriminatory\" } bin_text_pred = bin_label_map [ pred_class ] return bin_text_pred , pred_class encode_binary ( text ) Preprocesses and tokenizes the input text for binary classification. Parameters: text ( str ) \u2013 The input text to be preprocessed and tokenized. Returns: BatchEncoding \u2013 The preprocessed and tokenized input text. Source code in nlpinitiative\\modeling\\predict.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def encode_binary ( self , text : str ): \"\"\"Preprocesses and tokenizes the input text for binary classification. Parameters ---------- text : str The input text to be preprocessed and tokenized. Returns ------- BatchEncoding The preprocessed and tokenized input text. \"\"\" bin_tokenized_input = self . bin_tokenizer ( text , return_tensors = \"pt\" , truncation = True , padding = True , max_length = 512 ) return bin_tokenized_input encode_input ( text ) Preprocesses and tokenizes the input text sentiment classification (both models). Parameters: text ( str ) \u2013 The input text to be preprocessed and tokenized. Returns: tuple [ BatchEncoding , BatchEncoding ] \u2013 A tuple containing preprocessed and tokenized input text for both the binary and multilabel regression models. Source code in nlpinitiative\\modeling\\predict.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def encode_input ( self , text : str ): \"\"\"Preprocesses and tokenizes the input text sentiment classification (both models). Parameters ---------- text : str The input text to be preprocessed and tokenized. Returns ------- tuple[BatchEncoding, BatchEncoding] A tuple containing preprocessed and tokenized input text for both the binary and multilabel regression models. \"\"\" bin_inputs = self . encode_binary ( text ) ml_inputs = self . encode_multilabel ( text ) return bin_inputs , ml_inputs encode_multilabel ( text ) Preprocesses and tokenizes the input text for multilabel regression. Parameters: text ( str ) \u2013 The input text to be preprocessed and tokenized. Returns: BatchEncoding \u2013 The preprocessed and tokenized input text. Source code in nlpinitiative\\modeling\\predict.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def encode_multilabel ( self , text : str ): \"\"\"Preprocesses and tokenizes the input text for multilabel regression. Parameters ---------- text : str The input text to be preprocessed and tokenized. Returns ------- BatchEncoding The preprocessed and tokenized input text. \"\"\" ml_tokenized_input = self . ml_regr_tokenizer ( text , return_tensors = \"pt\" , truncation = True , padding = True , max_length = 512 ) return ml_tokenized_input init_model_and_tokenizer ( model_path ) Initializes a model and tokenizer for use in inference using the models path. Parameters: model_path ( Path ) \u2013 Directory path to the models tensor file. Returns: tuple [ PreTrainedTokenizer | PreTrainedTokenizerFast , PreTrainedModel ] \u2013 A tuple containing the tokenizer and model objects. Source code in nlpinitiative\\modeling\\predict.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def init_model_and_tokenizer ( self , model_path : Path ): \"\"\"Initializes a model and tokenizer for use in inference using the models path. Parameters ---------- model_path : Path Directory path to the models tensor file. Returns ------- tuple[PreTrainedTokenizer | PreTrainedTokenizerFast, PreTrainedModel] A tuple containing the tokenizer and model objects. \"\"\" with open ( model_path / \"config.json\" ) as config_file : config_json = json . load ( config_file ) model_name = config_json [ \"_name_or_path\" ] model_type = config_json [ \"model_type\" ] tokenizer = AutoTokenizer . from_pretrained ( model_name ) model = AutoModelForSequenceClassification . from_pretrained ( model_path , model_type = model_type ) model . eval () return tokenizer , model","title":"Model Training, Evaluation and Inference"},{"location":"model-management/#model-pipeline-operations","text":"","title":"Model Pipeline Operations"},{"location":"model-management/#model-training-and-evaluation","text":"","title":"Model Training and Evaluation"},{"location":"model-management/#nlpinitiative.modeling.train","text":"Script file containing the logic for training NLP models.","title":"train"},{"location":"model-management/#nlpinitiative.modeling.train.RegressionTrainer","text":"Bases: Trainer A custom class for overriding the compute_loss method used in regression model training. Source code in nlpinitiative\\modeling\\train.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 class RegressionTrainer ( Trainer ): \"\"\"A custom class for overriding the compute_loss method used in regression model training.\"\"\" def compute_loss ( self , model , inputs , return_outputs : bool = False , ** kwargs ): \"\"\"Overridden comput_loss method necessary for training the multilabel regression model. Parameters ---------- model The model being trained. inputs The dataset being used to train the model. return_outputs : bool, optional True if the outputs should be returned with the losses, False otherwise (default is False). **kwargs Any other additional parameters/configurations to use. Returns ------- tuple[Any, Any] | Any A tuple consisting of the loss values and the corresponding outputs (if return_outputs is True), or just the loss values (if return_outputs is False). \"\"\" labels = inputs . get ( \"labels\" ) outputs = model ( ** inputs ) logits = outputs . get ( \"logits\" ) # Ensure labels and logits are float labels = labels . to ( torch . float32 ) logits = logits . to ( torch . float32 ) loss_fct = torch . nn . MSELoss () loss = loss_fct ( logits , labels ) return ( loss , outputs ) if return_outputs else loss","title":"RegressionTrainer"},{"location":"model-management/#nlpinitiative.modeling.train.RegressionTrainer.compute_loss","text":"Overridden comput_loss method necessary for training the multilabel regression model. Parameters: model \u2013 The model being trained. inputs \u2013 The dataset being used to train the model. return_outputs ( bool , default: False ) \u2013 True if the outputs should be returned with the losses, False otherwise (default is False). **kwargs \u2013 Any other additional parameters/configurations to use. Returns: tuple [ Any , Any ] | Any \u2013 A tuple consisting of the loss values and the corresponding outputs (if return_outputs is True), or just the loss values (if return_outputs is False). Source code in nlpinitiative\\modeling\\train.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def compute_loss ( self , model , inputs , return_outputs : bool = False , ** kwargs ): \"\"\"Overridden comput_loss method necessary for training the multilabel regression model. Parameters ---------- model The model being trained. inputs The dataset being used to train the model. return_outputs : bool, optional True if the outputs should be returned with the losses, False otherwise (default is False). **kwargs Any other additional parameters/configurations to use. Returns ------- tuple[Any, Any] | Any A tuple consisting of the loss values and the corresponding outputs (if return_outputs is True), or just the loss values (if return_outputs is False). \"\"\" labels = inputs . get ( \"labels\" ) outputs = model ( ** inputs ) logits = outputs . get ( \"logits\" ) # Ensure labels and logits are float labels = labels . to ( torch . float32 ) logits = logits . to ( torch . float32 ) loss_fct = torch . nn . MSELoss () loss = loss_fct ( logits , labels ) return ( loss , outputs ) if return_outputs else loss","title":"compute_loss"},{"location":"model-management/#nlpinitiative.modeling.train.bin_train_args","text":"Generates training arguments for use in binary classification model training. Parameters: output_dir ( str , default: MODELS_DIR / 'binary_classification' ) \u2013 The output directory to store the trained model (default is models/binary_classification). eval_strat ( str , default: 'steps' ) \u2013 The evaluation strategy (default is 'steps'). save_strat ( str , default: 'steps' ) \u2013 The save strategy (default is 'steps'). logging_steps ( int , default: 500 ) \u2013 The periodicity for which logging will occur (default is 500). save_steps ( int , default: 500 ) \u2013 The step periodicity for which a model will be saved (default is 500). learn_rate ( float , default: 2e-05 ) \u2013 A hyper parameter for determining model parameter adjustment during training (default is 2e-5). batch_sz ( int , default: 8 ) \u2013 The training data batch sizes (default is 8). num_train_epochs ( int , default: 3 ) \u2013 The number of training epochs to be performed (default is 3). weight_decay ( float , default: 0.01 ) \u2013 The weight decay to apply (default is 0.01). best_model_at_end ( bool , default: True ) \u2013 True if the best model is to be saved at the completion of model training, False otherwise (default is True). best_model_metric ( str , default: 'f1' ) \u2013 The metric used for determining the best performing model (default is 'f1'). greater_better ( bool , default: True ) \u2013 True for if the better performing model should have the greater value (based on the specified metric), False otherwise (default is True). Returns: TrainingArguments \u2013 The training arguments object used for conducting binary classification model training. Source code in nlpinitiative\\modeling\\train.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 def bin_train_args ( output_dir : Path = MODELS_DIR / \"binary_classification\" , eval_strat : str = \"steps\" , save_strat : str = \"steps\" , logging_steps : int = 500 , save_steps : int = 500 , learn_rate : float = 2e-5 , batch_sz : int = 8 , num_train_epochs : int = 3 , weight_decay : float = 0.01 , best_model_at_end : bool = True , best_model_metric : str = \"f1\" , greater_better : bool = True , ): \"\"\"Generates training arguments for use in binary classification model training. Parameters ---------- output_dir : str, optional The output directory to store the trained model (default is models/binary_classification). eval_strat : str, optional The evaluation strategy (default is 'steps'). save_strat : str, optional The save strategy (default is 'steps'). logging_steps : int, optional The periodicity for which logging will occur (default is 500). save_steps : int, optional The step periodicity for which a model will be saved (default is 500). learn_rate : float, optional A hyper parameter for determining model parameter adjustment during training (default is 2e-5). batch_sz : int, optional The training data batch sizes (default is 8). num_train_epochs : int, optional The number of training epochs to be performed (default is 3). weight_decay : float, optional The weight decay to apply (default is 0.01). best_model_at_end : bool, optional True if the best model is to be saved at the completion of model training, False otherwise (default is True). best_model_metric : str, optional The metric used for determining the best performing model (default is 'f1'). greater_better : bool, optional True for if the better performing model should have the greater value (based on the specified metric), False otherwise (default is True). Returns ------- TrainingArguments The training arguments object used for conducting binary classification model training. \"\"\" return _train_args ( output_dir , eval_strat , save_strat , logging_steps , save_steps , learn_rate , batch_sz , num_train_epochs , weight_decay , best_model_at_end , best_model_metric , greater_better , )","title":"bin_train_args"},{"location":"model-management/#nlpinitiative.modeling.train.compute_bin_metrics","text":"Computes the metrics values resulting from the evaluation of the trained binary classification model. Parameters: eval_predictions \u2013 A tuple consisting of the label and prediction pair. Returns: dict [ str , Any ] \u2013 A dict consisting of the computed metrics values from evaluation of the trained binary classification model. Source code in nlpinitiative\\modeling\\train.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def compute_bin_metrics ( eval_predictions ): \"\"\"Computes the metrics values resulting from the evaluation of the trained binary classification model. Parameters ---------- eval_predictions A tuple consisting of the label and prediction pair. Returns ------- dict[str, Any] A dict consisting of the computed metrics values from evaluation of the trained binary classification model. \"\"\" predictions , lbls = eval_predictions preds = predictions . argmax ( axis = 1 ) probs = torch . nn . functional . softmax ( torch . tensor ( predictions ), dim = 1 ) . numpy () prec , recall , f1 , _ = precision_recall_fscore_support ( lbls , preds , average = \"binary\" ) acc = accuracy_score ( lbls , preds ) auprc = average_precision_score ( lbls , probs [:, 0 ]) auroc = roc_auc_score ( lbls , probs [:, 0 ]) return { \"accuracy\" : acc , \"precision\" : prec , \"recall\" : recall , \"f1\" : f1 , \"auprc\" : auprc , \"auroc\" : auroc , }","title":"compute_bin_metrics"},{"location":"model-management/#nlpinitiative.modeling.train.compute_reg_metrics","text":"Computes the metrics values resulting from the evaluation of the trained multilabel regression model. Parameters: eval_predictions \u2013 A tuple consisting of the labels corresponding prediction pair. Returns: dict [ str , float ] \u2013 A dict consisting of the computed metrics values from evaluation of the trained multilabel regression model. Source code in nlpinitiative\\modeling\\train.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def compute_reg_metrics ( eval_predictions ): \"\"\"Computes the metrics values resulting from the evaluation of the trained multilabel regression model. Parameters ---------- eval_predictions A tuple consisting of the labels corresponding prediction pair. Returns ------- dict[str, float] A dict consisting of the computed metrics values from evaluation of the trained multilabel regression model. \"\"\" preds , lbls = eval_predictions mse = mean_squared_error ( lbls , preds , multioutput = \"raw_values\" ) sqrt_mse = np . sqrt ( mse ) mae = mean_absolute_error ( lbls , preds , multioutput = \"raw_values\" ) r2 = r2_score ( lbls , preds , multioutput = \"raw_values\" ) pear_corr = [ pearsonr ( lbls [:, i ], preds [:, i ])[ 0 ] if len ( np . unique ( lbls [:, i ])) > 1 else np . nan for i in range ( lbls . shape [ 1 ]) ] mean_rmse = sqrt_mse . mean () mean_mae = mae . mean () mean_r2 = r2 . mean () mean_pear = np . nanmean ( pear_corr ) return { \"mean_rmse\" : mean_rmse , \"mean_mae\" : mean_mae , \"mean_r2\" : mean_r2 , \"mean_pearson\" : mean_pear , ## Per category \"rmse_per_cat\" : sqrt_mse . tolist (), \"mae_per_cat\" : mae . tolist (), \"r2_per_cat\" : r2 . tolist (), \"pearson_per_cat\" : pear_corr , }","title":"compute_reg_metrics"},{"location":"model-management/#nlpinitiative.modeling.train.get_bin_model","text":"Generates a model object to be trained for binary classification. Parameters: model_name ( str , default: DEF_MODEL ) \u2013 The name of the pretrained model to be fine-tuned (default is the DEF_MODEL specified in nlpinitiative/config.py). Returns: PreTrainedModel \u2013 The corresponding pretrained subclass model object for binary classification. Source code in nlpinitiative\\modeling\\train.py 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 def get_bin_model ( model_name : str = DEF_MODEL ): \"\"\"Generates a model object to be trained for binary classification. Parameters ---------- model_name: str, optional The name of the pretrained model to be fine-tuned (default is the DEF_MODEL specified in nlpinitiative/config.py). Returns ------- PreTrainedModel The corresponding pretrained subclass model object for binary classification. \"\"\" return AutoModelForSequenceClassification . from_pretrained ( model_name )","title":"get_bin_model"},{"location":"model-management/#nlpinitiative.modeling.train.get_ml_model","text":"Generates a model object to be trained for multilabel regression. Parameters: model_name ( str , default: DEF_MODEL ) \u2013 The name of the pretrained model to be fine-tuned (default is the DEF_MODEL specified in nlpinitiative/config.py). Returns: PreTrainedModel \u2013 The corresponding pretrained subclass model object for multilabel regression. Source code in nlpinitiative\\modeling\\train.py 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 def get_ml_model ( model_name : str = DEF_MODEL ): \"\"\"Generates a model object to be trained for multilabel regression. Parameters ---------- model_name: str, optional The name of the pretrained model to be fine-tuned (default is the DEF_MODEL specified in nlpinitiative/config.py). Returns ------- PreTrainedModel The corresponding pretrained subclass model object for multilabel regression. \"\"\" return AutoModelForSequenceClassification . from_pretrained ( model_name , num_labels = len ( CATEGORY_LABELS ) )","title":"get_ml_model"},{"location":"model-management/#nlpinitiative.modeling.train.ml_regr_train_args","text":"Generates training arguments for use in multilabel regression model training. Parameters: output_dir ( str , default: MODELS_DIR / 'multilabel_regression' ) \u2013 The output directory to store the trained model (default is models/multilabel_regression). eval_strat ( str , default: 'steps' ) \u2013 The evaluation strategy (default is 'steps'). save_strat ( str , default: 'steps' ) \u2013 The save strategy (default is 'steps'). logging_steps ( int , default: 500 ) \u2013 The periodicity for which logging will occur (default is 500). save_steps ( int , default: 500 ) \u2013 The step periodicity for which a model will be saved (default is 500). learn_rate ( float , default: 2e-05 ) \u2013 A hyper parameter for determining model parameter adjustment during training (default is 2e-5). batch_sz ( int , default: 8 ) \u2013 The training data batch sizes (default is 8). num_train_epochs ( int , default: 3 ) \u2013 The number of training epochs to be performed (default is 3). weight_decay ( float , default: 0.01 ) \u2013 The weight decay to apply (default is 0.01). best_model_at_end ( bool , default: True ) \u2013 True if the best model is to be saved at the completion of model training, False otherwise (default is True). best_model_metric ( str , default: 'eval_mean_rmse' ) \u2013 The metric used for determining the best performing model (default is 'eval_mean_rmse'). greater_better ( bool , default: False ) \u2013 True for if the better performing model should have the greater value (based on the specified metric), False otherwise (default is False). Returns: TrainingArguments \u2013 The training arguments object used for conducting multilabel regression model training. Source code in nlpinitiative\\modeling\\train.py 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 def ml_regr_train_args ( output_dir : Path = MODELS_DIR / \"multilabel_regression\" , eval_strat : str = \"steps\" , save_strat : str = \"steps\" , logging_steps : int = 500 , save_steps : int = 500 , learn_rate : float = 2e-5 , batch_sz : int = 8 , num_train_epochs : int = 3 , weight_decay : float = 0.01 , best_model_at_end : bool = True , best_model_metric : str = \"eval_mean_rmse\" , greater_better : bool = False , ): \"\"\"Generates training arguments for use in multilabel regression model training. Parameters ---------- output_dir : str, optional The output directory to store the trained model (default is models/multilabel_regression). eval_strat : str, optional The evaluation strategy (default is 'steps'). save_strat : str, optional The save strategy (default is 'steps'). logging_steps : int, optional The periodicity for which logging will occur (default is 500). save_steps : int, optional The step periodicity for which a model will be saved (default is 500). learn_rate : float, optional A hyper parameter for determining model parameter adjustment during training (default is 2e-5). batch_sz : int, optional The training data batch sizes (default is 8). num_train_epochs : int, optional The number of training epochs to be performed (default is 3). weight_decay : float, optional The weight decay to apply (default is 0.01). best_model_at_end : bool, optional True if the best model is to be saved at the completion of model training, False otherwise (default is True). best_model_metric : str, optional The metric used for determining the best performing model (default is 'eval_mean_rmse'). greater_better : bool, optional True for if the better performing model should have the greater value (based on the specified metric), False otherwise (default is False). Returns ------- TrainingArguments The training arguments object used for conducting multilabel regression model training. \"\"\" return _train_args ( output_dir , eval_strat , save_strat , logging_steps , save_steps , learn_rate , batch_sz , num_train_epochs , weight_decay , best_model_at_end , best_model_metric , greater_better , )","title":"ml_regr_train_args"},{"location":"model-management/#nlpinitiative.modeling.train.train","text":"Performs training on the binary classification and multilabel regression models. Parameters: bin_trainer ( Trainer ) \u2013 The trainer object for performing binary classification model training. ml_trainer ( Trainer ) \u2013 The trainer object for performing multilabel regression model training. token ( str , default: None ) \u2013 A Hugging Face token with read/write access privileges to allow exporting the trained models (default is None). Returns: tuple [ dict [ str , float ], dict [ str , float ]] \u2013 A tuple consisting of the dicts containing the metrics evaluation results for the binary classification and multilabel regression model training. Source code in nlpinitiative\\modeling\\train.py 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 def train ( bin_trainer : Trainer , ml_trainer : Trainer , token : str = None ): \"\"\"Performs training on the binary classification and multilabel regression models. Parameters ---------- bin_trainer : Trainer The trainer object for performing binary classification model training. ml_trainer : Trainer The trainer object for performing multilabel regression model training. token : str, optional A Hugging Face token with read/write access privileges to allow exporting the trained models (default is None). Returns ------- tuple[dict[str, float], dict[str, float]] A tuple consisting of the dicts containing the metrics evaluation results for the binary classification and multilabel regression model training. \"\"\" bin_trainer . train () bin_model = bin_trainer . model bin_model . save_pretrained ( save_directory = MODELS_DIR / \"binary_classification\" / \"best_model\" ) ml_trainer . train () ml_model = ml_trainer . model ml_model . save_pretrained ( save_directory = MODELS_DIR / \"multilabel_regression\" / \"best_model\" ) if token : upload_best_models ( token = token ) bin_eval = bin_trainer . evaluate () ml_eval = ml_trainer . evaluate () return bin_eval , ml_eval","title":"train"},{"location":"model-management/#nlpinitiative.modeling.train.upload_best_models","text":"Pushes the current best performing binary classification and multilabel regression models to their respective linked Hugging Face Model Repositories. Parameters: token ( str ) \u2013 A Hugging Face token with read/write access privileges to allow exporting the trained models (default is None). Source code in nlpinitiative\\modeling\\train.py 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 def upload_best_models ( token : str ): \"\"\"Pushes the current best performing binary classification and multilabel regression models to their respective linked Hugging Face Model Repositories. Parameters ---------- token : str, optional A Hugging Face token with read/write access privileges to allow exporting the trained models (default is None). \"\"\" def load_model ( path : Path ): \"\"\"Initializes a PreTrainedModel object from the specified model path. Parameters ---------- path : Path The file path to the model. Returns ------- PreTrainedModel An instantiated PreTrainedModel object. \"\"\" with open ( path / \"config.json\" ) as config_file : config_json = json . load ( config_file ) model_type = config_json [ \"model_type\" ] return AutoModelForSequenceClassification . from_pretrained ( path , model_type = model_type ) bin_model = load_model ( MODELS_DIR / \"binary_classification/best_model\" ) ml_model = load_model ( MODELS_DIR / \"multilabel_regression/best_model\" ) bin_model . push_to_hub ( BIN_REPO , token = token ) hfh . upload_file ( path_or_fileobj = MODELS_DIR / \"binary_classification/best_model/config.json\" , path_in_repo = \"config.json\" , repo_id = BIN_REPO , token = token , ) ml_model . push_to_hub ( ML_REPO , token = token ) hfh . upload_file ( path_or_fileobj = MODELS_DIR / \"multilabel_regression/best_model/config.json\" , path_in_repo = \"config.json\" , repo_id = ML_REPO , token = token , )","title":"upload_best_models"},{"location":"model-management/#model-inference","text":"","title":"Model Inference"},{"location":"model-management/#nlpinitiative.modeling.predict","text":"Script file used for performing inference with an existing model.","title":"predict"},{"location":"model-management/#nlpinitiative.modeling.predict.InferenceHandler","text":"A class that handles performing inference using the trained binary classification and multilabel regression models. Source code in nlpinitiative\\modeling\\predict.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 class InferenceHandler : \"\"\"A class that handles performing inference using the trained binary classification and multilabel regression models.\"\"\" def __init__ ( self , bin_model_path : Path = MODELS_DIR / \"binary_classification/best_model\" , ml_regr_model_path : Path = MODELS_DIR / \"multilabel_regression/best_model\" , ): \"\"\"Constructor for instantiating an InferenceHandler object. Parameters ---------- bin_model_path : Path, optional Directory path to the binary model tensor file (default is models/binary_classification/best_model). ml_regr_model_path : Path, optional Directory path to the multilabel regression model tensor file (default is models/multilabel_regression/best_model). \"\"\" self . bin_tokenizer , self . bin_model = self . init_model_and_tokenizer ( bin_model_path ) self . ml_regr_tokenizer , self . ml_regr_model = self . init_model_and_tokenizer ( ml_regr_model_path ) def init_model_and_tokenizer ( self , model_path : Path ): \"\"\"Initializes a model and tokenizer for use in inference using the models path. Parameters ---------- model_path : Path Directory path to the models tensor file. Returns ------- tuple[PreTrainedTokenizer | PreTrainedTokenizerFast, PreTrainedModel] A tuple containing the tokenizer and model objects. \"\"\" with open ( model_path / \"config.json\" ) as config_file : config_json = json . load ( config_file ) model_name = config_json [ \"_name_or_path\" ] model_type = config_json [ \"model_type\" ] tokenizer = AutoTokenizer . from_pretrained ( model_name ) model = AutoModelForSequenceClassification . from_pretrained ( model_path , model_type = model_type ) model . eval () return tokenizer , model def encode_binary ( self , text : str ): \"\"\"Preprocesses and tokenizes the input text for binary classification. Parameters ---------- text : str The input text to be preprocessed and tokenized. Returns ------- BatchEncoding The preprocessed and tokenized input text. \"\"\" bin_tokenized_input = self . bin_tokenizer ( text , return_tensors = \"pt\" , truncation = True , padding = True , max_length = 512 ) return bin_tokenized_input def encode_multilabel ( self , text : str ): \"\"\"Preprocesses and tokenizes the input text for multilabel regression. Parameters ---------- text : str The input text to be preprocessed and tokenized. Returns ------- BatchEncoding The preprocessed and tokenized input text. \"\"\" ml_tokenized_input = self . ml_regr_tokenizer ( text , return_tensors = \"pt\" , truncation = True , padding = True , max_length = 512 ) return ml_tokenized_input def encode_input ( self , text : str ): \"\"\"Preprocesses and tokenizes the input text sentiment classification (both models). Parameters ---------- text : str The input text to be preprocessed and tokenized. Returns ------- tuple[BatchEncoding, BatchEncoding] A tuple containing preprocessed and tokenized input text for both the binary and multilabel regression models. \"\"\" bin_inputs = self . encode_binary ( text ) ml_inputs = self . encode_multilabel ( text ) return bin_inputs , ml_inputs def classify_text ( self , text : str ): \"\"\"Performs inference on the input text to determine the binary classification and the multilabel regression for the categories. Determines whether the text is discriminatory. If it is discriminatory, it will then perform regression on the input text to determine the assesed percentage that each category applies. Parameters ---------- text : str The input text to be classified. Returns ------- dict[str, Any] The resulting classification and regression values for each category. \"\"\" result = { \"text_input\" : text , \"results\" : []} sent_res_arr = [] sentences = sent_tokenize ( text ) for sent in sentences : text_prediction , pred_class = self . discriminatory_inference ( sent ) sent_result = { \"sentence\" : sent , \"binary_classification\" : { \"classification\" : text_prediction , \"prediction_class\" : pred_class , }, \"multilabel_regression\" : None , } if pred_class == 1 : ml_results = { \"Gender\" : None , \"Race\" : None , \"Sexuality\" : None , \"Disability\" : None , \"Religion\" : None , \"Unspecified\" : None , } ml_infer_results = self . category_inference ( sent ) for idx , key in enumerate ( ml_results . keys ()): ml_results [ key ] = min ( max ( ml_infer_results [ idx ], 0.0 ), 1.0 ) sent_result [ \"multilabel_regression\" ] = ml_results sent_res_arr . append ( sent_result ) result [ \"results\" ] = sent_res_arr return result def discriminatory_inference ( self , text : str ): \"\"\"Performs inference on the input text to determine the binary classification. Parameters ---------- text : str The input text to be classified. Returns ------- tuple[str, Number] A tuple consisting of the string classification (Discriminatory or Non-Discriminatory) and the numeric prediction class (1 or 0). \"\"\" bin_inputs = self . encode_binary ( text ) with torch . no_grad (): bin_logits = self . bin_model ( ** bin_inputs ) . logits probs = torch . nn . functional . softmax ( bin_logits , dim =- 1 ) pred_class = torch . argmax ( probs ) . item () bin_label_map = { 0 : \"Non-Discriminatory\" , 1 : \"Discriminatory\" } bin_text_pred = bin_label_map [ pred_class ] return bin_text_pred , pred_class def category_inference ( self , text : str ): \"\"\"Performs inference on the input text to determine the regression values for the categories of discrimination. Parameters ---------- text : str The input text to be classified. Returns ------- list[float] A tuple consisting of the string classification (Discriminatory or Non-Discriminatory) and the numeric prediction class (1 or 0). \"\"\" ml_inputs = self . encode_multilabel ( text ) with torch . no_grad (): ml_outputs = self . ml_regr_model ( ** ml_inputs ) . logits ml_op_list = ml_outputs . squeeze () . tolist () results = [] for item in ml_op_list : results . append ( min ( 1.0 , max ( 0.0 , item ))) return results","title":"InferenceHandler"},{"location":"model-management/#nlpinitiative.modeling.predict.InferenceHandler.__init__","text":"Constructor for instantiating an InferenceHandler object. Parameters: bin_model_path ( Path , default: MODELS_DIR / 'binary_classification/best_model' ) \u2013 Directory path to the binary model tensor file (default is models/binary_classification/best_model). ml_regr_model_path ( Path , default: MODELS_DIR / 'multilabel_regression/best_model' ) \u2013 Directory path to the multilabel regression model tensor file (default is models/multilabel_regression/best_model). Source code in nlpinitiative\\modeling\\predict.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def __init__ ( self , bin_model_path : Path = MODELS_DIR / \"binary_classification/best_model\" , ml_regr_model_path : Path = MODELS_DIR / \"multilabel_regression/best_model\" , ): \"\"\"Constructor for instantiating an InferenceHandler object. Parameters ---------- bin_model_path : Path, optional Directory path to the binary model tensor file (default is models/binary_classification/best_model). ml_regr_model_path : Path, optional Directory path to the multilabel regression model tensor file (default is models/multilabel_regression/best_model). \"\"\" self . bin_tokenizer , self . bin_model = self . init_model_and_tokenizer ( bin_model_path ) self . ml_regr_tokenizer , self . ml_regr_model = self . init_model_and_tokenizer ( ml_regr_model_path )","title":"__init__"},{"location":"model-management/#nlpinitiative.modeling.predict.InferenceHandler.category_inference","text":"Performs inference on the input text to determine the regression values for the categories of discrimination. Parameters: text ( str ) \u2013 The input text to be classified. Returns: list [ float ] \u2013 A tuple consisting of the string classification (Discriminatory or Non-Discriminatory) and the numeric prediction class (1 or 0). Source code in nlpinitiative\\modeling\\predict.py 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def category_inference ( self , text : str ): \"\"\"Performs inference on the input text to determine the regression values for the categories of discrimination. Parameters ---------- text : str The input text to be classified. Returns ------- list[float] A tuple consisting of the string classification (Discriminatory or Non-Discriminatory) and the numeric prediction class (1 or 0). \"\"\" ml_inputs = self . encode_multilabel ( text ) with torch . no_grad (): ml_outputs = self . ml_regr_model ( ** ml_inputs ) . logits ml_op_list = ml_outputs . squeeze () . tolist () results = [] for item in ml_op_list : results . append ( min ( 1.0 , max ( 0.0 , item ))) return results","title":"category_inference"},{"location":"model-management/#nlpinitiative.modeling.predict.InferenceHandler.classify_text","text":"Performs inference on the input text to determine the binary classification and the multilabel regression for the categories. Determines whether the text is discriminatory. If it is discriminatory, it will then perform regression on the input text to determine the assesed percentage that each category applies. Parameters: text ( str ) \u2013 The input text to be classified. Returns: dict [ str , Any ] \u2013 The resulting classification and regression values for each category. Source code in nlpinitiative\\modeling\\predict.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def classify_text ( self , text : str ): \"\"\"Performs inference on the input text to determine the binary classification and the multilabel regression for the categories. Determines whether the text is discriminatory. If it is discriminatory, it will then perform regression on the input text to determine the assesed percentage that each category applies. Parameters ---------- text : str The input text to be classified. Returns ------- dict[str, Any] The resulting classification and regression values for each category. \"\"\" result = { \"text_input\" : text , \"results\" : []} sent_res_arr = [] sentences = sent_tokenize ( text ) for sent in sentences : text_prediction , pred_class = self . discriminatory_inference ( sent ) sent_result = { \"sentence\" : sent , \"binary_classification\" : { \"classification\" : text_prediction , \"prediction_class\" : pred_class , }, \"multilabel_regression\" : None , } if pred_class == 1 : ml_results = { \"Gender\" : None , \"Race\" : None , \"Sexuality\" : None , \"Disability\" : None , \"Religion\" : None , \"Unspecified\" : None , } ml_infer_results = self . category_inference ( sent ) for idx , key in enumerate ( ml_results . keys ()): ml_results [ key ] = min ( max ( ml_infer_results [ idx ], 0.0 ), 1.0 ) sent_result [ \"multilabel_regression\" ] = ml_results sent_res_arr . append ( sent_result ) result [ \"results\" ] = sent_res_arr return result","title":"classify_text"},{"location":"model-management/#nlpinitiative.modeling.predict.InferenceHandler.discriminatory_inference","text":"Performs inference on the input text to determine the binary classification. Parameters: text ( str ) \u2013 The input text to be classified. Returns: tuple [ str , Number ] \u2013 A tuple consisting of the string classification (Discriminatory or Non-Discriminatory) and the numeric prediction class (1 or 0). Source code in nlpinitiative\\modeling\\predict.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def discriminatory_inference ( self , text : str ): \"\"\"Performs inference on the input text to determine the binary classification. Parameters ---------- text : str The input text to be classified. Returns ------- tuple[str, Number] A tuple consisting of the string classification (Discriminatory or Non-Discriminatory) and the numeric prediction class (1 or 0). \"\"\" bin_inputs = self . encode_binary ( text ) with torch . no_grad (): bin_logits = self . bin_model ( ** bin_inputs ) . logits probs = torch . nn . functional . softmax ( bin_logits , dim =- 1 ) pred_class = torch . argmax ( probs ) . item () bin_label_map = { 0 : \"Non-Discriminatory\" , 1 : \"Discriminatory\" } bin_text_pred = bin_label_map [ pred_class ] return bin_text_pred , pred_class","title":"discriminatory_inference"},{"location":"model-management/#nlpinitiative.modeling.predict.InferenceHandler.encode_binary","text":"Preprocesses and tokenizes the input text for binary classification. Parameters: text ( str ) \u2013 The input text to be preprocessed and tokenized. Returns: BatchEncoding \u2013 The preprocessed and tokenized input text. Source code in nlpinitiative\\modeling\\predict.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def encode_binary ( self , text : str ): \"\"\"Preprocesses and tokenizes the input text for binary classification. Parameters ---------- text : str The input text to be preprocessed and tokenized. Returns ------- BatchEncoding The preprocessed and tokenized input text. \"\"\" bin_tokenized_input = self . bin_tokenizer ( text , return_tensors = \"pt\" , truncation = True , padding = True , max_length = 512 ) return bin_tokenized_input","title":"encode_binary"},{"location":"model-management/#nlpinitiative.modeling.predict.InferenceHandler.encode_input","text":"Preprocesses and tokenizes the input text sentiment classification (both models). Parameters: text ( str ) \u2013 The input text to be preprocessed and tokenized. Returns: tuple [ BatchEncoding , BatchEncoding ] \u2013 A tuple containing preprocessed and tokenized input text for both the binary and multilabel regression models. Source code in nlpinitiative\\modeling\\predict.py 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def encode_input ( self , text : str ): \"\"\"Preprocesses and tokenizes the input text sentiment classification (both models). Parameters ---------- text : str The input text to be preprocessed and tokenized. Returns ------- tuple[BatchEncoding, BatchEncoding] A tuple containing preprocessed and tokenized input text for both the binary and multilabel regression models. \"\"\" bin_inputs = self . encode_binary ( text ) ml_inputs = self . encode_multilabel ( text ) return bin_inputs , ml_inputs","title":"encode_input"},{"location":"model-management/#nlpinitiative.modeling.predict.InferenceHandler.encode_multilabel","text":"Preprocesses and tokenizes the input text for multilabel regression. Parameters: text ( str ) \u2013 The input text to be preprocessed and tokenized. Returns: BatchEncoding \u2013 The preprocessed and tokenized input text. Source code in nlpinitiative\\modeling\\predict.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def encode_multilabel ( self , text : str ): \"\"\"Preprocesses and tokenizes the input text for multilabel regression. Parameters ---------- text : str The input text to be preprocessed and tokenized. Returns ------- BatchEncoding The preprocessed and tokenized input text. \"\"\" ml_tokenized_input = self . ml_regr_tokenizer ( text , return_tensors = \"pt\" , truncation = True , padding = True , max_length = 512 ) return ml_tokenized_input","title":"encode_multilabel"},{"location":"model-management/#nlpinitiative.modeling.predict.InferenceHandler.init_model_and_tokenizer","text":"Initializes a model and tokenizer for use in inference using the models path. Parameters: model_path ( Path ) \u2013 Directory path to the models tensor file. Returns: tuple [ PreTrainedTokenizer | PreTrainedTokenizerFast , PreTrainedModel ] \u2013 A tuple containing the tokenizer and model objects. Source code in nlpinitiative\\modeling\\predict.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def init_model_and_tokenizer ( self , model_path : Path ): \"\"\"Initializes a model and tokenizer for use in inference using the models path. Parameters ---------- model_path : Path Directory path to the models tensor file. Returns ------- tuple[PreTrainedTokenizer | PreTrainedTokenizerFast, PreTrainedModel] A tuple containing the tokenizer and model objects. \"\"\" with open ( model_path / \"config.json\" ) as config_file : config_json = json . load ( config_file ) model_name = config_json [ \"_name_or_path\" ] model_type = config_json [ \"model_type\" ] tokenizer = AutoTokenizer . from_pretrained ( model_name ) model = AutoModelForSequenceClassification . from_pretrained ( model_path , model_type = model_type ) model . eval () return tokenizer , model","title":"init_model_and_tokenizer"}]}